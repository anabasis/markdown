#

## OpenShift ipfailover POD 적정 리소스(CPU/Memory) 문의

* Platform Version : Redhat OpenShift 3.9

ipfailover 생성 시 Deployment Config에 별도의 limit 값이 지정되지 않았는데,

OpenShift ipfailover POD의 적정 리소스 (CPU/Memory)에 대한 설정값이 있을까요?

---

OpenShift ipfailover pod에 대한 recommended 값은 존재하지 않으나,

많은 resource를 사용하지 않기 때문에 100m/100Mi 설정값을 권고합니다.

## OpenShift Loggign(EKF) 로그를 수동으로 삭제하는 방법

* Platform Version : Redhat OpenShift 3.9

OpenShift Logging(EFK) 로그를 설정값에 의해 주기적으로 삭제하는 것이 아닌 수동으로 삭제하는 방법

---

일반적으로 elasticsearch의 log 양을 줄이기 위해서는 CURATOR_DEFAULT_DAYS 값을 변경하는 것을 권고드립니다. (KCS [1] 참조)

직접 삭제하고자 하는 경우, 아래와 같은 api를 이용하면 가능할 것으로 보입니다.
~~~~~
curl -XDELETE 'http://localhost:9200/logstash-2016-05-'
~~~~~

보다 자세한 내용은 KCS [2]를 참조하실 수 있습니다.

[1] https://access.redhat.com/solutions/3622791 - How to Check and Change the Amount of Logs Being Kept in Elasticsearch by OpenShift Projects
[2] https://access.redhat.com/solutions/2485631 - OpenShift Log Aggregation - Directly querying elasticsearch

## Pod Resource Limit(CPU/Memory)를 재시작 없이 변경하는 방법

* Platform Version : Redhat OpenShift 3.9

OpenShift 컨테이너 Resource Limit(CPU/Memory)를 Pod 재시작 없이 실시간 변경하는 방법

---

Limits는 deploymentconfigs, daemonsets, 또는 다른 deployment objects들에 정의될 수 있으며, pod가 create 또는 update 시에 적용됩니다.
이러한 변경 사항이 적용되려면 실행중인 pod는 재구동이 되어야 합니다.

관련 내용은 KCS [1]을 참조하실 수 있습니다.

"When making changes to the application limits or quotas, it is necessary for the applications to be restarted for the changes to take effect."

"Limits can be defined on application deploymentconfigs, daemonsets, or other deployment objects, to set a maximum memory and CPU that application uses. Quotas can be assigned to projects to mandate total memory or CPU usage in the project. However, running applications will need to be restarted for any of these changes to take effect."

[1] https://access.redhat.com/solutions/1556153
How to modify limits and quota for projects in Openshift 3

## OpenShift Node 증설

* Platform Version : Redhat OpenShift 3.11

Node unschdule 작업
 > new node openshift_schedulable=false

Node list
 worker4.ocp311.lds.kr 192.168.45.230

dns 등록 ip 정보 확인
  dig worker4.ocp311.lds.kr
    > worker4.ocp311.lds.kr 120 IN A 192.168.45.230

  신규 노드 사전 작업
  hostname 정의
  # hostname set-hostname [FQDN]

  bastion curl : 5000 // Docker registry
  # curl http://bastion.ocp311.lds.kr:5000

  bastion curl : 80 // Yum repository
  # curl http://bastion.ocp311.lds.kr:80

  NetworkManager 동작 확인
  # /etc/sysconfig/network-scripts
  # systemctl enable NetworkManager
  # systemctl status NetworkManager
 
  Selinux 동작 확인
  # /etc/selinux/config
    SELINUX=enforcing
  # sestatus
  # touch /.autorelabel

  기본 패키지 설치
  # yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct
  # yum update
  # reboot

  docker 설치
  # yum install docker-1.13.1
  # systemctl enable docker && systemctl start docker && systemctl status docker
 
  ssh key 복사
  # ssh-copy-id worker4.ocp311.lds.kr

  ansible 배포
  == bastion Node ==
  vi /etc/ansible/hosts.ocp311.fqdn.newnode
  [OSEv3:children]
  ...
  new_nodes
  worker4.ocp311.lds.kr openshift_node_group_name='node-config-compute' openshift_schedulable=false
 
  scaleup 실행
  # cd /usr/share/ansible/openshift-ansible
  # ansible-playbook -i /etc/ansible/hosts.ocp311.fqdn.newnode \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml \
-vvvv

  # oc get nodes -w
 
  신규노드 schedule
  == master node ==
  oc adm manage-node worker4.ocp311.lds.kr --schedulable=true
  oc get node

참고 URL
https://docs.openshift.com/container-platform/3.11/install/host_preparation.html
https://docs.openshift.com/container-platform/3.11/install/host_preparation.html#ensuring-host-access
https://docs.openshift.com/container-platform/3.11/install_config/adding_hosts_to_existing_cluster.html
https://docs.openshift.com/container-platform/3.11/install_config/adding_hosts_to_existing_cluster.htm

## OpenShift Pod 삭제 시 hang 발생 처리

* Platform Version : Redhat OpenShift 3.9

OpenShift Pod 삭제 시 hang 발생 처리

1. 삭제 절차
 - 콘솔에서 Delete pod immediately without waiting for the processes to terminate gracefully 미체크 : 현상 발생

---
일반적인 pod delete 과정은 아래와 같습니다.

1. user deletes a pod
2. pod deletion timestamp is set
3. kubelet observes pod update, and cleans up pod on node
4. kubelet sends final deletion request of pod
5. pod is removed

kubelet에 의한 final deletion request가 보내지지 못하는 경우 pod는 'terminating' phase로 남게 되어, 경우에 따라 명확하게 --force --grace-period=0 옵션을 추가하여 삭제할 수 있습니다.
이 2 옵션을 추가해도 삭제가 안되는 경우, OpenShift API에 아래와 같은 명령을 보내 직접 object를 삭제할 수 있습니다.

$ echo '{ "propagationPolicy": "Background" }' | curl -k -X DELETE -d @-  -H "Authorization: Bearer <token>" -H 'Accept: application/json' -H 'Content-Type: application/json'  https://<master_URL>:<port>/api/v1/namespaces/<namespace>/pods/<pod_name>;

## Node 장애 시 statefulset으로 배포된 Pod 복구방안

* Platform Version : Redhat OpenShift 3.9

OpenShift Node 장애 시 statefulset Pod 복구방안

node 장애 시 pod가 unknown, terminating 상태가 지속될 경우 복구방법은 아래와 같은데,

1. The Node object is deleted (either by you, or by the Node Controller).
2. The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver.
3. Force deletion of the Pod by the user.
> 참고 링크
    - https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/

그 중 권장방법은 1, 2번이라고 문서에 나와있는데,

 Q1) pod 복구를 해야할 상황 시 어떤 절차를 수행하면 될까요?
 Q2) 서버 기동중인 상태에서 pod가 unkown 상태 시 체크해야될 부분이 있을까요?
 Q3) 1번 (노드 삭제 후 추가) 시에 작업 절차 방법은 어떻게 될까요?
---

 Q1) pod 복구를 해야할 상황 시 어떤 절차를 수행하면 될까요?
 A1) node가 완전히 down 되어 복구가 불가능한 상황이거나, 네트워크에서 영구적으로 제거된 상황일 경우, pod 삭제보단 노드 삭제를 권고드립니다.
      만약 노드가 다시 복구 되었다면 일반적으로 kubelet 이 해당 pod를 kill 하게 되고 apiserver 로 부터 해당 name을 삭제하면 됩니다.
      노드가 복구되었음에도 pod가 terminating 혹은 unknown 상태가 유지가 되어있다면 다음과 같은 방법으로 pod event 및 로그를 봐야합니다.

 Q2) 서버 기동중인 상태에서 pod가 unkown 상태 시 체크해야될 부분이 있을까요?
 A2) 아래와 같은 명령으로, 파드의 event 및 로그를 체크해야 합니다.
  $ oc get event
  $ oc logs <pod name>
 
 Q3) 1번 (노드 삭제 후 추가) 시에 작업 절차 방법은 어떻게 될까요?
  $ oc delete node <node name>
 
노드 추가 방법
https://docs.openshift.com/container-platform/3.9/install_config/adding_hosts_to_existing_cluster.html#adding-cluster-hosts_adding-hosts-to-cluster

## pv 생성 방법

* Platform Version : Redhat OpenShift 3.11

1. pv.yaml 파일 생성
 $ cat lds-pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: lds-pv-01
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /exports/lds01
    server: 192.168.0.2

2. nfs 디렉토리 생성 및 권한 설정
 $ mkdir /exports/lds01
 $ chmod -R 777 /exports/lds01
 $ chown -R nfsnobody:nfsnobody /exports/lds01

3. pv 생성
 $ oc create -n pv-test -f lds-pv.yaml
 $ oc get pv | grep lds-pv-01
 $ oc get pvc

참고 URL
https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/storage.html

## OpenShift sftp container 활용하기

Community에서 제공하는 SFTP Container 테스트 현황을 아래와 같이 공유 합니다.

Test 결과 External IP를 사용한 형태는 안되니 참고 하시기 바랍니다.


1) Test 1
    분석
        Install an SFTP server on OpenShift
            https://medium.com/grensesnittet/install-an-sftp-server-on-openshift-818ea30a4319
       
        < Test History
           
    설계
        Installation overview
            Container source from docker hub or GitHub.
            Config map for the users:
                /etc/sftp/users.conf
            Config map for ssh keys and config:
                /etc/ssh/ssh_host_ed25519_key
                /etc/ssh/ssh_host_ed25519_key.pub
                /etc/ssh/ssh_host_rsa_key
                /etc/ssh/ssh_host_rsa_key.pub
                /etc/ssh/sshd_config
            NodePort allowing external traffic to port 30022 -> internal port 22
            A Persistent Volume (PV) storage for the user's upload directory:
                /home/user/upload
    실행
        Project
            < Use swa-test Project
                oc new-project int-sftp --display-name="Internal sftp server"
                oc project int-sftp
           
        Prepare Image
            < On Bastion
                docker pull atmoz/sftp:alpine-3.7
                docker images | grep atmoz
               
                docker tag docker.io/atmoz/sftp:alpine-3.7 \
                    dpaasbas1v.example-openshift.com:5000/linuxdata/atmoz-sftp:alpine-3.7
                   
                docker push dpaasbas1v.example-openshift.com:5000/linuxdata/atmoz-sftp:alpine-3.7
               
            < On Master Node
                oc import-image atmoz-sftp:alpine-3.7 -n openshift --confirm \
                    --from dpaasbas1v.example-openshift.com:5000/linuxdata/atmoz-sftp:alpine-3.7 \
                    --insecure=true


        New application
            < Create App
                oc new-app atmoz/sftp:alpine-3.7
                oc new-app https://github.com/atmoz/sftp#alpine-3.7
           
            oc new-app -n swa-test atmoz-sftp:alpine-3.7
                mkdir: can't create directory '/var/run/sftp': Permission denied
                    < 해당 로그 발생 안함
                        oc adm policy add-scc-to-user anyuid -z default -n swa-test
                            scc “anyuid” added to: [“system:serviceaccount:swa-test:default”]
                        oc edit scc anyuid -n swa-test
                            users:
                            - system:serviceaccount:swa-test:default
                   
                [entrypoint] FATAL: No users provided!
                    > 아래 configmap 정보 추가
                   
        SFTP users config map
            Webconsole > Resources > Config Maps
                Create Config Map
                    Name : sftp-etc-sftp
                    Key : users.conf
                    Value
                        foo:123:1001:100:upload
                        bar:abc:1002:100:upload
                        baz:xyz:1003:100:upload
                       
                    Create
                   
            Webconsole > Resources > Config Maps > Select Config Map
                Edit YAML
                    \r 내용 삭제
                   
        Mount the sftp config map
            Webconsole > Deployments > Select Deployments
                < atmoz-sftp
                > Configuration > Add Config Files
                    Add Config Files to atmoz-sftp
                        Source : sftp-etc-sftp
                        Mount Path : /etc/sftp
                       
                        Add
                       
            자동 Deploy 및 ssh key 생성됨
                ssh_host_ed25519_key
                ssh_host_ed25519_key.pub
                ssh_host_rsa_key
                ssh_host_rsa_key.pub
                sshd_config
               
        SSH config map
            Check file list
                cat /etc/ssh/ssh_host_ed25519_key
                cat /etc/ssh/ssh_host_ed25519_key.pub
                cat /etc/ssh/ssh_host_rsa_key
                cat /etc/ssh/ssh_host_rsa_key.pub
                cat /etc/ssh/sshd_config
                           
                < 다음 명령어를 통해 생성도 가능
                    ssh-keygen -t ed25519 -f ssh_host_ed25519_key < /dev/null
                    ssh-keygen -t rsa -b 4096 -f ssh_host_rsa_key < /dev/null
                   
       
            Webconsole > Resources > Config Maps
                Create Config Map
                    Name : sftp-etc-ssh
                        Key : ssh_host_ed25519_key
                        Value
                            <-- File Content -->
                        Add Item
                           
                        Key : ssh_host_ed25519_key.pub
                        Value
                            <-- File Content -->
                        Add Item
                           
                        Key : ssh_host_rsa_key
                        Value
                            <-- File Content -->
                        Add Item
                           
                        Key : ssh_host_rsa_key.pub
                        Value
                            <-- File Content -->
                        Add Item
                           
                        Key : sshd_config
                        Value
                            <-- File Content -->
                       
                    Create
                   
        Mount ssh config map
            Webconsole > Deployments > Select Deployments
                < atmoz-sftp
                > Configuration > Add Config Files
                    Add Config Files to atmoz-sftp
                        Source : sftp-etc-ssh
                        Mount Path : /etc/ssh
                       
                        Add
           
        Change to defaultMode: 384 setting for sftp-stc-ssh
            384 decimal = 600 octal = user +rw
            (See http://permissions-calculator.org/)
           
            Webconsole > Deployments > Select Deployments
                < atmoz-sftp
                > Edit YAML
                    - configMap:
                      defaultMode: 384
                      name: sftp-etc-ssh
                     
        Test the SSH config map
            Check Pod Log
                [entrypoint] Parsing user data: “foo:123:1001:100:upload”
                Creating mailbox file: No such file or directory
                [entrypoint] Creating directory: /home/foo/upload
                [entrypoint] Parsing user data: “bar:abc:1002:100:upload”
                Creating mailbox file: No such file or directory
                [entrypoint] Creating directory: /home/bar/upload
                [entrypoint] Parsing user data: “baz:xyz:1003:100:upload”
                Creating mailbox file: No such file or directory
                [entrypoint] Creating directory: /home/baz/upload
                [entrypoint] Executing sshd
                Server listening on 0.0.0.0 port 22.
                Server listening on :: port 22.
               
        Expose service port
            < External IP Test
           
        Test the service port
            < On jenkins
                cd /tmp/
                sftp -P 22 bar@100.30.222.75
                    < Input password / abc
                   
                    put release.version
                   
                < 성공
       
2) Test 2
    설계
        nodPort 사용
    실행
        Check Service
            oc get service -n swa-test atmoz-sftp --export -o yaml \
                > service-swa-test-atmoz-sftp.yml
               
            oc edit service -n swa-test atmoz
           
            oc get svc --all-namespaces -o yaml | grep nodePort
           
            oc get pod -n swa-test -o wide | grep atmoz-sftp
               
        Expose service port
            < External IP Test
                Webconsole > Applications > Services > Select Service
                    < atmoz-sftp
                    > Edit YAML
                        nodeport range
                        30000-32767
...
spec:
...
  ports:
    - name: 22-tcp
      nodePort: 30022
      port: 22
      protocol: TCP
      targetPort: 22
  type: NodePort
...

        Test the service port
            < On Infra Node
                cd /tmp/
                sftp -P 30022 bar@dpaaswkr1v.example-openshift.com
               
                sftp -P 22 bar@100.30.222.75
                    < Input password / abc
                   
                    put release.version
                   
                < 성공

    Ref
        atmoz sftp
            https://github.com/atmoz/sftp
        Using a NodePort to Get Traffic into the Cluster
            https://docs.openshift.com/container-platform/3.11/dev_guide/expose_service/expose_internal_ip_nodeport.html
            

## OpenShift etcd 백업 및 복원 절차


* Platform Version : Redhat OpenShift 3.9

1. etcd란
분산된 시스템 또는 클러스터의 설정을 공유하는 서비스 검색 및 스케줄러 조정을 위한 오픈소스이다.
분산형 키-값으로 저장소이며, etcd는 호스트에 스케줄링 되는 작업을 조정하며, 컨테이너에 대한 오버레이트 네트워킹 설정을 지원한다.

쿠버네티스와 etcd
쿠버네티스의 기본 데이터 저장소인 etcd는 모든 쿠버네티스 클러스터의 상태를 저장하고 복제한다. etcd는 쿠버네티스 클러스터를 구성하는 필수 요소이며 데이터들은 persistent 하게 저장된다.

etcd 는 컨테이너 오케스트레이션을 위한 쿠버네티스의 기본 데이터 저장소이며, 클라우드 네이티브 애플리케이션은 etcd를 사용함으로써 더욱 일광성 있는 가동 시간을 유지한다. 애플리케이션은 etcd에 데이터를 읽고 쓰며, 이를 통해 설정 데이터를 배포하여 노드 설정에 대한 이중화 및 복구를 제공한다.

2. etcd 백업 절차
 2.1. etcd 스냅 샷 캡처
 etcd의 현재 상태를  저장하기 위해 스냅 샷 캡처를 한다.
 # mkdir -p backup/etcd-$(date +%Y%m%d)
 # etcdctl3 snapshot save /var/lib/etcd/snapshot.db
 
 * etcd 환경 변수 및 인증서 적용 *
 # export ETCDCTL_API=3
 # source /etc/etcd/etcd.conf

 # etcdctl --cert=$ETCD_PEER_CERT_FILE \
  --key=$ETCD_PEER_KEY_FILE \
  --cacert=$ETCD_TRUSTED_CA_FILE \
  --endpoints=$ETCD_LISTEN_CLIENT_URLS \
    snapshot save snapshot.db

3. etcd 복원 절차
 3.1. etcd 구성원 서비스 중지
 모든 etcd 구성원에서 etcd, master-api, master-controllers 서비스를 중지한다.
 # system stop etcd.service atomic-openshift-master-api atomic-openshift-master-controllers
 

 3.2. etcd 데이터 삭제
 etcd 복원 절차를 수행 할 노드에서 데이터를 다시 생성하므로 이전 데이터를 삭제한다.
 # rm -r /var/lib/etcd

 3.3. etcd 복원
 etcd 스냅샷 복원 명령을 실행하여 파일의 값을 복원 시킨다.
 # etcdctl3 snapshot restore backup/etcd-20200729/db --data-dir /var/lib/etcd --name master1.test.com --initial-cluster "master1.test.com=https://192.168.88.211:2380" \ --initial-cluster-token="etcd-cluster-1" --initial-advertise-peer-urls=https://192.168.88.211:2380
 
 * 인증서 및 키 적용 *
 # etcdctl  --cert=$ETCD_PEER_CERT_FILE \
    --key=$ETCD_PEER_KEY_FILE \
    --cacert=$ETCD_TRUSTED_CA_FILE \
    snapshot restore snapshot.db \
    --name $ETCD_NAME \
    --initial-cluster $ETCD_INITIAL_CLUSTER \
    --initial-cluster-token $ETCD_INITIAL_CLUSTER_TOKEN \
    --initial-advertise-peer-urls $ETCD_INITIAL_ADVERTISE_PEER_URLS

 3.4. 파일 권한 및 컨텍스트 설정
 # chown -R etcd:etcd /var/lib/etcd/
 # restorecon -Rv /var/lib/etcd

 3.5. etcd 서비스 시작 및 상태 확인
 # systemctl start etcd
 # systemctl status etcd

 3.6. OpenShift 서비스 시작
 각 마스터에서 master-api, master-controolers 서비스를 시작한다.
 # systemctl start atomic-openshift-master-api atomic-openshift-master-controllers

 3.7. etcd 상태 체크
 # ETCD_ALL_ENDPOINTS=` etcdctl3 --write-out=fields  member list | awk '/ClientURL/{printf "%s%s",sep,$3; sep=","}'`
 # etcdctl3 --endpoints=$ETCD_ALL_ENDPOINTS  endpoint status  --write-out=table

 3.8 클러스터 상태 확인
 # oc get nodes,pods -n kube-system
 
 
##  OpenShift nodejs container 실행 테스트

안녕하세요.


Redhat에서 제공하는 nodejs image를 Container 테스트 현황을 아래와 같이 공유 합니다.

OKD에서도 해당 Image를 사용하면 그대로 구현 가능할 것이라고 생각 됩니다.


1) Test 1
    설계
        nodejs Test
            https://docs.openshift.com/container-platform/3.11/using_images/s2i_images/nodejs.html
         
            Git
                Clone
                upload on gogs
            Image 준비
            Template Test
            Guide
    Ref
        Easily deploy Node.js applications to Red Hat OpenShift using Nodeshift
            https://developers.redhat.com/blog/2019/08/30/easily-deploy-node-js-applications-to-red-hat-openshift-using-nodeshift/
        Deploy a 3-tier Node.js app on Red Hat OpenShift
            https://developers.redhat.com/articles/deploy-a-NodeJS-app-OpenShift
        Node.js
            https://docs.openshift.com/container-platform/3.11/using_images/s2i_images/nodejs.html
        Container images
            https://access.redhat.com/containers/
                node js
               
2) Test 2
    설계
        template nodejs.json
            https://github.com/sclorg/nodejs-ex/blob/master/openshift/templates/nodejs.json
        Run Your Nodejs projects on OpenShift in Two Simple Steps
            https://www.openshift.com/blog/run-your-nodejs-projects-on-openshift-in-two-simple-steps
    실행
        nodejs
            https://docs.openshift.com/container-platform/3.11/using_images/s2i_images/nodejs.html
           
        Images
            Search
                https://access.redhat.com/containers/
                    node js
                https://catalog.redhat.com/software/containers/rhscl/nodejs-12-rhel7/5da02401bed8bd2245d95cae
               
            Check image
                < On Master
                    oc get is -n openshift nodejs
                   
                    oc describe -n openshift is nodejs
                        nodejs:12
                       
            Download
                < On Bastion
                RHEL 7 Based Images
                    < Pull a base builder image to build on
                    docker pull registry.redhat.io/rhscl/nodejs-12-rhel7:1-24
                   
                    docker tag registry.redhat.io/rhscl/nodejs-12-rhel7:1-24 \
                        dpaasbas1v.example.com:5000/example/nodejs-12-rhel7:1-24
                       
                    docker tag registry.redhat.io/rhscl/nodejs-12-rhel7:1-24 \
                        dpaasbas1v.example.com:5000/example/nodejs-12-rhel7:latest
                       
                    docker push dpaasbas1v.example.com:5000/example/nodejs-12-rhel7:1-24
                   
            Upload
                < On Master
                   
                    oc import-image nodejs:12 -n openshift --confirm \
                        --from='dpaasbas1v.example.com:5000/example/nodejs-12-rhel7:1-24' \
                        --insecure=true
                       
                    oc delete is nodejs:10 -n openshift
                       
                    oc import-image nodejs:10 -n openshift --confirm \
                        --from='dpaasbas1v.example.com:5000/example/nodejs-12-rhel7:1-24' \
                        --insecure=true
                   
        Build Process
            Note
                Starts a container from the builder image.
                Downloads the application source.
                Streams the scripts and application sources into the builder image container.
                Runs the assemble script (from the builder image).
                Saves the final image.
            Prepare source
                < Pull an application code
                github
                    download source
                        < On Bastion Node
                        https://github.com/sclorg/nodejs-ex.git
                       
                        wget https://github.com/sclorg/nodejs-ex/archive/master.zip
                       
                        scp /tmp/master.zip dpaasmst2v.example.com:/tmp/
        Template
            Check
                oc get template -n openshift
               
            Create
                mkdir ~/Documents/Work/20201201_nodejs/template
                cd ~/Documents/Work/20201201_nodejs/template
               
                vi nodejs-example.json
                    https://raw.githubusercontent.com/sclorg/nodejs-ex/master/openshift/templates/nodejs.json
                   
                    %s/nodejs-example/nodejs-example/g
                   
                oc create -n openshift -f nodejs-example.json
               
                oc get template -n openshift nodejs-example
               
                oc edit template -n openshift nodejs-example
               
                oc get template -n openshift nodejs-example --export -o yaml \
                    > template-nodejs-example.yml

3) Test 3
    실행
        < 성공
        Application Test
            Create App
                Webconsole > Search Catalog < nodejs
                    example nodejs
                        Information
                            Next
                        Configuration
                            Name : nodejs-12-test01
                            Namespace : openshift
                            Version of NodeJS Image : 12
                            Git Repository URL : https://github.com/sclorg/nodejs-ex.git
                           
                            Create
                        Results
                            Close
                           
            Create Secret
                Webconsole > Resources > Secret > Create Secret
                    Secret Type : Source Secret
                    Secret Name : nodejs-12-test01-git-secret
                    Authentication Type : Basic Authentication
                    Username : example
                    Passwor or Token : example
                   
            Edit Build
                Webconsole > Builds > Select Builds
                    Actions > Edit
                        Source Configuration
                            Source Secret : nodejs-12-test01-git-secret
                           
    Ref
        Nodejs와 Tomcat의 차이점
            https://www.python2.net/questions-84274.htm
        Managing Node.js - Express Sessions with Redis
            https://medium.com/mtholla/managing-node-js-express-sessions-with-redis-94cd099d6f2f
        Anatomy of NodeJS and Apache
            https://medium.com/swlh/anatomy-of-nodejs-and-apache-9ac173d4a0f8
            
            
## OpenShift Prune 기능 정리

안녕하세요.

OpenShift에서 리소스 확보를 위해 오래된 image, build, deployment 등을 삭제하는 방법을 공유합니다.

* Platform Version : Redhat OpenShift 3.9

1. Prune 정의
OpenShift Container 에서 생성된 API 오브젝트는 애플리케이션 구축 및 배포시 etcd date store에 누적됩니다.
etcd data store에 누적이 되다보면 관리자는 더 이상 필요하지 않은 이전 버전의 오브젝트를 저장소 용량 확보를 위해 제거 해줘야 합니다.
예를 들어 사용하지 않는 오래된 이미지가 있다면 Prune 기능을 통해 잘라내면 디스크 공간을 차지하는 오래된 이미지 삭제를 할 수 있습니다.

prune 명령어를 통해 Build, Deployment, Image 세 가지를 삭제할 수 있는데, 특정 네임스페이스를 지정하지 않으면 전역으로 설정됩니다.

2. Prune (Builds, Deployments) 적용 방법
-- confirm=flase (defaults)
prune을 적용 시킬 때 사용합니다. 기본 값은 false로 되어 있어 이 옵션을 지정하지 않으면 삭제되지 않습니다.

--keep-complete=5 (defaults)
Builds 또는 Deployments 상태가 completed인 경우 순차적으로 오래된 값부터 출력합니다.

--keep-failed=1 (defaults)
Builds 또는 Deployment 상태가 Failed, Canceled인 경우 순차적으로 오래된 값부터 출력합니다.

--orphans=flase (defaults)
Buildconfig, Deployconfig 가 남아있지 않으면서, Complete 또는 Failed 상태인 값을 출력합니다.

사용 예) prune 사용 예제
$ oc adm prune builds --keep-failed=1 --keep-complete=5, --orphans -n test-web
NAMESPAC      NAME
web-test1        httpd-web-3
web-test1        httpd-web-2
web-test1        httpd-web-4

출력 값을 보면 NAME 순서가 뒤죽박죽인걸 확인할 수 있는데, 출력값이 Complete와 Failed가 구분되었기 때문입니다.
--confirm 옵션을 넣지 않았기 때문에 실제로 값이 적용되진 않습니다.

사용 예) confirm 옵션 값 적용
$ oc adm prune builds --keep-failed=1 --keep-complete=5 --orphans --confirm
web-test1        httpd-web-3
web-test1        httpd-web-2
web-test1        httpd-web-4

$ oc adm prune builds --keep-failed=1 --kepp-complete=5 --orphans
Dry run enabled - no modifications will be made. Add --confirm to remove builds

confirm 옵션을 넣게되면 출력됐던 모든 값들이 제거됩니다.

## OpenShift ConfigMap 정리

안녕하세요.

컨피그맵은 컨테이너에서 필요한 환경설정 내용을 컨테이너와 분리해서 제공해주는 기능입니다. 애플리케이션을 배포하다 보면 환경에 따라서 다른 설정 값을 사용하는 경우가 있는데, 애플리케이션 이미지는 같지만 환경 변수가 차이 나는 경우 매번 다른 컨테이너 이미지를 만드는 것은 관리상 불편할 수 밖에 없는데, 컨피그맵은 컨테이너와 환경설정 컨피그맵으로 분리해 둠으로써 하나의 이미지로 개발 / 운영이 가능해집니다.

컨피그맵은 암호화되지 않는 정보, 보통 애플리케이션의 설정 값으로 배포할 때 사용됩니다. 클러스터에 민감한 정보를 사용하려면 "Secret" 기능을 이용해야 합니다.

ConfigMap에 정의해놓은 값을 Pod로 넣는 방법은 크게 두가지가 있습니다.
 - 정의한 값을 Pod의 환경 변수 (Environment variable)로 넘기는 방법
 - 정의한 값을 Pod의 디스크 볼륨으로 마운트 하는 방법

Configmap 작성
명령어를 사용하여 ConfigMap 디렉토리, 특정 파일 또는 literal 값을 넣어 작성할 수 있습니다.

Directory 방식
데이터를 채울 데이터가 포함된 파일이 있는 디렉토리를 ConfigMap으로 만드는 방식입니다.
hello.properties와 lds.properties 라는 파일이 있고 파일의 내용이 아래와 같습니다.

$ ls example-files
hello.properties
lds.properties

$ cat example-files/hello.properties
myname=lds
email=lds@linuxdata.co.kr
address=samseong

$ cat example-files/lds.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

디렉토리 방식으로 Configmap을 만들 땐 --from-file 옵션으로 디렉토리를 지정해주면 됩니다.
디렉토리에 있는 각 파일의 이름이 Key가 되고 Value는 파일의 내용이 됩니다.

$ oc create configmap hello-config \
--from-file=example-files/

하나의 ConfigMap에 여러 개의 키가 생성된 것을 확인할 수 있습니다.
(describe 옵션을 사용하였을 땐 키의 이름과 크기만 표시됩니다)
$ oc describe configmaps hello-config
Name:                hello-config
Namespace:      default
Labels:              <none>
Annotations:    <none>

Data
hello.properties:    54 bytes
lds.properties:            83 bytes

키 값을 보려면 oc get다음 -o 옵션을 사용 하여 객체를 만들 수 있습니다.
(ex oc get configmaps hello-config -o yaml)

File 방식
설정을 파일 형태로 해서 Pod에 공유하는 방법도 있습니다.
hello.properties 라는 파일이 있고 파일의 내용이 아래와 같습니다.
myname=lds
email=lds@linuxdata.co.kr
address=samseong

파일을 이용해서 ConfigMap을 만들때는 아래와 같이 --from-file 을 이용해서 파일명을 넘겨주면 됩니다.
파일을 이용해서 ConfigMap을 생성하면, 아래와 같이 키는 파일명이 되고, 값은 파일 내용이 됩니다.

$ oc create configmap hello-world \
    --from-file=example-files/hello.properties

Literal 방식
문자로 간단하게 생성하는 방법 키가 “hello” 로 하고 값이 “world인 config 맵을 생성합니다.
$ oc create configmap hello-config \
    --from-literal=hello=world

또는 YAML 파일로도 작성할 수 있습니다.

hello-world.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hello-config
data:
  hello: world

데이터 항목에 [Key] : [Value] 형식으로 라인을 추가하면 여러개의 값을 하나의 ConfigMap에 저장할 수 있습니다.

## New features and enhancements 문서 둘러 보기

안녕하세요.

OpenShift 4 문서를 둘러 보면 관련 의견을 공유하고자 작성해 보았습니다.

이번 주제는
New features and enhancements
    https://docs.okd.io/latest/whats_new/new-features.html


Installation and upgrade
    Upgrading from 3.x to 4 is currently not available. You must perform a new installation of OpenShift v4.
        OpenShift 3 Version에서 4 Version으로 업그레이드 지원 안함

OpenShift SDN
    The default mode is now NetworkPolicy.
    < OpenShift 3기준에서는 Project 기반으로 나누어진것에서 바뀐것으로 보임

Number of pods per node
    3.x tested maximum
        250
    4.x tested maximum
        500
    4 Version에서 Node당 Pod 갯수가 늘어난것을 확인 할수 있음

OKD tested cluster maximums for major releases
    https://docs.okd.io/latest/scalability_and_performance/planning-your-environment-according-to-object-maximums.html#planning-your-environment-according-to-object-maximums
    

## OpenShift Metrics-Server 배포


안녕하세요.

Horizontal Pod Autoscaler에 대해서 알아보려고 합니다.
OpenShift 에서 Horizontal Pod Autoscaler(HPA)를 이용하여 설정한 CPU 사용률 기반으로 Replicaset, Deployment 의 Pod 수를 자동으로 Scaling 할 수 있습니다.
Horizontal Pod Autoscaler는 크기를 조정할 수 없는 오브젝트(예: 데몬셋(DaemonSet))에는 적용되지 않습니다.

HPA를 하기 위해서는 Pod의 부하에 대해 모니터링및 수집을 하는 Metrics-Server가 필요합니다.
기본적으로 OKD 설치 시 사용되는 Ansible 스크립트에는 Metrics Server 설치가 Flase 설정되어 있으므로, 설치 시 옵션값을 변경하거나, 수동으로 설치해야합니다.

OpenShift에 Metrcis-Server를 배포하는 방법에 대해 소개하겠습니다.

* Metrics의 데이터를 저장하기 위해 Pod의 볼륨 또는 Persistent Volume을 생성해야 합니다.

1. Metrcis storage 준비
 1) 마운트할 디렉토리 생성
 $ mkdir /exports

 2) nfs 설정
 $ vi /etc/exports
 /exports *(rw,no_root_squash)

 3) nfs 서비스 활성화
 $ systemctl restart nfs-server
 $ exportfs

2. Persistent Volume 생성
 1) pv 생성
 $ cat pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  creationTimestamp: null
  finalizers:
  - kubernetes.io/pv-protection
  labels:
    storage: metrics
  name: metrcis-canssandra
  namespace: "openshift-infra"
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 30Gi
  nfs:
    path: /exports/metrics
    server: 192.168.50.100
  persistentVolumeReclaimPolicy: Retain
status: {}
 
 2) pvc 생성
 $ cat pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: null
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    storage: metrics
  namespace: "openshift-infra"
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
    selector:
      matchLabels:
        project: openshift-infra
  volumeName: metrics-cassandra-1
status: {}

3. Ansible 인벤토리 설정
...
# Cluster Metrics
openshift_metrics_install_metrics=true
openshift_metrics_cassandra_nodeselector={'node-role.kubernetes.io/infra': 'true'}
openshift_metrics_hawkular_nodeselector={'node-role.kubernetes.io/infra': 'true'}
openshift_metrics_heapster_nodeselector={'node-role.kubernetes.io/infra': 'true'}
openshift_metrics_install_hawkular_agent=true
openshift_metrics_server_install=true

# Metrics NFS
openshift_metrics_storage_labels={'storage': 'metrics'}
openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_host=192.168.211.147
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=50Gi
...

4. Playbook 실행
$ ansible-playbook -i </path/inventory> /usr/share/ansible/openshift-ansible/playbooks/openshift-logging/config.yml -vv

참고 URL
https://docs.okd.io/3.11/install_config/cluster_metrics.html

## OpenShift_Registry_Storage

안녕하세요.

OKD에는 OCR(OpenSHift Container Registry)라는 통합 컨테이너 레지스트리가 존재합니다.
통합 레지스트리는 새 이미지가 푸시 될 때마다 네임스페이스 이름 및 이미지 메타 데이터와 같은 데이터를 OKD에 전달하고 OKD는 새 이미지에 반응해 빌드 및 배포를 합니다.

OCR은 초기 설치 시 자동으로 배치 되는데, Volume 형식이 emptyDir로 되어있어 노드에서 파드가 제거되면 데이터가 영구적으로 삭제됩니다.
OCR에 기존 데이터를 백업하고 데이터를 영구 저장하기 위해 PV(Persistent Volume)을 붙여 복원하는 테스트 결과를 공유하려고 합니다.

* Platform Version : OpenShift 3.9

# oc -n default get dc/docker-registry -o yaml
...
        volumeMounts:
        - mountPath: /registry
          name: registry-storage
        - mountPath: /etc/secrets
          name: registry-certificates
...
      volumes:
      - emptyDir: {}
        name: registry-storage
...

=== registry data backup ===
# oc rsync <docker-registry pod-name>:/registry ./<destination-dir-name>

=== pv 및 pvc 생성 ===
# oc -n default create -f pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  creationTimestamp: null
  finalizers:
  - kubernetes.io/pv-protection
  labels:
    deploymentconfig: docker-registry
  name: registry-1
  namespace: "default"
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 30Gi
  nfs:
    path: /exports/registry
    server: 192.168.211.147
  persistentVolumeReclaimPolicy: Retain
status: {}

# oc -n default create -f pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: null
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    deploymentconfig: docker-registry
  name: registry
  namespace: "default"
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
    selector:
      matchLabels:
        project: default
  volumeName: registry-1
status: {}

=== Registry scale down 후 Volume 수정 ===
# oc -n default scale dc/docker-registry --replicas=0

# oc -n default edit dc/docker-registry

...
      - name: registry-storage
        persistentVolumeClaim:
          claimName: <pvc-name>
...

=== Registry scale up ===
# oc -n default scale dc/docker-registry --replicas=1

=== 기존 데이터 복원 ===
# oc -n default rsync registry/docker <docker-registry pod-name>:/registr

참고 URL
https://docs.okd.io/3.9/dev_guide/volumes.html#adding-volumes
https://docs.okd.io/3.9/dev_guide/volumes.html#removing-volumes



















