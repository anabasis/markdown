# Administer Splunk User Behavior Analytics

## Administer and Manage Splunk UBA

Tasks you can perform to administer Splunk UBA
As a Splunk UBA admin, you can perform the following:

Task	More Information
Customize functionality in Splunk UBA to produce more relevant threats and anomalies for your analysts.	
Monitor policy violations with custom threats.
Take action on anomalies with anomaly action rules.
Customize anomaly scoring rules.
Use the command line to configure Splunk UBA and start and stop services.	
Start and stop Splunk UBA services from the command line.
Manage Splunk UBA configuration properties in the uba-site.properties file.
Manage the users and servers in your deployment and configure authentication options.	
Manage user accounts and account roles in Splunk UBA.
Configure authentication for Splunk platform users.
Configure authentication using single sign-on.
Configure warm standby, restore from automated incremental backups, and migrate configurations to larger clusters.	
Configure warm standby in Splunk UBA.
Prepare automated incremental backups in Splunk UBA.
Migrate Splunk UBA using the backup and restore scripts.
Add different types of data to Splunk UBA.	See Which data sources do I need? in the Get Data into Splunk User Behavior Analytics manual.
Monitor the health of your Splunk UBA deployment.	
Use the Splunk UBA Monitoring App to monitor the health of your Splunk UBA deployment from the Splunk platform. See About the Splunk UBA Monitoring app in the Splunk UBA Monitoring App manual.
Monitor the health of your Splunk UBA deployment.
Configure integrations with other systems and send data to other systems.	
See How Splunk UBA sends and receives data from the Splunk platform in the Send and Receive Data from the Splunk Platform manual.
Send Splunk UBA data to Splunk Enterprise without Splunk Enterprise Security.
Send Splunk UBA threats to analysts using email.
Send threats from Splunk UBA to ServiceNow.

Determine which version of Splunk UBA you are running
Find the version of Splunk UBA that you are running, and provide this information when you need to contact Customer Support.

Find your Splunk UBA version from the Splunk UBA web interface
To find your Splunk UBA version using the Splunk UBA web interface, perform the following tasks:

From the home page in Splunk UBA, scroll to the bottom and click About.
The following popup window appears:
This screen image shows the About UBA popup window, which details the installed platform and content versions of Splunk UBA. The relevant fields are described in the following text.
The Application Version shows the latest platform version that is installed. The Subscription Content shows the latest content release that is installed. See About Splunk UBA release types for more information about the different types of Splunk UBA releases.
Click the X at the top-right corner of the window to dismiss the window.
Find your Splunk UBA version from the CLI
If the Splunk UBA web interface is not available, you can also find your Splunk UBA version from the CLI. Perform the following tasks:

SSH to the Splunk UBA server as the caspida user.
Run the following command:
cat /opt/caspida/conf/version.properties
For example:

caspida@ubahost-001:~$ cat /opt/caspida/conf/version.properties
git-version=b3f9a422e58b178b713b42186541b41d3487c762
release-number=4.2.0
build-number=3
release-version=4.2.0-20180927-000003
caspida@ubahost-001:~
You can also find your Splunk UBA version by running the health check script:

SSH to the Splunk UBA server as the caspida user.
Run the following command:
/opt/caspida/bin/utils/uba_health_check.sh
At the end of the output, you will see the operating system and Splunk UBA version. For example:

current os:               'Red Hat Enterprise Linux Server release 7.5 (Maipo)'
current uba:              '4.2.0-20180927-000003'
current content:          '4.2.0-20180927-000003'
current db:               '4.2'


Start and stop Splunk UBA services from the command line
Use some common command line interface (CLI) commands to perform administrative tasks in Splunk UBA.

To run these commands, log in to the Splunk UBA management node as the caspida user.

Task	CLI Commands
Stop and start the Splunk UBA web interface.	Run the following commands on the management node:
sudo service caspida-ui stop
sudo service caspida-ui start
Stop and start the resource monitor services.	Run the following commands on the management node:
sudo service caspida-resourcesmonitor stop
sudo service caspida-resourcesmonitor start
You can also tail the resource monitor log files to help you troubleshoot:

tail -f /var/log/caspida/monitor/resourcesMonitor.out
Synchronize configuration changes to all nodes in a distributed deployment.	In any distributed deployment, changes to the /etc/caspida/local/conf/uba-site.properties file must be synchronized to all nodes in the cluster. To do this, run the following command on the management node:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
See Manage Splunk UBA configuration properties in the uba-site.properties file for information about setting Splunk UBA configuration properties.

Stop and start Splunk UBA services only on all nodes. The following services are stopped:
kafka-server
caspida-jobmanager
caspida-eventstore
caspida-outputconnector
caspida-jobagent
caspida-ui
caspida-offlinerulexec
caspida-realtimetuleexec
caspida-resourcemonitor
caspida-sysmon
spark-master
spark-worker
spark-history
Run the following command on the management node:
/opt/caspida/bin/Caspida stop
/opt/caspida/bin/Caspida start
Stop and start Splunk UBA services (listed with the /opt/caspida/bin/Caspida stop/start command) and all dependent platform services on all nodes:
zookeeper-server
hadoop-hdfs-namenode
hadoop-hdfs-datanode
hadoop-hdfs-secondarynamenode
influxdb
postgresql
redis-server
hive-metastore
impala-state-store
impala-catalog
impala-server
docker
kubelet
Run the following command on the management node:
/opt/caspida/bin/Caspida stop-all
/opt/caspida/bin/Caspida start-all
Stop and start the Splunk UBA containers.	Run the following command on the management node:
/opt/caspida/bin/Caspida stop-containers
/opt/caspida/bin/Caspida start-containers
Stop and start the Splunk UBA data sources.	Run the following command on the management node:
/opt/caspida/bin/Caspida stop-datasources
/opt/caspida/bin/Caspida start-datasources
Check the version number of your Splunk UBA packages.	Run the following command on Ubuntu systems:
wget --version
Run the following command on other supported Linux systems:

rpm -qa | grep wget
Get a list of the nodes in your Splunk UBA cluster.	
grep caspida.cluster.nodes /opt/caspida/conf/deployment/caspida-deployment.conf

Manage Splunk UBA configuration properties in the uba-site.properties file
Configure Splunk UBA using the properties in the /etc/caspida/local/conf/uba-site.properties file. Customizations made in this file are not modified during any upgrade procedures. See How to set configuration properties in Splunk UBA.

Configure Splunk UBA properties for the following product areas:

Splunk UBA environment properties
Splunk UBA and Splunk Enterprise Security (ES) properties
Event drilldown properties
Raw event data ingestion properties
Asset and identity data ingestion properties
Kafka data ingestion properties
Anomaly and threat properties
Backup and restore properties
In the tables in each section, the values in the Default behavior column indicate the default Splunk UBA behavior when a configuration property is not set.

How to set configuration properties in Splunk UBA
A file called /opt/caspida/conf/uba-default.properties is used by Splunk UBA to manage many of the processes and micro-services required to operate Splunk UBA. To edit any of these default properties, or to add new properties, copy this file to /etc/caspida/local/conf/uba-site.properties file. Only edit the uba-site.properties file when changes are required. The /etc/caspida/local/conf directory is not affected by any upgrade scripts so configuration changes in this location can persist across product upgrades.

Perform the following steps to edit the /etc/caspida/local/conf/uba-site.properties and have the changes take effect:

Log in to the Splunk UBA management node as the caspida user.
Edit the /etc/caspida/local/conf/uba-site.properties file and add or edit the desired property and value.
Save and exit the file.
Synchronize the configuration changes across the cluster:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Stop and restart Caspida.
/opt/caspida/bin/Caspida stop
/opt/caspida/bin/Caspida start
Splunk UBA environment properties
This table lists the configuration properties affecting your Splunk UBA setup.

Property	Description	Default behavior
system.docker.networkcidr	Use this property to customize the IP addresses of your Docker containers to avoid conflicts in your network.

See Change the IP address of your Docker containers.

Not set.
ui.idleTimeout	Use this property to change or disable the timeout value for the Splunk UBA web interface.

See Disable the Splunk UBA web interface timeout.

30 minutes
Health monitor indicators	Many health monitor indicators have configurable properties that allow you change the threshold at which a warning or error is generated.

See Health Monitor status code reference.

Varies.
Splunk UBA and Splunk Enterprise Security integration properties
This table lists the configuration properties for Splunk UBA and Splunk Enterprise Security (ES) integration.

Property	Description	Default behavior
uba.splunkes.integration.enabled	Define whether or not Splunk UBA integration with Splunk ES is enabled.

See Send Splunk UBA anomalies and threats to Splunk Enterprise Security as notable events in the Send and Receive Data from the Splunk Platform manual.

true
uba.splunkes.retry.delay.minutes	Configure how often Splunk UBA sends threats to Splunk ES.

See How threats and notables are synchronized in the Send and Receive Data from the Splunk Platform manual.

5 minutes
uiServer.host	The name of the Splunk UBA server specified when running the /opt/caspida/bin/Caspida setup command during Splunk UBA installation must match the value stored in the uiServer.host property in the /etc/caspida/local/conf/uba-site.properties file in Splunk UBA.

See Splunk Enterprise and Splunk ES requirements in the Send and Receive Data from the Splunk Platform manual.

N/A
uba.sys.audit.push.splunk.enabled	Set this property to true to enable Splunk UBA audit events to be sent to Splunk ES.

See Send audit events to Splunk ES in the Send and Receive Data from the Splunk Platform manual.

Not set.
identity.resolution.export.enabled	Set this property to true to send user and device association data from Splunk UBA to Splunk ES. User and device association data from Splunk UBA is visible on the Session Center dashboard in Splunk ES.

See Set up Splunk UBA to send user and device association data to Splunk ES in the Send and Receive Data from the Splunk Platform manual.

true
Event drilldown properties
This table lists the configuration properties for using event drilldown in Splunk UBA.

Property	Description	Default behavior
triggering.event.pre.calculate.links.anomaly.threshold	Adjust the anomaly score threshold for caching the SPL to retrieve contributing anomalies.
See Splunk UBA caches the SPL for important anomalies in Use Splunk User Behavior Analytics.

8
triggering.event.timeout.millis	Timeout value for the SPL in retrieving an anomaly's contributing events.

See Configure properties to increase the timeout interval in Use Splunk User Behavior Analytics.

300000
triggering.event.enable.reverse.ir	Whether or not to enable reverse IR.

See Configure properties to increase the timeout interval in Use Splunk User Behavior Analytics.

false
triggering.event.search.backend.submission	Submit the generated SPL to the Splunk platform using same credentials as the one used to create the data source.

See Working with long URLs in Use Splunk User Behavior Analytics.

true
Raw event data ingestion properties
This table lists the configuration properties for Splunk UBA to ingest raw events from the Splunk platform.

Property	Description	Default behavior
splunk.live.micro.batching	Splunk UBA ingests data from the Splunk platform by performing micro batch queries.

See How data gets in to Splunk UBA in Get Data into Splunk User Behavior Analytics.

true
splunk.live.micro.batching.delay.seconds	Define the point in time where Splunk UBA begins data ingestion.

See How data gets in to Splunk UBA in Get Data into Splunk User Behavior Analytics.

180
splunk.live.micro.batching.interval.seconds	The length of time for each micro batch query.

See How data gets in to Splunk UBA in Get Data into Splunk User Behavior Analytics.

60 seconds
connector.splunk.max.backtrace.time.in.hour	The window of time that determines when to begin data ingestion, especially after a data source is stopped and then restarted.

See How data gets in to Splunk UBA in Get Data into Splunk User Behavior Analytics.

4 hours
parser.global.input_timezone	Set the time zone you want to use when ingesting events, in particular for file-based data sources.

See Add file-based data sources to Splunk UBA in Get Data into Splunk User Behavior Analytics.

UTC
Asset and identity data ingestion properties
This table lists the configuration properties for Splunk UBA to ingest asset and identity data.

Property	Description	Default behavior
attribution.keyvalue.delimiter	The delimiter to use when ingesting assets data with multi-values fields.

See Configure asset ingestion for multi-valued fields in Get Data into Splunk User Behavior Analytics.

Comma (,)
assets.proxy.query.adformat	Specify whether Splunk UBA should use MULTILINE or XML format when querying Windows Security Event logs for proxy servers.

See Perform asset identification by using the Splunk Assets data source in Get Data into Splunk User Behavior Analytics.

MULTILINE
identity.resolution.blacklist.threshold.device.hostnamecount	To help Splunk UBA identify multi-user systems, data from last 24 hours is analyzed to find occurrences of more than 2 device mappings per hour for more than 6 hours. Edit this property to change the number of device mappings.

See View IDR exclusion lists in Splunk UBA in Get Data into Splunk User Behavior Analytics.

2
identity.resolution.blacklist.threshold.device.hostnamehours	To help Splunk UBA identify multi-user systems, data from last 24 hours is analyzed to find occurrences of more than 2 device mappings per hour for more than 6 hours. Edit this property to change the number of consecutive hours.

See View IDR exclusion lists in Splunk UBA in Get Data into Splunk User Behavior Analytics.

6
identity.resolution.hrcache.capacity	Set the value of this property to three times the number of HR accounts being monitored by Splunk UBA to avoid potential performance issues.

See Set the HR data cache capacity in the Get Data into Splunk User Behavior Analytics manual.

300,000
Kafka data ingestion properties
This table lists the configuration properties related to anomalies and threats in Splunk UBA.

For additional documentation about these properties, see Configure Kafka data ingestion in the Splunk UBA Kafka Ingestion App manual.

Property	Description	Default behavior
splunk.kafka.ingestion.search.delay.seconds	The point in time where Splunk UBA begins Kafka ingestion.	180 seconds
splunk.kafka.ingestion.search.interval.seconds	The length of the time in seconds for each batch query.	60 seconds
splunk.kafka.ingestion.search.max.lag.seconds	The maximum, lag, or amount of time between the end time of the most recent batch query and the time Kafka ingestion starts.	3600 seconds
Anomaly and threat properties
This table lists the configuration properties related to anomalies and threats in Splunk UBA.

Property	Description	Default behavior
entity.score.lookbackWindowMonths	Entity scoring is based on anomalies and threats from the past 2 months. Configure this property to change the time window.

See Filter the scope of anomalies and threats in Use Splunk User Behavior Analytics.

2 months
persistence.anomalies.trashed.maintain.days	Splunk UBA purges anomalies more than 90 days old. Configure the property to change this value.

See Splunk UBA cleans up old anomalies in the trash in User Splunk User Behavior Analytics.

90 days
persistance.anomalies.trashed.del.limit	Splunk UBA removes batches of 300,000 anomalies when purging old anomalies. Configure the property to change the batch size.

See Splunk UBA cleans up old anomalies in the trash in User Splunk User Behavior Analytics.

300,000
rule.engine.process.timeout.min	The number of minutes allowed for a threat rule to run and complete before it times out.

See Manage the number of threats and anomalies in your environment in User Splunk User Behavior Analytics.

60
Backup and restore properties
This table lists the configuration properties related to backup and restore in Splunk UBA.

For additional documentation about these properties, see Backup and restore Splunk UBA using automated incremental backups.

Property	Description	Default behavior
backup.filesystem.full.interval	The frequency with which Splunk UBA performs an automated full backup without stopping Splunk UBA.	1 week
backup.filesystem.enabled	Set this property to designate whether or not automated backups are enabled on the system.	true
backup.filesystem.directory	Set this property to designate the location where the automated backups are stored.	/backup
Warm standby properties
This table lists the configuration properties related to warm standby in Splunk UBA.

For more information about these properties, see Set up the standby Splunk UBA system.

Property	Description	Default behavior
replication.enabled	Set this property to enable the primary system to synchronize with the standby system.	Not set
replication.primary.host	Specify the management node of the primary Splunk UBA cluster.	Not set
replication.standby.host	Specify the management node of the standby Splunk UBA cluster.	Not set
Custom content properties
This table lists the configuration properties related to custom models and cubes in Splunk UBA.

For more information about these properties, see Set limits for the number of custom models, cubes, measures and dimensions in Splunk UBA in the Develop Custom Content in Splunk User Behavior Analytics manual.

Property	Description	Default behavior
custom.cubes.non.deleted.max	The maximum number of custom cubes that can be created.	6
custom.cubes.dimensions.max	The maximum number of dimensions allowed in a custom cube.	6
custom.cubes.measures.max	The maximum number of measures allowed in a custom cube.	3
custom.models.enabled.max	The maximum number of active custom models allowed.	6


When jobs run in Splunk UBA
Jobs and processes in Splunk UBA run at various times.

Time	Frequency	Job or Process
12:00AM	Daily	Anomaly Purger
12:00AM	Daily	Batch models
12:00AM	Weekly each Saturday	VirusTotal script
2:00AM	Daily	HR data refreshed
6:00AM	Daily	IDR exclusion list updates
N/A	Hourly, daily, or weekly from the time the data source is configured	Assets data
N/A	Every 5 minutes	ES Notable Events sync
N/A	Every 30 minutes	View Event Drilldown links


Where services run in Splunk UBA
View /opt/caspida/conf/deployment/caspida-deployment.conf to see where services are running in your deployment.

To see where services run in other deployments, view the .conf files in the /opt/caspida/conf/deployment/recipes directory. For example, to see where services run in a 7-node deployment, see the /opt/caspida/conf/deployment/recipes/deployment-7_node.conf file:

##
# Copyright (C) 2005-2016 Splunk Inc. All rights reserved.
# All use of this Software is subject to the terms and conditions accepted upon installation of
# and/or purchase of license for the Software.
##

#
# 7-node deployment configuration
# Replace node1,node2..node7 with respective ipaddress/hostname during
# deployment
#
caspida.cluster.nodes=node1,node2,node3,node4,node5,node6,node7
zookeeper.servers=node1,node2,node3
hadoop.namenode.host=node1
hadoop.snamenode.host=node2
hadoop.datanode.host=node4,node5,node6,node7
persistence.datastore.tsdb=node1:8086
database.host=node1
#database.standby=node8
persistence.redis.server=node4,node5
hive.host=node1
spark.master=node7:7077
spark.worker=node7
spark.history=node7
spark.server=node7
impala.statestore.host=node1
impala.catalog.host=node1
kafka.brokers=node2:9092
kafka.ssl.brokers=node2:9093
impala.server.host=node1
uiServer.host=node1
jobmanager.restServer=node1:9002
jobmanager.agents=node2
rule.offline.exec.host=node1
rule.realtime.exec.host=node1
sysmonitor.host=node1
resourcesmonitor.host=node1
output.connector.host=node1
kubernetes.restServer=node1:6443
system.network.interface=eth0
container.master.host=node1
container.worker.host=node3,node4,node5,node6
The node numbers represent the Splunk UBA host names in the order they were specified when running the /opt/caspida/bin/Caspida setup command during setup. For example, if you specified ubahost1,ubahost2,ubahost3,ubahost4,ubahost5, then node 1 corresponds to ubahost1, node 2 corresponds to ubahost2, and so on.

You can view the order in which the host names were entered by performing the following stepss:

Log in to any Splunk UBA node as the caspida user.
Run the following command:
grep caspida.cluster.nodes /opt/caspida/conf/deployment/caspida-deployment.conf
Below is a sample output of this command:

caspida.cluster.nodes=ubahost1,ubahost2,ubahost3,ubahost4,ubahost5




## Manage User Accounts and Access to Splunk UBA

Manage user accounts and account roles in Splunk UBA
Add user accounts to Splunk User Behavior Analytics.

Understand account roles
Each user account is associated with a role in Splunk UBA that defines the user's level of access and privileges in the system.

The following types of user account roles exist in Splunk UBA:

Admin (uba_admin)
Analyst (uba_analyst)
Content_Developer (uba_content_developer)
PII_Unmask (uba_pii_unmask)
User (uba_user)
To add additional roles, you can create custom roles or clone existing roles. See Create a custom role or Clone an existing role.

To view account roles, perform the following tasks:

In Splunk UBA, select Manage > UBA Accounts.
Select Account Roles
Click on the elipsis icon in the role to view the default privileges associated with each role.
The default privileges for each role have the following permissions:

UBA Role	User	PII_Unmask	Content_Developer	Analyst	Admin
Anomalies	View	View	View	View	View/Edit
Anomaly Rules	View		View	View	View/Edit
Assets	View		View	View	View/Edit
Audit Logs					View
Black/White Lists	View		View	View	View/Edit
Cluster					View/Edit
Cubes	View	View	View/Edit	View	View
Data Sources	View		View	View	View/Edit
Diagnostics			View	View	View
Event Filters					View/Edit
HR Data					View/Edit
IDR Exclusions	View	View	View	View	View/Edit
License	View	View	View	View	View/Edit
Models	View	View	View/Edit	View	
Output Connectors	View		View	View	View/Edit
PII Masking Settings (for defining global PII masking settings)					View/Edit
PII Unmask (to unmask PII for users assigned to this role)		View			View
Service Apps					View
Subscription Content	View	View	View	View	View/Edit
System Settings					View/Edit
Threat Rules	View		View	View	View/Edit
Threats	View	View	View/Edit	View/Close	View/Close
User Accounts					View/Edit
Watchlists	View	View	View/Edit	View/Edit	View/Edit
In order for a user to have access to PII Masking Settings, the user must also have access to System Settings.

Create a custom role
Create a custom role to grant or restrict specific privileges, in the event that the default UBA roles do not provide enough granularity for your needs. For example, you can create a custom admin with full admin privileges but restrict the ability to create or edit user accounts.

To create a custom role:

Select Manage > UBA Accounts.
Click Account Roles.
Select New Account Role.
Specify a name for the role.
In the remainder of the screen, select the desired privileges for each target area. All users associated with this role will have the specified privileges.
Click OK to create the role.
To configure a Splunk platform user to log in to Splunk UBA using this role, you must configure the role with the exact name on the Splunk platform. It is a good idea to begin all UBA roles with uba_ to match the default UBA roles uba_user, uba_analyst, and uba_admin. See Configure authentication for Splunk platform users.

When creating a new role in the Splunk platform, you must first select the uba_user role in the Inheritance section of the page. After the new role is created, it can be assigned to any user in the Splunk platform.

To configure a role for single sign-on (SSO) authentication, you must configure the role with the exact name as the group name in your SSO identity provider.

Clone an existing role
To clone an existing role:

Select Manage > UBA Accounts.
Click Account Roles.
Select the the clone icon icon in the role you want to clone.
Change the name for the role as desired.
In the remainder of the screen, select the desired privileges for each target area. All users associated with this role will have the specified privileges.
Click OK to clone the role.
To configure a Splunk platform user to log in to Splunk UBA using this role, you must configure the role with the exact name on the Splunk platform. See Configure authentication for Splunk platform users.

When cloning a role in the Splunk platform, you must first select the uba_user role in the Inheritance section of the page. After the role is cloned, it can be assigned to any user in the Splunk platform.

Add a local user account
To create a new local user account:

Select Manage > UBA Accounts.
Click New UBA Account.
Enter a Username.
Type a password and confirm the password.
Select a Role for the account.
Click the checkbox in Allow PII Unmasking if you want this user to be able to view PII.
See Disable PII masking for specific users in Splunk UBA for more information.
Click OK to create the account.


Configure authentication for Splunk platform users
Configure how Splunk platform users are authenticated when accessing Splunk UBA.

Configure load balancing for persistent sessions
Use a third-party hardware or software load balancer in front of your set of clustered search heads to access the set of search heads through a single interface, without needing to specify a particular one. Configure the load balancer so that user sessions are "sticky" or "persistent" to remain on a single search head throughout the session. See Use a load balancer with search head clustering in the Splunk Enterprise Distributed Search manual.

Configure Splunk authentication using Splunk UBA
Perform the following tasks to configure Splunk authentication using Splunk UBA:

On the Splunk platform, create the same roles that exist in Splunk UBA. For first-time deployments, you must create the uba_user, uba_analyst, and uba_admin roles, along with any other custom roles created in Splunk UBA. There must be a one-to-one mapping of roles between the Splunk platform and Splunk UBA, and the role names must match. Role names are case-insensitive, so a role called uba_testRole on the Splunk platform maps to uba_testrole in Splunk UBA. To learn more about creating users and roles in the Splunk platform, see About users and roles.
When creating a new role in the Splunk platform, you must first select the uba_user role in the Inheritance section of the page. After the new role is created, it can be assigned to any user in the Splunk platform.

When testing authentication with the Splunk platform, the user account being used for testing must also have one of the uba_user, uba_analyst, or uba_admin roles assigned to it.

In Splunk UBA, select Manage > Settings.
Verify the Authentication tab is selected (by default).
Select UBA Authentication to have your Splunk UBA instance authenticate users.
Select Splunk Authentication to have your Splunk instance perform user authentication. You are prompted to provide additional information:
Host name and port of your Splunk instance. If search head clustering is configured and a load balancer is available, it is recommended to specify the load balancer host name to avoid a single point of failure. Ensure that port 8089 is accessible on the load balancer.
By default only the Splunk accounts with the uba_user role can log in as UBA users. If the Splunk Users option is selected, Splunk accounts with the user role can also log in as UBA users.
By default only the Splunk accounts with the uba_admin role can log in as UBA admins. If the Splunk Admins option is selected, Splunk accounts with the admin role can also log in as UBA admins.
Select both Splunk Users and Splunk Admins and click Test Connection to verify that the connection with your Splunk instance is working.
Click OK to save your changes.
Configure Splunk authentication using the CLI
If you do not want to create new roles in the Splunk platform, set the allowSplunkUserRole and allowSplunkAdminRole settings to true to allow users with the Splunk platform user role or admin role, respectively, to log in to Splunk UBA from the Splunk platform.

If you configure Splunk Authentication by using Splunk UBA, this configuration overrides any setting made using the CLI.

Log in to the Splunk UBA management server as the caspida user using SSH.
Open the /etc/caspida/local/conf/uba-site.properties file.
Edit or create the ui.splunk.authentication setting to match the following example:
ui.splunk.authentication={"hostname": "<SplunkServer>", "port": "8089", "allowSplunkUserRole": true, "allowSplunkAdminRole": false}
Set allowSplunkUserRole to true to allow users with the user role in the Splunk platform to view data from Splunk UBA in the Splunk platform. Replace <SplunkServer> with the Splunk search head host name. If search head clustering is configured and a load balancer is available, it is recommended to specify the load balancer host name to avoid a single point of failure. Ensure that port 8089 is accessible on the load balancer.

Configure authentication using single sign-on
Integrate Splunk UBA with your existing authentication system using single sign-on (SSO). You can configure SSO in Splunk UBA with multiple identity providers.

To configure SSO for any supported identity provider using metadata files, see Configure SSO using metadata files.
If you want to configure SSO without using metadata files:
See Configure SSO with Ping Identity as your identity provider.
See Configure SSO with Okta as your identity provider.
See Configure SSO with ADFS as your identity provider.
See Configure SSO with OneLogin as your identity provider.
Required attributes
Splunk UBA requires the following attributes from your SSO identity provider:

SSO Attribute	Description
role	The list of groups to which the user is assigned. A user's role is used to map to the roles in your Splunk UBA instance's SAML configuration.
realName	Name of the user that will be used as the login display name.
mail	Email address of the user that will be used as the login display name.
If both realName and mail are provided, the email address is used as the login display name. If neither is provided, you will see "Unknown User".

Configure SSO using metadata files
Configure single sign-on for all identity providers using metadata files in your environment.

Log into Splunk UBA as a user with Admin privileges.
Create an account role that matches the group name in your identity provider. For example, if your identity provider user is assigned to the group uba_users, create an account role in Splunk UBA called uba_users (not case-sensitive). You can either create a new account role, or clone an existing one. All users in this group will have the permissions configured in this role. See Create a custom role or Clone an existing role.
Since User, Analyst, Admin, and Content_Developer are Splunk UBA roles by default, you do not need to create these account roles if your identity provider group names match these role names.

If the role is not properly configured in Splunk UBA, you will see the following error message:

"No permissions are granted to this username."
After the account role is created, select Manage > Settings.
Verify that Authentication is selected, then click on the SSO Authentication checkbox.
Click Download File to download the SP metadata file from Splunk UBA. Add this file to your SAML environment to connect it to Splunk UBA.
Click Select File to download or browse and select your metadata file, or copy and paste your metadata directly into the Metadata Contents field and click Apply. Refer to your identity provider documentation if you are not sure how to locate your metadata file.
Enter an entity ID in the EntityId field. This is an identifier for this Splunk UBA instance that is unique across all entities on the identity provider.
Review and verify the remaining fields on the page that are automatically populated from the metadata files.
Click OK.
Configure SSO with Ping Identity as your identity provider
To configure SSO for Splunk UBA with Ping Identity as your identity provider, make sure you have properly configured your Ping Identity environment, including:

Create a Service Provider connection on Ping Federate with "Browser SSO profile" as the Connection Type.
Select and export the signed certificate for Digital Signature Settings. Save this file to the /var/vcap/store/caspida/certs/idpcerts directory in Splunk UBA.
Import the Splunk UBA 3rd party/self-signed certificate as a Digital Verification Certificate.
Incorrectly importing the certificate may result in infinite login redirect loops. If you are seeing this behavior verify that the Splunk UBA 3rd party/self-signed certificate is imported correctly.

In Splunk UBA, perform the following tasks:

Log into Splunk UBA as a user with Admin privileges.
Create an account role that matches the Ping Identity group name. For example, if your Ping Identity user is assigned to the group uba_users, create an account role in Splunk UBA called uba_users (not case-sensitive). You can either create a new account role, or clone an existing one. All users in this group will have the permissions configured in this role. See Create a custom role or Clone an existing role.
Since User, Analyst, Admin, and Content_Developer are Splunk UBA roles by default, you do not need to create these account roles if your identity provider group names match these role names.

If the role is not properly configured in Splunk UBA, you will see the following error message:

"No permissions are granted to this username."
After the account role is created, select Manage > Settings.
Verify that Authentication is selected, then click on the SSO Authentication checkbox and complete the fields.
Field	Description
EntityId	An identifier for this Splunk UBA instance that is unique across all entities in your Ping Identity environment. For example, SplunkUBA.
IdP Certificate	Location and name of the PingIdentity certificate. This file is exported and located in the Splunk UBA certs/idpcerts directory. For example, /var/vcap/store/caspida/certs/idpcerts/ping.pem.
Private Key file	Full path and name of the Splunk UBA 3rd party certificate or self-signed certificate. The certificate must be located in the Splunk UBA certs directory or a subdirectory under the certs directory based on the current deployment settings. For example, /var/vcap/store/caspida/certs/mycerts/my-server.key.pem. See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics for more information about creating 3rd party or self-signed certificates.
Login Url	SSO application endpoint. Click Application Endpoints on the IdP Configuration menu to see a list of endpoints and descriptions applicable to your federation role. An SSO application endpoint has the format Ping URL + SSO endpoint + ?PartnerSpId=xxx, such as https://sso002.example.com:9031/idp/startSSO.ping?PartnerSpId=splunkuba01.
Login Callback Path	The location where the SAML assertion is sent with an HTTP POST. This is often referred to as the SAML Assertion Consumer Service (ACS) URL for your Splunk UBA instance. For example, if your identity provider is configured with https://uba/saml/acs, then specify /saml/acs in this field.
Logout Url	SLO application endpoint. Click Application Endpoints on the IdP Configuration menu to see a list of endpoints and descriptions applicable to your federation role. An SLO application endpoint has the format Ping URL + SLO endpoint + ?PartnerSpId=xxx, such as https://sso002.example.com:9031/idp/startSLO.ping?PartnerSpId=splunkuba01.
Logout Callback Path	The location where the logout response will be sent. For example, if your identity provider is configured with https://uba/saml/logout, then specify /saml/logout in this field.
Click OK.
Verify that you want to restart Splunk UBA for these changes to take effect. If yes, click OK to restart Splunk UBA.
Configure SSO with Okta as your identity provider
To configure SSO for Splunk UBA with Okta as your identity provider, make sure you have properly configured your Okta environment, including:

Added Splunk UBA as a new App with the Splunk UBA 3rd party/self-signed certificate uploaded
Configured the desired user groups
Downloaded the Okta X.509 certificate. Save this file to the /var/vcap/store/caspida/certs/idpcerts directory in Splunk UBA.
Then, perform the following tasks:

Log into Splunk UBA as a user with Admin privileges.
Create an account role that matches the Okta group name. For example, if your Okta user is assigned to the group uba_users, create an account role in Splunk UBA called uba_users. You can either create a new account role, or clone an existing one. All users in this group will have the permissions configured in this role. See Create a custom role or Clone an existing role.
Since User, Analyst, Admin, and Content_Developer are Splunk UBA roles by default, you do not need to create these account roles if your identity provider group names match these role names.

If the role is not properly configured in Splunk UBA, you will see the following error message:

"No permissions are granted to this username."
After the account role is created, select Manage > Settings.
Verify that Authentication is selected, then click on the SSO Authentication checkbox and complete the fields.
Field	Description
EntityId	An identifier for this Splunk UBA instance that is unique across all entities in your Okta environment. For example, SplunkUBA
IdP Certificate	Location and name of the Okta X.509 certificate. This file is downloaded from Okta and located in the Splunk UBA certs/idpcerts directory. For example, /var/vcap/store/caspida/certs/idpcerts/okta.pem.
Private Key file	Full path and name of the Splunk UBA 3rd party certificate or self-signed certificate. The certificate must be located in the Splunk UBA certs directory or a subdirectory under the certs directory based on the current deployment settings. For example, /var/vcap/store/caspida/certs/mycerts/my-server.key.pem. See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics for more information about creating 3rd party or self-signed certificates.
Login Url	Single sign-on URL of the identity provider.
Login Callback Path	The location where the SAML assertion is sent with an HTTP POST. This is often referred to as the SAML Assertion Consumer Service (ACS) URL for your Splunk UBA instance. For example, if your identity provider is configured with https://uba/saml/acs, then specify /saml/acs in this field.
Logout Url	Single logout URL of the identity provider.
Logout Callback Path	The location where the logout response will be sent. For example, if your identity provider is configured with https://uba/saml/logout, then specify /saml/logout in this field.
Click OK.
Verify that you want to restart Splunk UBA for these changes to take effect. If yes, click OK to restart Splunk UBA.
Configure SSO with ADFS as your identity provider
To configure SSO for Splunk UBA with ADFS as your identity provider, make sure you have properly configured your ADFS environment, including:

Add the Relying Party Trust with the Splunk UBA 3rd party/self-signed certificate uploaded
Manually generate and download the ADFS X.509 certificate.
In ADFS, go to ADFS > Endpoints.
Locate the FederationMetadata URL in the Metadata section. This URL can be accessed by using a browser to download/save the XML metadata into a file. An example URL is:
https://localhost/FederationMetadata/2007-06/FederationMetadata.xml
Get the unique content of the <X509Certificate> and use the content to create the certificate file called adfs.pem. Add this file to the /var/vcap/store/caspida/certs/idpcerts directory in Splunk UBA.
Then, perform the following tasks:

Log into Splunk UBA as a user with Admin privileges.
Create an account role that matches the ADFS group name. For example, if your ADFS user is assigned to the group uba_users, create an account role in Splunk UBA called uba_users. You can either create a new account role, or clone an existing one. All users in this group will have the permissions configured in this role. See Create a custom role or Clone an existing role.
Since User, Analyst, Admin, and Content_Developer are Splunk UBA roles by default, you do not need to create these account roles if your identity provider group names match these role names.

If the role is not properly configured in Splunk UBA, you will see the following error message:

"No permissions are granted to this username."
After the account role is created, select Manage > Settings.
Verify that Authentication is selected, then click on the SSO Authentication checkbox and complete the fields.
Field	Description
EntityId	An identifier for this Splunk UBA instance that is unique across all entities in your ADFS environment. For example, SplunkUBA
IdP Certificate	Location and name of the ADFS X.509 certificate you generated earlier. For example, /var/vcap/store/caspida/certs/idpcerts/adfs.pem.
Private Key file	Full path and name of the Splunk UBA 3rd party certificate or self-signed certificate. The certificate must be located in the Splunk UBA certs directory or a subdirectory under the certs directory based on the current deployment settings. For example, /var/vcap/store/caspida/certs/mycerts/my-server.key.pem. See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics for more information about creating 3rd party or self-signed certificates.
Login Url	Single sign-on URL of the identity provider.
Login Callback Path	SAML Assertion Consumer Endpoints path. For example, if your identity provider is configured with https://uba/saml/acs, then specify /saml/acs in this field.
Logout Url	Single logout URL of the identity provider.
Logout Callback Path	The SAML Logout Endpoints path. For example, if your identity provider is configured with https://uba/saml/logout, then specify /saml/logout in this field.
Click OK.
Verify that you want to restart Splunk UBA for these changes to take effect. If yes, click OK to restart Splunk UBA.
Configure SSO with OneLogin as your identity provider
To configure SSO for Splunk UBA with OneLogin as your identity provider, make sure you have properly configured your OneLogin environment, including:

Added Splunk UBA as a new app.
Configured the desired user groups.
Downloaded the OneLogin X.509 certificate. Save this file to the /var/vcap/store/caspida/certs/idpcerts directory in Splunk UBA.
Then, perform the following tasks in Splunk UBA:

Log into Splunk UBA as a user with Admin privileges.
Create an account role that matches the OneLogin group name. For example, if your OneLogin user is assigned to the group uba_users, create an account role in Splunk UBA called uba_users (not case-sensitive). You can either create a new account role, or clone an existing one. All users in this group will have the permissions configured in this role. See Create a custom role or Clone an existing role.
Since User, Analyst, Admin, and Content_Developer are Splunk UBA roles by default, you do not need to create these account roles if your identity provider group names match these role names.

If the role is not properly configured in Splunk UBA, you will see the following error message:

"No permissions are granted to this username."
After the account role is created, select Manage > Settings.
Verify that Authentication is selected, then click on the SSO Authentication checkbox and complete the fields.
Field	Description
EntityId	An identifier for this Splunk UBA instance that is unique across all entities in your OneLogin environment. For example, SplunkUBA
IdP Certificate	The location and name of the OneLogin certificate. This is downloaded from OneLogin and located in the Splunk UBA certs/idpcerts directory, as described earlier in the procedure. For example, /var/vcap/store/caspida/certs/idpcerts/OneLogin.pem.
Private Key file	The full path and name of the Splunk UBA 3rd party certificate or self-signed certificate. The certificate must be located in the Splunk UBA certs directory or a subdirectory under the certs directory based on the current deployment settings. For example, /var/vcap/store/caspida/certs/mycerts/my-server.key.pem. See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics for more information about creating 3rd party or self-signed certificates.
Login Url	The OneLogin single sign-on URL, provided as the SAML Endpoint (HTTP) in your OneLogin App SSO tab.
Login Callback Path	The location where the SAML assertion is sent with an HTTP POST. This is often referred to as the SAML Assertion Consumer Service (ACS) URL for your Splunk UBA instance. For example, if your identity provider is configured with https://uba/saml/acs, then specify /saml/acs in this field.
Logout Url	The OneLogin single logout URL, provided as the SLO Endpoint (HTTP) in your OneLogin App SSO tab.
Logout Callback Path	The location where the logout response will be sent. For example, if your identity provider is configured with https://uba/saml/logout, then specify /saml/logout in this field.
Click OK.
Verify that you want to restart Splunk UBA for these changes to take effect. If yes, click OK to restart Splunk UBA.

Use the Splunk UBA login type when Splunk authentication or SSO is not available
Use the Splunk UBA login type to log in to Splunk UBA for the following situations:

You have configured Splunk authentication and the Splunk platform becomes unavailable.
You have configured SSO and your SSO platform becomes unavailable.
To use the Splunk UBA login type, append ?loginType=uba to the URL when accessing the Splunk UBA server. For example:

https://example.ubaserver.com/?loginType=uba


## Configure a Warm Standby Splunk UBA System


Configure warm standby in Splunk UBA
Configure a warm standby failover solution for Splunk UBA. When configured, the primary system synchronizes data with the standby system so that the standby system can be used in a read-only capacity. All platform services such as Zookeeper, Hadoop, Postgres, Impala, Redis, and containers are running on the standby system, but all Splunk UBA services are stopped.

See How Splunk UBA synchronizes the primary and standby systems for more information about synchronization between the primary and standby systems.

Set the roles of the primary and standby systems
Configuring warm standby in Splunk UBA requires that you clearly define the role of each system whenever there is a change.

When a primary system (System A) becomes unavailable due to an unexpected outage or a planned upgrade, you can manually failover to the standby system (System B).
In order for System B to act as the primary system, you must configure System B to be the primary system and System A to be the standby system. Splunk UBA can't change system roles automatically.
Then, if you want System A to return as the primary system, you must first failover from System B to System A, then switch the roles of both systems so that System A is the primary system and System B is the standby system.
The following table shows how you must change the roles of both systems. In the example, System A is the system currently running Splunk UBA, and System B will be set up as the backup system.

Scenario	Description
This screen image shows step 1 in the warm standby scenario: setting up a second Splunk UBA system to be the standby system. There is an arrow labeled "sync" from the primary system (System A) to the standby system (System B).	Configure the standby Splunk UBA system (System B), so that all platform services are running and all Splunk UBA services are stopped. System A synchronizes data to System B in read-only mode.
First, verify all requirements. See Requirements to set up warm Standby for Splunk UBA.
Follow the instructions in Set up the standby Splunk UBA system to set up System B as the standby system.
This screen image shows step 2 in the warm standby scenario: failing over from System A to System B when System A becomes unavailable.	As needed, manually failover from the primary system to the standby system. See Failover to a standby Splunk UBA system.

After the failover, System A is unavailable and System B is running Splunk UBA as a standalone system. Replication between the systems is disabled when you perform the failover, and System A does not automatically become the standby System. If you want to bring System A back into the warm standby configuration, you must restore it as the standby system to System B.

This screen image shows step 3 in the warm standby scenario: bring System A back up and switch its role to be the standby system.	If needed, recover the system that became unavailable (System A) and change its role to be the standby system. If you performed a failover as part of a planned upgrade or HA/DR test, power System A back up, then change its role to be the standby system. Both of the following tasks must be performed:
Configure System B to be the primary system. Enable replication on System B to synchronize data to System A. System A runs in read-only mode, with all the platform services running and all Splunk UBA services stopped.
Configure System A to be the standby system. See Set up the standby Splunk UBA system.
If you want to continue with System B as the primary system and System A as the standby system, you don't need to do anything else.

If you want to restore System A as the primary system and System B as the standby system, perform both tasks 4 and 5 below.

This screen image shows step 4 in the warm standby scenario: failing over from System B to System A.	Manually failover from System B back to System A. See Failover to a standby Splunk UBA system. The failover operation disables replication between the systems.
This screen image shows step 5 in the warm standby scenario: switching the roles on both systems so that System A is returned to primary system, and System B becomes the standby.	Change the roles of both systems to make System A the primary system again. Both of the following tasks must be performed:
Configure System A to be the primary system. Enable replication on System A to synchronize data to System B.
Configure System B to be the standby system. See Change the role of both systems to switch the primary and standby systems.


Requirements to set up warm standby for Splunk UBA
Verify that the following requirements are met in preparation for configuring warm standby for Splunk UBA:

The standby Splunk UBA system must be configured separately from the primary system and must meet all of the same system requirements. Verify that the standby system meets all of the requirements in the table:
Standby System Requirement	Description
Same number of nodes.	The standby system must have the same number of nodes as the primary system. See Plan and scale your Splunk UBA deployment in Install and Upgrade Splunk User Behavior Analytics.
Same hardware requirements.	All nodes in the standby system must meet the minimum hardware requirements for all Splunk UBA servers, including allocating enough space on the management node if you are configuring incremental backups. See Hardware requirements in Install and Upgrade Splunk User Behavior Analytics.
Same SSH keys.	The standby system must use the same SSH keys as the primary system. Copy the SSH keys from the existing primary Splunk UBA system to all servers in the standby system. See Install Splunk User Behavior Analytics in Install and Upgrade Splunk User Behavior Analytics and follow the instructions for your deployment and operating system.
Set up passwordless SSH.	Each node in the standby and primary systems must have passwordless SSH capability to any other node in either system. See Install Splunk User Behavior Analytics in Install and Upgrade Splunk User Behavior Analytics and follow the instructions for your deployment and operating system.
Set up separate certificates.	The standby system must have its own certificates that are setup separately from the primary system.
See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics.
If you send anomalies and threats from Splunk UBA to Splunk Enterprise Security (ES) using an output connector, see Configure the Splunk platform to receive data from Splunk UBA's output connector in Send and Receive Data from the Splunk Platform to set up the Splunk ES certificate in Splunk UBA.
Configuration of the /etc/hosts file.	The /etc/hosts file on each node in both the standby and primary systems must have the hostnames of all other nodes in both the standby and primary systems. See Configure host name lookups and DNS in Install and Upgrade Splunk User Behavior Analytics.
The standby system must have the same ports open as the primary system. See Network requirements in Install Splunk User Behavior Analytics. The following ports must be open behind the firewall in both the primary and standby systems:
Port 8020 on the management node (node 1) in all deployment sizes.
Port 5432 on the database node in all deployment sizes. For deployments of 1 - 10 nodes, this is node 1. In 20 node deployments, this is node 2.
Port 22 on all nodes in all deployment sizes must be open for scp and SSH to work.
Port 50010 must be open on all the data nodes. This table identifies the data nodes per deployment:
Deployment size	Data nodes
1 node	Node 1
3 nodes	Node 3
5 nodes	Nodes 4 and 5
7 nodes	Nodes 4, 5, 6, and 7
10 nodes	Nodes 6, 7, 8, 9, and 10
20 nodes	Nodes 11, 12, 13, 14, 15, 16, 17, 18, 19, and 20
The Splunk Enterprise deployment where Splunk UBA pulls data from must also be highly available. This is required for Splunk UBA to re-ingest data from Splunk Enterprise. || See Use clusters for high availability and ease of management in the Splunk Enterprise Distributed Deployment Manual.
The raw events on Splunk Enterprise must be available for Splunk UBA to consume. If the Splunk Enterprise deployment is unable to retain raw events for Splunk UBA to re-ingest, the replay cannot be fully performed.
If the primary and standby Splunk UBA systems are deployed across multiple sites, the standby Splunk UBA system must have its own Splunk Enterprise deployment equivalent to the primary system in order to provide equivalent ingestion throughput.
Splunk UBA warm standby requires Python 3.


Set up the standby Splunk UBA system
After meeting the requirements, perform the following tasks to deploy and set up a secondary Splunk UBA system as the read-only warm standby system:

(Optional) If the standby system has existing data, run the following command to clean up the system:
/opt/caspida/bin/CaspidaCleanup
Run the following command on the management node of both the primary and standby systems:
/opt/caspida/bin/Caspida stop
Add the following deployment properties to /opt/caspida/conf/deployment/caspida-deployment.conf on both the primary and and standby systems:
On the primary system, uncomment caspida.cluster.replication.nodes and add standby system nodes. For example, if for 3-node deployment of host s1, s2 and s3, add:
caspida.cluster.replication.nodes=s1,s2,s3
In AWS environments, add the private IP addresses of each node.
On the standby system, uncomment caspida.cluster.replication.nodes and add the primary system nodes. For example, if for 3-node deployment of host p1, p2 and p3, add:
caspida.cluster.replication.nodes=p1,p2,p3
In AWS environments, add the private IP addresses of each node.

The host names or IP addresses of the nodes on the primary and standby systems do not need to be the same, as long as they are all defined in caspida-deployment.conf as shown in this example.

Run sync-cluster on the management node on both the primary and standby systems:
/opt/caspida/bin/Caspida sync-cluster
Allow traffic across the primary and standby systems:
Setup passwordless SSH communication across all nodes of primary and standby systems. See Setup passwordless communication between the UBA nodes in Install and Upgrade Splunk User Behavior Analytics.
Set up firewalls by running the following commands on the management node on both the primary and standby systems:
/opt/caspida/bin/Caspida disablefirewall-cluster
/opt/caspida/bin/Caspida setupfirewall-cluster
/opt/caspida/bin/Caspida enablefirewall-cluster
Register and enable replication.
On both primary and standby systems, add the following properties into /etc/caspida/local/conf/uba-site.properties on the management node. If the replication.enabled property already exists, make sure it is set to true.
replication.enabled=true
replication.primary.host=<management node of primary cluster>
replication.standby.host=<management node of standby cluster>
In the primary cluster, enable the replication system job by adding the ReplicationCoordinator property into /etc/caspida/local/conf/caspida-jobs.json file on the management node. The ReplicationCoordinator must be set to true. Below is a sample of the file before adding the property:
/**
 * Copyright 2014 - Splunk Inc., All rights reserved.
 * This is Caspida proprietary and confidential material and its use
 * is subject to license terms.
 */
{
  "systemJobs": [
    {
      // "name" : "ThreatComputation",
      // "cronExpr"   : "0 0 0/1 * * ?",
      // "jobArguments" : { "env:CASPIDA_JVM_OPTS" :  "-Xmx4096M" }
    }
  ]
} 
After adding the property, the file should look like this:

/**
 * Copyright 2014 - Splunk Inc., All rights reserved.
 * This is Caspida proprietary and confidential material and its use
 * is subject to license terms.
 */
{
  "systemJobs": [
    {
      // "name" : "ThreatComputation",
      // "cronExpr"   : "0 0 0/1 * * ?",
      // "jobArguments" : { "env:CASPIDA_JVM_OPTS" :  "-Xmx4096M" }
    },
    {
      "name"         : "ReplicationCoordinator",
      "enabled"      : true
    }
  ]
} 
On both the primary and standby systems, run sync-cluster on the management node to synchronize the configuration changes:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf/
On management node of the primary system, run the following command:
/opt/caspida/bin/replication/setup -d standby -m primary
If the same node has been registered before, a reset is necessary. Run the command again with the reset option:

/opt/caspida/bin/replication/setup -d standby -m primary -r
On management node of standby system, run the following command:
/opt/caspida/bin/replication/setup -d standby -m standby
If the standby system is running RHEL, CentOS, or Oracle Linux operating systems, run the following command to create a directory on each node in the cluster:
sudo mkdir -m a=rwx /var/vcap/sys/run/caspida
Start Splunk UBA on the primary system by running the following command on the management node:
/opt/caspida/bin/Caspida start

How Splunk UBA synchronizes the primary and standby systems
Critical data stored in the Postgres database, such as the threats and anomalies generated by Splunk UBA rules and models, is synchronized in real-time.

All other data, such as the datastores in HDFS (with some (with some metadata in Postgres) along with the data in Redis and InfluxDB, is synchronized every four hours. A checkpoint is created for each sync. When there is a failover, the standby Splunk UBA system begins to replay data ingestion from the last available checkpoint. The Splunk platform may retain new events for Splunk UBA to consume, but the search performance may be different depending on the time between the last checkpoint and when the standby system begins ingesting events. For example, some events in the Splunk platform may have been moved to the cold bucket, which negatively affects the search performance. In addition, since the ingestion lag in Splunk UBA is configurable per data source, some raw events with time stamps beyond the configured lag are excluded from the replay.

Additional data loss may occur if at the time of the failover, there are events in the data pipeline that are not yet consumed by Splunk UBA and therefore cannot persist in Splunk UBA. These events are lost and cannot be recovered during a failover operation.

The initial sync of full data transfer is triggered automatically when next scheduled job starts, defined by ReplicationCoordinator in /etc/caspida/local/conf/caspida-jobs.json.

Verify that the primary and standby systems are synchronized
You can verify your setup and that the initial sync has started by viewing the table in the Postgres database that tracks the status of the sync between the primary and standby systems.

In all deployments smaller than 20 nodes, run the following command on the management node to check. In 20-node clusters, run the command on node 2:

psql -d caspidadb -c 'select * from replication'
Below is an example output for a 3-node cluster:

caspida@ubanode001$ psql -d caspidadb -c 'select * from replication'
 id |   host     |  type   | status |               message                   |          modtime           | cycleid | cyclestatus
----+------------+---------+--------+-----------------------------------------+----------------------------+---------+-------------
 25 | ubanode01  | Standby | Active |                                         | 2021-04-04 04:10:57.191609 | 0000002 |
 26 | ubanode001 | Primary | Active | nodes:ubanode001,ubanode002,ubanode003  | 2021-04-04 04:10:57.212676 | 0000002 | 
(2 rows)
After the setup is completed, the status of the standby system is Inactive. After the first sync cycle is completed, the status is Active. If the initial sync fails, Splunk UBA will retry to sync every four hours. After four failures, the status of the standby system is Dead and replication is not attempted again until the issue is resolved.

Synchronize the primary and standby systems on-demand
To trigger a full sync right away, use the following command on the management node of the primary system:

curl -X POST -k -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)"  https://localhost:9002/jobs/trigger?name=ReplicationCoordinator
View the /var/log/caspida/replication/replication.log file on the management node of the primary system for additional information about the progress and status of the sync.

Resolve synchronization failures between the primary and standby systems
If the synchronization between the primary and standby systems fail, perform the following tasks to resume the synchronization operations:

Run the following commands on the management node in the standby system:
/opt/caspida/bin/CaspidaCleanup 
/opt/caspida/bin/Caspida stop
Run the following command on the management node in the primary system:
/opt/caspida/bin/replication/setup standby -m primary -r
Run the following command on the management node in the standby system:
/opt/caspida/bin/replication/setup standby -m standby -r
Wait until an entry in the replication table shows up on standby system. In all deployments smaller than 20 nodes, run the following command on the management node to check. In 20-node clusters, run the command on node 2:
psql -d caspidadb -c 'select * from replication'
Run the curl command on the management node in the primary system to initiate a full sync:
curl -X POST -k -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)" https://localhost:9002/jobs/trigger?name=ReplicationCoordinator 
Check the status of the replication table in both the primary and standby systems and verify the full cycle is triggered and there are no errors. Run the following command on the management node:
tail -f /var/log/caspida/replication/replication.log
Check the status of the replication table in both the primary and standby systems. After the replication is done, both the primary and standby systems have an Active status and the cycles should be synchronized. In all deployments smaller than 20 nodes, run the following command on the management node of both systems to check. In 20-node clusters, run the command on node 2 of both systems:
psql -d caspidadb -c 'select * from replication'


Failover to a standby Splunk UBA system
Perform the following tasks to failover Splunk UBA to the standby system. Make sure the standby system has been properly configured for warm standby. See Set up the backup Splunk UBA deployment for warm standby for instructions.

Before failing over
Perform the following tasks before performing the failover:

If the primary Splunk UBA system is still up and running, make a note of a few key metrics such as the total number of anomalies or total number of data sources. After you perform the failover, you can compare these metrics on the new primary Splunk UBA system to verify that the failover was successful.
By default, Splunk UBA can go back four hours to ingest data from data sources that are stopped and restarted. If the amount of time between when then primary system goes down and the failover to the backup system occurs is greater than four hours, adjust the connector.splunk.max.backtrace.time.in.hour property in the /etc/caspida/local/conf/uba-site.properties file. Perform the following tasks:
Log in to the management node on the standby Splunk UBA system as the caspida user.
Edit the /etc/caspida/local/conf/uba-site.properties file.
Add or edit the connector.splunk.max.backtrace.time.in.hour property. For example, if the primary system went down at 11PM on Friday and the failover was performed at 8AM on Monday, set the property to 57 hours or more to ingest data from the time that the primary system went down. See Time-based search for more information about configuring this property.
Synchronize the cluster in distributed deployments:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
Use the health monitor to check the lag in your data sources by monitoring the DS_LAGGING_WARN property.
When the data source lag returns to normal and you are no longer getting warning messages, return the connector.splunk.max.backtrace.time.in.hour property to its default value.
Run the failover command
Perform the following tasks to run the failover command and fail over to the standby Splunk UBA system:

Log in to the management node on the standby Splunk UBA system as the caspida user.
Run the failover command:
/opt/caspida/bin/replication/failover
This command promotes the standby system to be the primary Splunk UBA system.
Check and verify that the uiServer.host property in the /etc/caspida/local/conf/uba-site.properties file in the standby system matches the setting in the primary system. Depending on whether there is a proxy or DNS server between Splunk UBA and Splunk Enterprise Security (ES), this property may be changed during the failover operation. See Specify the host name of your Splunk UBA server in Install and Configure Splunk User Behavior Analytics for instructions.
If needed, edit the data sources to point to a Splunk search head with a different host name than before:
In Splunk UBA, select Manage > Data Sources.
Edit the data source for which you need to change the host name.
Change the URL to have the name or IP address of the new host.
Navigate through the wizard and change any other information as desired.
Click OK. A new job for this data source will be started.
If needed, edit the Splunk ES output connector to update the URL:
In Splunk UBA, select Manage > Output Connectors.
Click the Splunk ES output connector and update the URL.
Click OK. This will automatically trigger a one-time sync with Splunk ES.
Verify the failover operation
Perform the following tasks to verify that the failover operation was successful:

Log in to the Splunk UBA web interface on the standby system that you failed over to.
Verify the metrics such as the total number of anomalies or total data sources from the original primary system and make sure they match.
Log in to the CLI of the Splunk UBA system that you failed over to.
Run a tail against /var/log/caspida/replication/failover.log and look for the following message:
Failover successfully. This system is promoted to primary system now.
Recreate the standby property file if the failover cannot be completed
If you encounter any issues during the failover process that cause the failover to stop or fail, you must recreate the /opt/caspida/conf/replication/properties/standby file before you try the failover again. Perform the following tasks when a failover operation can't be completed:

Identify and fix the issue causing the failover.
Run the following command to recreate the standby property file:
touch /opt/caspida/conf/replication/properties/standby
Run the failover command again.
Options for restoring the Splunk UBA system that became unavailable
After the failover, the standby Splunk UBA system (System B) will be running as an independent system without HA/DR configured. After you restore the original Splunk UBA deployment that went down, you have the following options:

Setup System A as the standby system for System B. See Set up the standby Splunk UBA system.
Return System A to be the primary system, with System B running as the standby system. You must perform all of the following tasks in order:
Setup System A as the standby system for System B. See Set up the standby Splunk UBA system.
Failover from System B back to System A. See Failover to a standby Splunk UBA system.
Change the roles of both systems. See Change the role of both systems to switch the primary and standby systems.

Change the role of both systems to switch the primary and standby systems
Perform the following tasks if you want to switch the roles of both systems. For example, is System A is currently acting as the standby system and System B is the primary, switch the roles of both systems so that System A becomes the primary system and System B is the standby system.

On System A, run the following commands:
/opt/caspida/bin/CaspidaCleanup
/opt/caspida/bin/Caspida stop 
On both System A and System B, set the following properties in /etc/caspida/local/conf/uba-site.properties:
Enable replication:
replication.enabled=true
Switch the active and standby systems:
replication.primary.host=<management node of System A>
replication.standby.host=<management node of System B>
Synchronize the cluster:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf/
On the management node of System A, run the following command:
/opt/caspida/bin/replication/setup -d standby -m primary
If System A has been registered before, run the command again with the reset option:

/opt/caspida/bin/replication/setup -d standby -m primary -r
On the management node of System B, run the following command:
/opt/caspida/bin/replication/setup -d standby -m standby -r
On the management node of System A, run the following command:
/opt/caspida/bin/Caspida start
On System A, verify that the initial sync between the systems has started. See How Splunk UBA synchronizes the primary and standby systems for instructions.


Stop the primary system from synchronizing with the standby system
If you have a case where the standby Splunk UBA system fails, perform the following tasks to stop the primary system from trying to synchronize with the standby system:

Log in to the management node of the primary Splunk UBA system as caspida.
Stop all Splunk UBA services:
/opt/caspida/bin/Caspida stop
Edit /etc/caspida/local/conf/uba-site.properties and change the replication.enabled property to false:
replication.enabled=false
Synchronize the cluster:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Start Splunk UBA services:
/opt/caspida/bin/Caspida start
In cases where warm standby can't be configured, you can continue to use automated incremental backups for your Splunk UBA data. See Backup and restore Splunk UBA using automated incremental backups.

## Set up Automated Incremental Splunk UBA Backups to Restore Splunk UBA


Prepare automated incremental backups in Splunk UBA
Configure periodic full and incremental backups without stopping Splunk UBA. When configured, Splunk UBA will perform a full backup of your system, followed by periodic incremental backups. The incremental backups include any changes to system configurations, custom models, anomaly action rules, HR data and entities, and, threats and anomalies.

Periodic incremental backups are performed without stopping Splunk UBA. You can configure the frequency of these backups by configuring the cron job in /opt/caspida/conf/jobconf/caspida-jobs.json.
A weekly full backup is performed without stopping Splunk UBA. You can configure the frequency of these backups using the backup.filesystem.full.interval property.
Use incremental backup and restore when you want to backup and restore Splunk UBA to the same operating system and same number of nodes.

You can use incremental backup and restore as an HA/DR solution that is less resource-intensive than the warm standby solution described in Configure warm standby in Splunk UBA. You can use the backups to restore Splunk UBA on the existing server, or on a new separate server.

Backup disk size requirements
Add an additional disk to the Splunk UBA management node mounted as /var/vcap/ubabackup for the Splunk UBA backups.

The size of the additional disk must follow these guidelines:

The disk size must be at least half the size of your deployment in terabytes. For example, a 10-node system requires a 5TB disk.
If you are creating archives, allow for an additional 50 percent of the backup disk size. For example, a 10-node system requires a 5TB disk for backups, and an additional 2.5TB if for archives, so you would need a 7.5TB disk for archived backups.
The table summarizes the minimum disk size requirements for Splunk UBA backups per deployment:

Number of Splunk UBA Nodes	Minimum Disk Size for Backup (without archives)	Minimum Disk Size for Backup (with archives)
1 Node	1TB	1.5TB
3 Nodes	1TB	1.5TB
5 Nodes	2TB	3TB
7 Nodes	4TB	6TB
10 Nodes	5TB	7.5TB
20 Nodes	10TB	15TB
If you have previous backups on the same disk, be sure to also take this into account when determining available disk space.

Scheduling Splunk UBA backups
Perform or schedule backups of Splunk UBA at 10:00 PM local time to avoid conflicts with the offline models, which begin running at Midnight each night.

How long will my backup take?
The amount of time it takes to perform a backup depends on a number of factors, such as:

The size of your environment
The age of your environment
Network bandwidth
Storage throughput
Splunk UBA on cloud deployments may be subject to performance restrictions that will significantly increase the backup/restore time
Creating a compressed archive will take considerably longer due to the time required to compress the data
As an example, a large multi-node deployment with 5TB of data may complete a backup in less than 2 hours if the network bandwidth and storage throughput are not limiting factors.

Backup and restore Splunk UBA using automated incremental backups
Perform the following steps to configure incremental backups of your Splunk UBA deployment:

On the Splunk UBA management node, attach an additional disk dedicated for filesystem backup. For example, mount a device on a local directory on the Splunk UBA management node.
In 20-node clusters, Postgres services run on node 2 instead of node 1. You will need two additional disks - one on node 1 and a second on node 2 for the Postgres services, or you may have a shared storage device that can be accessed by both nodes. Make sure that the backup folder on both nodes is the same. By default, the backups are written to the /backup folder.
Stop Splunk UBA.
/opt/caspida/bin/Caspida stop
Create a dedicated directory on the management node and change directory permissions so that backup files can be written into the directory. If warm standby is also configured, perform these tasks on the management node in the primary cluster.
sudo mkdir /backup
sudo chmod 777 /backup
Mount the dedicated device on the backup directory. For example, a new 5TB hard drive mounted on the backup directory:
caspida@node1:~$ df -h /dev/sdc
Filesystem      Size  Used Avail Use% Mounted on
/dev/sdc        5.0T  1G    4.9T   1% /backup
If the backup device is on the local disk, mount the disk using its UUID, which can be found in /etc/fstab. See Prepare the server for installation in Install and Upgrade Splunk User Behavior Analytics.
Add the following properties into /etc/caspida/local/conf/uba-site.properties:
backup.filesystem.enabled=true
backup.filesystem.directory.restore=/backup
Synchronize the configuration across the cluster:
/opt/caspida/bin/Caspida sync-cluster
Register filesystem backup:
/opt/caspida/bin/replication/setup filesystem
If the same host has been registered before, run the command again with the reset flag:

/opt/caspida/bin/replication/setup filesystem -r
Enable Postgres archiving.
Create the directory where archives will be stored. For example, /backup/wal_archive:
sudo mkdir /backup/wal_archive
sudo chown postgres:postgres /backup/wal_archive
Create a file called archiving.conf on the PostgreSQL node (node 2 for 20-node deployments, node 1 for all other deployments). On RHEL, Oracle Linux, and CentOS systems:
cd /var/vcap/store/pgsql/10/data/conf.d/
sudo mv archiving.conf.sample archiving.conf
On Ubuntu systems:

cd /etc/postgresql/10/main/conf.d/
sudo mv archiving.conf.sample archiving.conf
If your archive directory is not /backup/wal_archive, edit archiving.conf to change the archive directory.
Restart PostgreSQL services on the management node:
/opt/caspida/bin/Caspida stop-postgres
/opt/caspida/bin/Caspida start-postgres
On the management node:
In the primary cluster, enable the replication system job by adding the ReplicationCoordinator property into the /etc/caspida/local/conf/caspida-jobs.json file. Below is a sample of the file before adding the property:
/**
 * Copyright 2014 - Splunk Inc., All rights reserved.
 * This is Caspida proprietary and confidential material and its use
 * is subject to license terms.
 */
{
  "systemJobs": [
    {
      // "name" : "ThreatComputation",
      // "cronExpr"   : "0 0 0/1 * * ?",
      // "jobArguments" : { "env:CASPIDA_JVM_OPTS" :  "-Xmx4096M" }
    }
  ]
} 
After adding the property, the file should look like this:

/**
 * Copyright 2014 - Splunk Inc., All rights reserved.
 * This is Caspida proprietary and confidential material and its use
 * is subject to license terms.
 */
{
  "systemJobs": [
    {
      // "name" : "ThreatComputation",
      // "cronExpr"   : "0 0 0/1 * * ?",
      // "jobArguments" : { "env:CASPIDA_JVM_OPTS" :  "-Xmx4096M" }
    },
    {
      "name"         : "ReplicationCoordinator",
      "enabled"      : true
    }
  ]
} 
Run the following command to synchronize the cluster:
/opt/caspida/bin/Caspida sync-cluster
Start Splunk UBA:
/opt/caspida/bin/Caspida start
How Splunk UBA generates and stores the automated full and incremental backup files
An initial full backup is triggered automatically when the next scheduled job starts, as defined by the ReplicationCoordinator property in the /opt/caspida/conf/jobconf/caspida-jobs.json file. After the initial full backup, a series of incremental backups is performed until the next scheduled full backup. By default, Splunk UBA performs a full backup every 7 days. To change this interval, perform the following tasks:

Log in to the Splunk UBA management node as the caspida user.
Edit the backup.filesystem.full.interval property in /etc/caspida/local/conf/uba-site.properties.
Synchronize the cluster.
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
You can identify the base directories containing the full backups and the incremental backup directories by the first digit of the directory name.

A base directory has a sequence number starting with 1.
An incremental directory has a sequence number starting with 0.
In the following example, the base directory 1000123 contains a full backup taking up 35GB of space, while the incremental directories 0000124, 0000125 and 0000126 have backup files around 1.5GB for each.

caspida@node1:~$ du -sh /backup/caspida/*
1.5G    /backup/caspida/0000124
1.5G    /backup/caspida/0000125
1.4G    /backup/caspida/0000126
35G /backup/caspida/1000123
The following restore scenarios are supported, using this example:

From a base directory with all incremental directories. Using our example, this includes all of 1000123, 0000124, 0000125, and 0000126 so Splunk UBA is restored to the latest checkpoint. See Restore Splunk UBA from incremental backups for instructions.
From a base directory with some incremental directory with contiguous sequences. Using our example, we can use 1000123, 0000124 and 0000125. The 1000123 and 0000125 directories cannot be used without 0000124 as it skips the sequence number. See Restore Splunk UBA from incremental backups for instructions.
From a base directory only, such as 1000123 in our example. See Restore Splunk UBA from a base directory without incremental backups for instructions.
Generate a full backup on-demand without waiting for the next scheduled job
Perform the following tasks to generate a full backup without waiting for the next scheduled job to do it for you.

Make sure you have set up your Splunk UBA deployment for automated incremental backups.
On the management node, edit the /etc/caspida/local/conf/uba-site.properties file and set the backup.filesystem.full.interval property to 0 days. For example:
backup.filesystem.full.interval = 0d
Synchronize the configuration change across the cluster:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Use the following curl command to trigger a new cycle:
curl -X POST -k -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)"  https://localhost:9002/jobs/trigger?name=ReplicationCoordinator
Check the /var/log/caspida/replication/replication.log file to make sure the full backup is starting:
2020-06-15 14:01:56,120 INFO MainProcess.MainThread coordinator.prepCycle.209: Target cycle is: 0000154
2020-06-15 14:02:03,422 INFO MainProcess.MainThread coordinator.isFullBackup.308: Need to perform full backup. 
Last cycle: 2020-06-11 16:20:10; Interval: 0:00:00
(Recommended) Restore the backup.filesystem.full.interval property back to its default value of 7 days. You can set the property as follows and synchronize the cluster, or delete the the property altogether from the /etc/caspida/local/conf/uba-site.properties file and synchronize the cluster:
backup.filesystem.full.interval = 7d

Restore Splunk UBA from a full backup
This example shows how to restore from a full backup, using the base directory 1000123 without any accompanying incremental directories.

Prepare the server for the restore operation. If there is any existing data, run:
/opt/caspida/bin/CaspidaCleanup
Stop all services:
/opt/caspida/bin/Caspida stop-all
Restore Postgres.
On the Postgres node (node 2 in 20-node deployments, node 1 in all other deployments), clean any existing data. On RHEL, OEL, or CentOS systems, run the following command:
sudo rm -rf /var/lib/pgsql/10/main/*
On Ubuntu systems, run the following command:

sudo rm -rf /var/lib/postgresql/10/main/*
Copy all content under <base directory>/postgres/base to the Postgres node. For example, if you are copying from different server, use the following command on RHEL, OEL, or CentOS systems:
sudo scp -r caspida@ubap1:<BACKUP_HOME>/1000123/postgres/base/* /var/lib/pgsql/10/main
On Ubuntu systems, run the following command:

sudo scp -r caspida@ubap1:<BACKUP_HOME>/1000123/postgres/base/* /var/lib/postgresql/10/main
Edit the /var/lib/pgsql/10/main/recovery.conf (on RHEL, OEL, or CentOS systems) or /var/lib/postgresql/10/main/recovery.conf (on Ubuntu systems) file, clear all content, and add the following property:
restore_command = ''
Change ownership of the backup files. On RHEL, OEL, or CentOS systems, run the following command:
sudo chown -R postgres:postgres /var/lib/pgsql/10/main
On Ubuntu systems, run the following command:

sudo chown -R postgres:postgres /var/lib/postgresql/10/main
Start the Postgres service by running the following command on the management node:
/opt/caspida/bin/Caspida start-postgres
Monitor the Postgres logs in /var/log/postgresql, which show the recovering process.
Verify that Postgres is restored. Check in the /var/lib/pgsql/10/main (on RHEL, OEL, or CentOS systems) or /var/lib/postgresql/10/main (on Ubuntu systems) directory and verify that the recovery.conf file is renamed to recovery.done.
Once the recovery completes, query Postgres to see if the data is recovered. For example, run the following command from the Postgres CLI:
psql -d caspidadb -c 'SELECT * FROM dbinfo'
Restore Redis. Redis backups are full backups, even for incremental Splunk UBA backups. You can restore Redis from any backup directory, such as the most recent incremental backup directory. In our example, we can backup Redis from the 0000126 incremental backup directory. The Redis backup file ends with the node number. Be sure to restore the backup file on the correct corresponding node. For example, in a 5-node cluster, the Redis file must be restored on nodes 4 and 5. Assuming the backup files are on node 1, run the following command on node 4 to restore Redis:
sudo scp caspida@node1:<BACKUP_HOME>/0000126/redis/redis-server.rdb.4 /var/vcap/store/redis/redis-server.rdb
Similarly, run the following command on node 5:

sudo scp caspida@node1:<BACKUP_HOME>/0000126/redis/redis-server.rdb.5 /var/vcap/store/redis/redis-server.rdb
View your /etc/caspida/local/conf/caspida-deployment.conf file to see where Redis is running on in your deployment.
Restore InfluxDB. Similar to Redis, InfluxDB backups are full backups. You can restore InfluxDB from the most recent backup directory. In this example, InfluxDB is restored from the 0000126 incremental backup directory. On the management node, which hosts InfluxDB, start InfluxDB, clean it up, and restore from backup files:
sudo service influxdb start
influx -execute "DROP DATABASE caspida"
influx -execute "DROP DATABASE ubaMonitor"
influxd restore -portable <BACKUP_HOME>/0000126/influx
Restore HDFS. To restore HDFS, we need to first restore base, and incremental data in continues sequence. In our example, we first restore from 1000123, then 0000124, 0000125 and 0000126.
Start the necessary services. On the management node, run the following command:
/opt/caspida/bin/Caspida start-all --no-caspida
Restore HDFS from the base backup directory:
nohup hadoop fs -copyFromLocal <BACKUP_HOME>/1000123/hdfs/caspida /user &
Restoring HDFS can take a long time. Check the process ID to see if the restore is completed. For example if the PID is 111222, check by using the following command:

ps 111222
Change owner in HDFS:
sudo -u hdfs hdfs dfs -chown -R impala:caspida /user/caspida/analytics
sudo -u hdfs hdfs dfs -chown -R mapred:hadoop /user/history
sudo -u hdfs hdfs dfs -chown -R impala:impala /user/hive
sudo -u hdfs hdfs dfs -chown -R yarn:yarn /user/yarn
If the server you are restoring to is different from the one where the backup was taken, run the following commands to update the metadata:
sudo hive --service metatool -updateLocation hdfs://<RESTORE_HOST>:8020 hdfs://<BACKUP_HOST>:8020
impala-shell -q "INVALIDATE METADATA"
Note the host is node1 in deployment file.
Restore your rules and customized configurations from the latest backup directory:
Restore the configurations:
sudo cp -pr <BACKUP_HOME>/0000126/conf/* /etc/caspida/local/conf/
Restore the rules:
sudo rm -Rf /opt/caspida/conf/rules/*
sudo cp -prf <BACKUP_HOME>/0000126/rule/* /opt/caspida/conf/rules/
Start the server:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
/opt/caspida/bin/CaspidaCleanup container-grouping
/opt/caspida/bin/Caspida start
Check the Splunk UBA web UI to make sure the server is operational.
If the server for backup and restore are different, perform the following tasks:
Update the data source metadata:
curl -X PUT -Ssk -v -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)" https://localhost:9002/datasources/moveDS?name=<DS_NAME>
Replace <DS_NAME> with the data source name displayed in Splunk UBA.
Trigger a one-time sync with Splunk ES: If your Splunk ES host did not change, run the following command:
curl -X POST 'https://localhost:9002/jobs/trigger?name=EntityScoreUpdateExecutor' -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)" -H 'Content-Type: application/json' -d '{"schedule": false}' -k
If you are pointing to a different Splunk ES host, edit the host in Splunk UBA to automatically trigger a one-time sync.

Restore Splunk UBA from incremental backups
To restore Splunk UBA from online incremental backup files, at least one base backup directory containing a full backup must exist.

This example shows how to restore from a base directory 1000123 with all of the incremental directories 0000124, 0000125, and 0000126.

Prepare the server for the restore operation. If there is any existing data, run:
/opt/caspida/bin/CaspidaCleanup
Stop all services:
/opt/caspida/bin/Caspida stop-all
Restore Postgres.
On the Postgres node (node 2 in 20-node deployments, node 1 in all other deployments), clean any existing data. On RHEL, OEL, or CentOS systems, use the following command:
sudo rm -rf /var/lib/pgsql/10/main/*
On Ubuntu systems, use the following command:

sudo rm -rf /var/lib/postgresql/10/main/*
Copy all content under <base directory>/postgres/base to the Postgres node. For example, if you are copying from different server on RHEL, OEL, or CentOS systems, use the following command:
sudo scp -r caspida@ubap1:<BACKUP_HOME>/1000123/postgres/base/* /var/lib/pgsql/10/main
On Ubuntu systems, use the following command:

sudo scp -r caspida@ubap1:<BACKUP_HOME>/1000123/postgres/base/* /var/lib/postgresql/10/main
Remove unnecessary WAL files. On RHEL, OEL, or CentOS systems, use the following command:
sudo rm -rf /var/lib/pgsql/10/main/pg_wal/*
On Ubuntu systems, use the following command:

sudo rm -rf /var/lib/postgresql/10/main/pg_wal/*
Make sure the system has access to Postgres WAL archive directory. Modify the /var/lib/pgsql/10/main/recovery.conf (on RHEL, OEL, or CentOS systems) or /var/lib/postgresql/10/main/recovery.conf (on Ubuntu systems) file. Remove all contents in the file, and add the following properties:

restore_command = 'cp <WAL directory>/%f "%p"'
recovery_target_time = '<recovery timestamp>'
recovery_target_action = 'promote'
Where <WAL directory> is the directory with all Postgres WAL files, and <recovery timestamp> is the timestamp in backup file <BACKUP_HOME>/0000126/postgres/recovery_target_time.
For example, the recovery.conf file looks like this:

restore_command = 'cp /backup/wal_archive/%f "%p"'
recovery_target_time = '2019-09-16 12:36:03'
recovery_target_action = 'promote'
Change ownership of the backup files. On RHEL, OEL, or CentOS systems, use the following command:
sudo chown -R postgres:postgres /var/lib/pgsql/10/main
On Ubuntu systems, use the following command:

sudo chown -R postgres:postgres /var/lib/postgresql/10/main
Start Postgres services. Run the following command on management node:
/opt/caspida/bin/Caspida start-postgres
Monitor Postgres logs under /var/log/postgresql, which show the recovering process.
Verify that Postgres is restored. Check in the /var/lib/pgsql/10/main (on RHEL, OEL, CentOS systems) or /var/lib/postgresql/10/main (on Ubuntu systems) directory and verify that the recovery.conf file is renamed to recovery.done.
Once the recovery completes, query Postgres to see if data is recovered. For example, run the following command from the Postgres CLI:
psql -d caspidadb -c 'SELECT * FROM dbinfo'
Restore Redis. Redis backups are full backups, even for incremental Splunk UBA backups. You can restore Redis from any backup directory, such as the most recent incremental backup directory. In our example, we can backup Redis from the 0000126 incremental backup directory. The Redis backup file ends with the node number. Be sure to restore the backup file on the correct corresponding node. For example, in a 5-node cluster, the Redis file must be restored on nodes 4 and 5. Assuming the backup files are on node 1, run the following command on node 4 to restore Redis:
sudo scp caspida@node1:<BACKUP_HOME>/0000126/redis/redis-server.rdb.4 /var/vcap/store/redis/redis-server.rdb
Similarly, run the following command on node 5:

sudo scp caspida@node1:<BACKUP_HOME>/0000126/redis/redis-server.rdb.5 /var/vcap/store/redis/redis-server.rdb
View your /etc/caspida/local/conf/caspida-deployment.conf file to see where Redis is running on in your deployment.
Restore InfluxDB. Similar to Redis, InfluxDB backups are full backups. You can restore InfluxDB from the most recent backup directory. In this example, InfluxDB is restored from the 0000126 incremental backup directory. On the management node, which hosts InfluxDB, start InfluxDB, clean it up, and restore from backup files:
sudo service influxdb start
influx -execute "DROP DATABASE caspida"
influx -execute "DROP DATABASE ubaMonitor"
influxd restore -portable <BACKUP_HOME>/0000126/influx
Restore HDFS. To restore HDFS, we need to first restore base, and incremental data in continues sequence. In our example, we first restore from 1000123, then 0000124, 0000125 and 0000126.
Start the necessary services. On the management node, run the following command:
/opt/caspida/bin/Caspida start-all --no-caspida
Restore HDFS from the base backup directory and also restore the incremental backup directories:
nohup bash -c 'export BACKUPHOME=/backup; hadoop fs -copyFromLocal `ls ${BACKUPHOME}/caspida/1*/hdfs/caspida -d` && for dir in `ls ${BACKUPHOME}/caspida/0*/hdfs/caspida -d`; do hadoop fs -copyFromLocal -f $dir || exit 1;  done; echo Done' & 
Replace /backup as the value of BACKUP_HOME as needed, if you configured a different directory for your backups. Restoring HDFS can take a long time. Check the process ID to see if the restore is completed. For example if the PID is 111222, check by using the following command:

ps 111222
You can also check the nohup.out file and look for "Done" at the end of the file.
Change owner in HDFS:
sudo -u hdfs hdfs dfs -chown -R impala:caspida /user/caspida/analytics
sudo -u hdfs hdfs dfs -chown -R mapred:hadoop /user/history
sudo -u hdfs hdfs dfs -chown -R impala:impala /user/hive
sudo -u hdfs hdfs dfs -chown -R yarn:yarn /user/yarn
If the server you are restoring to is different from the one where the backup was taken, run the following commands to update the metadata:
hive --service metatool -updateLocation hdfs://<RESTORE_HOST>:8020 hdfs://<BACKUP_HOST>:8020
impala-shell -q "INVALIDATE METADATA"
Note the host is node1 in deployment file.
Restore your rules and customized configurations from the latest backup directory:
Restore the configurations:
cp -pr <BACKUP_HOME>/0000126/conf/* /etc/caspida/local/conf/
Restore the rules:
rm -Rf /opt/caspida/conf/rules/*
cp -prf <BACKUP_HOME>/0000126/rule/* /opt/caspida/conf/rules/
Start the server:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
/opt/caspida/bin/CaspidaCleanup container-grouping
/opt/caspida/bin/Caspida start
Check the Splunk UBA web UI to make sure the server is operational.
If the server for backup and restore are different, perform the following tasks:
Update the data source metadata:
curl -X PUT -Ssk -v -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)" https://localhost:9002/datasources/moveDS?name=<DS_NAME>
Replace <DS_NAME> with the data source name displayed in Splunk UBA.
Trigger a one-time sync with Splunk ES: If your Splunk ES host did not change, run the following command:
curl -X POST 'https://localhost:9002/jobs/trigger?name=EntityScoreUpdateExecutor' -H "Authorization: Bearer $(grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2)" -H 'Content-Type: application/json' -d '{"schedule": false}' -k
If you are pointing to a different Splunk ES host, edit the host in Splunk UBA to automatically trigger a one-time sync.

Perform periodic cleanup of the backup files
When a new full backup is completed, it is located in the caspida directory. All existing directories in the caspida directory are moved to delete directory. You can safely remove all content in the delete directory to help minimize the number of files retained on the system, while also preserving recovery capability to the latest checkpoint.

In the following example, it is safe to remove all backup directories 0000021 to 0000038 in /backup/delete/, while keeping 1000039 to 0000045 in /backup/caspida/. The 1000039 folder contains a full backup, while all the other directories starting with zero contain incremental backups.

caspida@node1:~$ ls -t /backup/caspida/ /backup/delete/
/backup/caspida/:
0000045  0000044  0000043  0000042  0000041  0000040  1000039
 
/backup/delete/:
0000038  0000036  1000034  0000032  0000030  0000028  0000026  0000024  0000022  1000020
0000037  0000035  0000033  0000031  0000029  0000027  0000025  0000023  0000021

Disable automated incremental backups
Perform the following steps to disable and stop Splunk UBA from performing automated incremental backups:

Log in to the PostgreSQL node as the caspida user in your Splunk UBA deployment. This is node 2 in 20-node deployments, or node 1 for all other deployments.
Remove the archiving.conf file.
On RHEL, Oracle Linux, and CentOS systems:
cd /var/vcap/store/pgsql/10/data/conf.d
rm -rf archiving.conf
On Ubuntu systems:

cd /etc/postgresql/10/main/conf.d
rm -rf archiving.conf
Log in to the management node in your Splunk UBA depoyment as the caspida.
Perform the following tasks on the Splunk UBA management node:
Set the backup.filesystem.enabled property to false:
backup.filesystem.enabled = false
Reset the filesystem replication setup:
/opt/caspida/bin/replication/setup filesystem -r
Restart PostgreSQL services:
/opt/caspida/bin/Caspida stop-postgres
/opt/caspida/bin/Caspida start-postgres



## Migrate Splunk UBA to the Next Larger Deployment Size

Migrate Splunk UBA using the backup and restore scripts
Use the backup and restore scripts located in /opt/caspida/bin/utils to migrate your Splunk UBA deployment to the next larger size on the same operating system. For example, you can migrate from 5 nodes to 7 nodes, or 10 nodes to 20 nodes. If you want to migrate from 7 nodes to 20 nodes, migrate from 7 nodes to 10 nodes first, then from 10 nodes to 20 nodes.

The restore script removes all file-based data sources and does not restore them on the target Splunk UBA system.

Below is a summary of the migration process using the backup and restore scripts. For example, to migrate from a 3 node cluster to a 5 node cluster:

Verify the requirements for using the backup and restore scripts.
Run the uba-backup.sh script on the 3 node cluster. The script stops Splunk UBA, performs the backup, then restarts Splunk UBA on the 3 node cluster.
Set up the 5 node cluster so that all nodes meet the system requirements, and install Splunk UBA. The version number of the Splunk UBA software must match the version number of the backup. See the Splunk UBA installation checklist in Install and Upgade Splunk User Behavior Analytics to begin a Splunk UBA installation.
Verify that Splunk UBA is up and running in the 5 node cluster. See Verify successful installation in Install and Upgrade Splunk User Behavior Analytics.
Run the uba-restore.sh script on the 5 node cluster. The sript stops Splunk UBA, restores the system from the earlier backup, then starts Splunk UBA.
In addition to migration, you can use the backup and restore scripts as an alternative way of capturing backups of your Splunk UBA system, in addition to or in place of the automated incremental backups. See Backup and restore Splunk UBA using automated incremental backups.

Requirements for using the backup and restore scripts
Make sure the following requirements are met before using the backup and restore scripts:

The target system you are migrating to must be set up with Splunk UBA already up and running.
The backup system and the target system you are migrating to must have the same version of Splunk UBA running on the same operating system.
The target system you are migrating to must be the same size or one deployment size larger than the backup system. See Plan and scale your Splunk UBA deployment in the Install and Upgrade Splunk User Behavior Analytics manual for information about the supported Splunk UBA deployment sizes.
The backup and restore scripts are case-sensitive. Verify that your DNS host name is an exact match with the host names in /etc/hosts and /etc/hostname. See Configure host name lookups and DNS in Install and Upgrade Splunk User Behavior Analytics.
Backup disk size requirements
Add an additional disk to the Splunk UBA management node mounted as /var/vcap/ubabackup for the Splunk UBA backups.

The size of the additional disk must follow these guidelines:

The disk size must be at least half the size of your deployment in terabytes. For example, a 10-node system requires a 5TB disk.
If you are creating archives, allow for an additional 50 percent of the backup disk size. For example, a 10-node system requires a 5TB disk for backups, and an additional 2.5TB if for archives, so you would need a 7.5TB disk for archived backups.
The table summarizes the minimum disk size requirements for Splunk UBA backups per deployment:

Number of Splunk UBA Nodes	Minimum Disk Size for Backup (without archives)	Minimum Disk Size for Backup (with archives)
1 Node	1TB	1.5TB
3 Nodes	1TB	1.5TB
5 Nodes	2TB	3TB
7 Nodes	4TB	6TB
10 Nodes	5TB	7.5TB
20 Nodes	10TB	15TB
If you have previous backups on the same disk, be sure to also take this into account when determining available disk space.

Scheduling Splunk UBA backups
Perform or schedule backups of Splunk UBA at 10:00 PM local time to avoid conflicts with the offline models, which begin running at Midnight each night.

How long will my backup take?
The amount of time it takes to perform a backup depends on a number of factors, such as:

The size of your environment
The age of your environment
Network bandwidth
Storage throughput
Splunk UBA on cloud deployments may be subject to performance restrictions that will significantly increase the backup/restore time
Creating a compressed archive will take considerably longer due to the time required to compress the data
As an example, a large multi-node deployment with 5TB of data may complete a backup in less than 2 hours if the network bandwidth and storage throughput are not limiting factors.

How to handle your Splunk UBA web interface certificates during migration
When Splunk UBA is restored using the restore script, your Splunk UBA web interface certification configuration is copied to the target system, but not the actual certificates themselves.

The following table summarizes how you need to handle your Splunk UBA web interface certificates during the migration process.

Restore scenario	How to restore your Splunk UBA web interface certificates
Restoring to rebuilt machines with the same host names and IP addresses	Manually copy your existing certificates to the restored system.
Restoring to a new system with different host names or IP addresses, and using custom certificates or storing your certificates in a non-default location.	Create new certificates on the restored system before you restore Splunk UBA. By default, Splunk UBA looks for certificates in /var/vcap/caspida/certs. See Request and add a new certificate to Splunk UBA to access the Splunk UBA web interface in Install and Upgrade Splunk User Behavior Analytics.
Restoring to a new system with different host names or IP address, and using the default self-signed certificates or storing your certificates in the default location.	Splunk UBA will create the proper certificate configuration for you so you don't need to create any new certificates.

Back up Splunk UBA using the backup script
Perform a full backup of Splunk UBA using the /opt/caspida/bin/utils/uba-backup.sh script. View the command line options by using the --help option. The table lists and describes the various options that can be used in the script.

Option	Description
--archive	Create a single archive containing all of the backup data. The archive is created after the backup is completed and Splunk UBA is restarted.
--archive-only	Keep the backup archive only but do not keep the backup folder.
--archive-type %FORMAT%	Specity the type of archive you want to create.
Use gzip for good compression and good timings
Use bzip2 for better compression but bad timings
Use xz for a balance between compression and timings in gzip and bzip2
Use tar for the fastest timings but with zero compression and large sizes
Install a package called pigz on the management node to use multi-threaded compression when creating .tgz archives. On Oracle Enterprise Linux, Red Hat Enterprise Linux, or CentOS systems, use the following command to install this package:

yum -y install pigz
--dateformat %FORMAT%	Override the default date/time format for the backup folder name. If this option is not used, the folder name is based on ISO 8601 format YYYY-MM-DD. To specify a backup folder name in the typical format used in the United States, specify MM-DD-YYYY. Using this option also overrides the date/time format of the logging messages.
--folder %FOLDER%	Override the target folder location where the backup is stored. Use this option if you configured a secondary volume for storing backups, such as another 1TB disk on the management node. Don't use NFS for performance ramifications.
--log-time	Add additional logging for how long each section takes, including all function calls and tasks. Use this option to help troubleshoot issues if your backup is taking more than two hours.
--no-checksum	Don't create a checksum of the archive.
--no-data	Don't back up any data, only the Splunk UBA configuration.
--no-prestart	Don't start Splunk UBA before the backup begins, because Splunk UBA is already running. Make sure Splunk UBA is up and running before using this option.
--no-start	Don't start Splunk UBA after the backup is completed. Use this option to perform additional post-backup actions that required Splunk UBA to be offline.
--no-time	Don't log the amount of time taken for each task.
--restart-on-fail	Restart Splunk UBA if the backup fails. If Splunk UBA encounters an error during the backup, the script attempts to restart Splunk UBA so the system does not remain offline.
--script %FILENAME%	Run the specified script after the backup is completed. Use this with the --no-start option if your script requires Splunk UBA to be offline.
--skip-freespace	Skip the free space checks.
--skip-hdfs-fsck	Skip the HDFS file system consistency check. This is useful in large environments if you want to skip this check due to time constraints.
--skip-start-live-ds	Don't start the live data sources.
--use-distcp	Perform a parallel backup of Hadoop. If the HDFS export is taking several hours, use this option to perform a parallel backup which may be faster. Use the --log-time option to examine how long the HDFS export is taking.
--validate	Validate that a backup can be successfully performed, but do not actually perform the backup.
Below is an example backup.

Log in to the management node of your Splunk UBA deployment as caspida using SSH.
Navigate to the /opt/caspida/bin/utils folder:
cd /opt/caspida/bin/utils
Run the backup script. Below is the command and its output:
[caspida@uba bin]$ ./uba-backup.sh --no-prestart --archive --archive-type tgz
UBA Backup Script - Version 2.1.5
Backup started at: Thu Oct  8 07:44:59 UTC 2020
Backup running on: uba-backup.splunk.com
Logfile:           /var/log/caspida/uba-backup-2020-10-08_07-44-59.log
Script Name:       uba-backup.sh
Script SHA:        ce57125beb8de883f8a22ed61bdb45f79a15062b74ccb4676ce4dbd1730b6c35 (sha256sum)
Parsing any CLI args
- Disabling UBA pre-start before backup
- Enabling archive creation
- Archive type: tgz
Backup folder: /var/vcap/ubabackup/2020-10-08_07-44-59
UBA version: 5.0.3
Node Count:  1
Checking hypervisor and network configuration
> Time taken: 4.182 seconds
Testing SSH connectivity to UBA nodes
> Time taken: 0.323 seconds
Retrieving system information for each UBA node
> Time taken: 5.289 seconds
Determining IP address of each UBA node
> Time taken: 0.011 seconds
Not starting UBA (pre-backup), disabled via CLI
Checking available free space
Space required: 18.26 GB
Space free:     890.34 GB
Space requirements for the backup have been met
> Time taken: 3.816 seconds
Checking HDFS folder structure
> Time taken: 6.807 seconds
Creating backup folder
> Time taken: 0.133 seconds
Retrieving list of active datasources
> Time taken: 0.219 seconds
Determining current counts/stats from PostgreSQL
> Time taken: 2.787 seconds
Stopping UBA
> Time taken: 208.426 seconds
Starting UBA (partial)
> Time taken: 104.810 seconds
Checking that HDFS is not in safe-mode
> Time taken: 2.256 seconds
Performing fsck of HDFS (this may take a while)
> Time taken: 2.425 seconds
Beginning parallel tasks (1)
Creating backup of deployment configuration
Creating backup of local configurations
Creating backup of UBA rules
Creating backup of version information
Waiting for parallel tasks to finish
> Time taken: 1.013 seconds
Beginning parallel tasks (2)
Creating backup of Postgres
Creating backup of Hadoop HDFS
Logging Redis information
Waiting for parallel tasks to finish
Stopping UBA
Creating backup of Timeseries data
Creating backup of Redis data
Waiting for parallel tasks to finish
> Time taken: 288.708 seconds
Creating summary of backup
> Time taken: 0.010 seconds
Beginning parallel tasks (3)
Starting UBA
Creating backup archive
Waiting for parallel tasks to finish
Creating archive checksum file
> Time taken: 268.030 seconds
Calculating backup space usage
> Time taken: 2.801 seconds
Backup completed successfully
Time taken: 0 hour(s), 15 minute(s), 2 second(s)
You can review the log file in /var/log/caspida/uba-backup-<timestamp>.log.


Restore Splunk UBA using the restore script
After you have created a backup, restore Splunk UBA using the /opt/caspida/bin/utils/uba-restore.sh script. View the command line options by using the --help option. The table lists and describes the various options that can be used in the script.

Option	Description
--dateformat %FORMAT%	Override the default date/time format for the backup folder name. If this option is not used, the folder name is based on ISO 8601 format YYYY-MM-DD. To specify a backup folder name in the typical format used in the United States, specify MM-DD-YYYY. Using this option also overrides the date/time format of the logging messages.
--folder %FOLDER%	Override the source folder to perform the restore. By default the script looks for the restore archive in the default /var/vcap/ubabackup directory. If you used the --folder option when creating the backup to store the backup in a different directory, specify that same directory using the --folder option when restoring Splunk UBA.
--log-time	Add additional logging for how long each section takes, including all function calls and tasks. Use this option to help troubleshoot issues if your restore is taking a long time.
--no-time	Don't log the amount of time taken for each task.
--reset-admin-pass	Reset the admin password to the system default changeme.
--skip-start-live-ds	Don't start the live data sources.
--validate	Validate that a restore can be successfully performed, but do not actually perform the restore.
Below is an example restore:

Log in to the management node of your Splunk UBA deployment as caspida using SSH.
Navigate to the /opt/caspida/bin/utils folder:
cd /opt/caspida/bin/utils
Run the restore script. In this example, we are restoring from a backup file in the /home/caspida directory from a single node system to another single node system. Below is the command and its output. Some repetitive portions of the command output are truncated for brevity:
caspida@ip-172-31-29-247:~/bin$ ./uba-restore.sh --folder /var/vcap/ubabackup/2020-10-08_07-44-59
UBA Restore Script - Version 2.1.5
Restore started at: Thu Oct  8 15:25:07 UTC 2020
Restore running on: uba-restore.splunk.com
Logfile:            /var/log/caspida/uba-restore-2020-10-08_15-25-07.log
Script Name:        uba-restore.sh
Script SHA:         fa544f7bfc291dcc83b3455072682a77167b3de6f1a014b07fff042674c47c4a (sha256sum)
Parsing any CLI args
- Set source folder to /var/vcap/ubabackup/2020-10-08_07-44-59
UBA version: 5.0.3
Node Count: 1
Backup Node Count: 1
Validating summary file
WARNING: The hostnames from the backup/restore hosts differ, this will be a migration
> Time taken: 0.111 seconds
Validating backup files
> Time taken: 0.012 seconds
Validating UBA version
> Time taken: 0.022 seconds
Execution Mode: Migration
Will restore/migrate from a 1-node backup to a 1-node cluster
Checking hypervisor and network configuration
> Time taken: 0.018 seconds
Testing SSH connectivity to UBA nodes
> Time taken: 1.327 seconds
Retrieving system information for each UBA node
> Time taken: 5.272 seconds
Attempting to retrieve the IP of each node (new)
> Time taken: 0.009 seconds
Attempting to retrieve the IP of each node (old)
> Time taken: 0.011 seconds
Stopping UBA (stop-all)
> Time taken: 223.002 seconds
Starting PostgreSQL
> Time taken: 3.143 seconds
Logging PostgreSQL sparkserverregistry table (pre-restore)
> Time taken: 0.392 seconds
Restoring PostgreSQL caspidadb database on UBA node 1 (ip-172-31-29-247)
> Time taken: 12.929 seconds
Restoring PostgreSQL metastore database on UBA node 1 (ip-172-31-29-247)
> Time taken: 2.405 seconds
Performing reset of connector stats/JMS schemas
> Time taken: 0.772 seconds
Stopping PostgreSQL
> Time taken: 0.922 seconds
Restoring timeseries data
> Time taken: 0.644 seconds
Backing up existing uba-system-env.sh/uba-tuning.properties
Restoring local configurations
> Time taken: 0.003 seconds
Restoring UBA rules
> Time taken: 0.007 seconds
Restoring uba-system-env.sh/uba-tuning.properties
Starting UBA (start-all --no-caspida)
Checking that HDFS isnt in safe-mode
- Safe mode is disabled
> Time taken: 1.910 seconds
Checking if the /user folder exists in HDFS
- Folder exists, will attempt to remove
Removing existing Hadoop HDFS content (attempt 1)
> Time taken: 2.104 seconds
Waiting for 10 seconds before proceeding
Checking if the /user folder exists in HDFS
- Folder has been removed, will continue
Restoring Hadoop HDFS (this may take a while)
  - Checking status of PID 109467 (2020-10-08_15-31-41)
    - Restore is still running, please wait
    - Folder size: 178.5 K (target: 13.3 G)
  - Checking status of PID 109467 (2020-10-08_15-32-03)
    - Restore is still running, please wait
    - Folder size: 707.8 K (target: 13.3 G)
...
  - Checking status of PID 109467 (2020-10-08_16-23-19)
    - Restore is still running, please wait
    - Folder size: 11.5 G (target: 13.3 G)
  - Checking status of PID 109467 (2020-10-08_16-23-41)
  - Backup job has finished
> Time taken: 3139.875 seconds
Changing ownership of Hadoop HDFS files
> Time taken: 30.450 seconds
Updating location of hive warehouse
> Time taken: 8.640 seconds
Updating location of caspida analytics database
> Time taken: 9.058 seconds
Flushing existing Redis data
> Time taken: 10.041 seconds
Stopping Redis for Redis restore
> Time taken: 2.352 seconds
Restoring Redis database (parallel mode)
Triggering Redis restore of node 1 to 1
  - Performing restore of data from UBA node 1 to UBA node 1 (ip-192-168-10-10)
  - Performing rsync of database from UBA node 1 to UBA node 1
  - Waiting for pid 87260 to finish
    - Process finished successfully
> Time taken: 11.011 seconds
Starting Redis after Redis restore
> Time taken: 3.300 seconds
Waiting for 60 seconds to let Redis settle
Retrieving Redis information (pre-fixup) (attempt 1)
Successfully retrieved Redis information (pre-fixup)
> Time taken: 0.097 seconds
Performing Redis restore fixup (attempt 1)
> Time taken: 0.109 seconds
Performing Redis restore rebalance (attempt 1)
> Time taken: 0.095 seconds
Successfully finished Redis fixup/rebalance
Retrieving Redis information (post-fixup)
> Time taken: 0.095 seconds
Determining number of Redis keys (post-restore)
- Retrieving keys from ubanode1 (ip-192-168-10-10)
> Time taken: 0.593 seconds
Not checking Redis key counts (old backup)
Tuning configuration
> Time taken: 0.611 seconds
Stopping UBA (stop)
> Time taken: 17.020 seconds
Syncing UBA cluster
> Time taken: 0.464 seconds
Copying /opt/caspida/conf to hdfs /user/caspida/config/etc/caspida/conf
> Time taken: 8.931 seconds
Configuring containerization
> Time taken: 93.969 seconds
Starting UBA
> Time taken: 188.828 seconds
Testing Impala
> Time taken: 11.236 seconds
Logging PostgreSQL sparkserverregistry table (post-restore)
> Time taken: 0.461 seconds
Comparing PostgreSQL backup/restore counts/stats
> Time taken: 3.391 seconds
Migrating datasources
  - Migrating datasource: ASSETS
  - Migrating datasource: AWSCLOUDTRAIL
  - Migrating datasource: OSXPROCESS
  - Migrating datasource: DHCP
  - Migrating datasource: DNS
  - Migrating datasource: ESNOTABLE
  - Migrating datasource: FIREWALL
  - Migrating datasource: IDENTITIES
  - Migrating datasource: IDSIPS
  - Migrating datasource: IPFLOW
  - Migrating datasource: LINUXAUTH
  - Migrating datasource: SQUID
  - Migrating datasource: WINEVENTLOG
  - Deleting file-based datasource: file-uba-assets.csv
  - Deleting file-based datasource: file-uba-identities.csv
> Time taken: 26.924 seconds
Restore completed successfully
Time taken: 1 hour(s), 6 minute(s), 41 second(s)
You can review the log file in /var/log/caspida/uba-restore-<timestamp>.log.
If you are integrating Splunk UBA with Splunk Enterprise Security (ES), install the Splunk ES SSL certificates in the restored deployment. See Configure the Splunk platform to receive data from the Splunk UBA output connector in the Send and Receive Data from the Splunk Platform manual.

Verify Splunk UBA is up and running after migration
See Verify successfull installation in Install and Upgrade Splunk User Behavior Analytics for information about how to verify that Splunk UBA is up and running properly.

You can also run the uba_pre_check.sh script as part of this verification. See Check system status before and after installation in Install and Upgrade Splunk User Behavior Analytics.





## Recover Splunk UBA After an Outage

Recover Splunk UBA after an outage
Recover Splunk UBA after a planned or unplanned outage.

Shut down Splunk UBA for a planned outage
Perform the following tasks to shut down Splunk UBA for a planned outage:

Log into Splunk UBA.
Select Manage > Data Sources.
Stop each running data source.
From the command line, log in to the Splunk UBA server as the caspida user using SSH. In distributed environments, log in to the management server.
Stop all services.
/opt/caspida/bin/Caspida stop-all
Shut down Splunk UBA.
sudo shutdown –h now
Restart Splunk UBA after an outage
After a planned or unplanned outage, restart all Splunk UBA services.

From the command line, log in to the Splunk UBA server using SSH as the caspida user. In distributed environments, log in to the management server.
Escalate caspida privileges to sudo.
sudo su - caspida
Start all services.
/opt/caspida/bin/Caspida start-all
Log into Splunk UBA.
Select Manage > Data Sources.
Start each data source.
Restart Splunk UBA and restart all services
Perform the following tasks to shut down Splunk UBA services, restart the server, and restart all Splunk UBA services.

Log in to Splunk UBA.
From the Splunk UBA menu bar, select Manage > Data Sources.
Stop each running data source.
From the command line, log in to the Splunk UBA server as the caspida user using SSH. In distributed environments, log in to the management server.
Stop all services.
/opt/caspida/bin/Caspida stop-all
Restart Splunk UBA.
sudo shutdown –r now
Verify that Splunk UBA is back online.
ping <UBA-hostname>
From the command line, log in to the Splunk UBA server using SSH as the caspida user. In distributed environments, log in to the management server.
Escalate caspida privileges to sudo.
sudo su - caspida
Start all services.
/opt/caspida/bin/Caspida start-all
Log into Splunk UBA.
Select Manage > Data Sources.
Start each data source.
Restart Splunk UBA Services
Perform the following tasks to restart Splunk UBA services. Restarting the Splunk UBA server does not restart the Splunk UBA services.

Log in to Splunk UBA.
Select Manage > Data Sources.
Stop each running data source.
From the command line, log in to the Splunk UBA server using SSH as the caspida user. In distributed environments, log in to the management server.
Stop all services.
/opt/caspida/bin/Caspida stop-all
After stop-all has completed, restart all services.
/opt/caspida/bin/Caspida start-all
Log in to Splunk UBA.
Select Manage > Data Sources.
Start each data source.


## Monitor Splunk UBA

Monitor your Splunk UBA deployment directly from Splunk Enterprise
The Splunk UBA Monitoring app provides a centralized solution for Splunk Enterprise users to monitor the health of Splunk UBA and investigate Splunk UBA issues directly from Splunk Enterprise.

See About the Splunk UBA Monitoring app in the Splunk UBA Monitoring App manual.

Monitor the health of your Splunk UBA deployment
The Health Monitor dashboard helps you assess the health of your Splunk UBA deployment and verify the quality of data added to Splunk UBA. You can open the Health Monitor by selecting System > Health Monitor.

You can also use the Health Monitor to review system errors. System errors appear as messages in the menu.

The bell icon appears when there are messages. Click the bell icon to view messages.
Click a message or error to open the Health Monitor dashboard.
You can have system errors emailed to you. See Monitor system health with the health check script.

Splunk UBA maintains historical information about the health of your Splunk UBA deployment. You can download the information when you collect diagnostic data from the System Monitor and System Resources Monitor modules for your Splunk UBA deployment. See Collect diagnostic data from your Splunk UBA deployment.

Enable test mode for specific health indicators
You can enable test mode for health indicators that are not helpful or that are not producing relevant system monitoring information. The test mode status replaces the OK, BAD, or WARN status for health status indicators.

Log in to the Splunk UBA management server as the caspida user using SSH.
Open the /etc/caspida/local/conf/uba-site.properties file in an editor.
Change the ubaMonitor.<module_id>.<indicator_id>.mutable parameter from true to false to enable test mode. For example:
ubaMonitor.pipeline.reToOCSNewAnomalyLag.mutable=false.
Save your file.
View system health
To view system health information, perform the following steps:

Select System > Health Monitor.
Click System, if it is not already selected.
This page displays the IP addresses, host names, and deployment types of the server nodes in your environment. This view is mostly informational and displays errors when CPU usage and disk usage are higher than 90%. Click a row in the table to learn more about a specific server.

View services health
To view services health information, select System > Cluster.

Splunk UBA relies on several services and processes to create anomalies and threats, process events, identify user and device associations, and more. Monitor the health of these services and processes on this page.

In a distributed system, the host IP shows for each service so that you can find errors related to a specific host.

Service name	Service process	Description
Analytics Aggregator Service	analyticsaggregator	This service aggregates and compresses data written by Analytics Writer service.
Analytics View Builder Service	analyticsviewbuilder	This service periodically updates materialized views in the analytics database. Splunk UBA uses these views to render dashboards.
Analytics Writer Service	analyticswriter	This service writes new aggregated events and models output to the analytics database.
Anomaly Aggregation Task	anomalyaggregationmodel	This model pre-processes new anomalies by enhances the entities in the anomalies, then storing the anomalies in the database.
Docker	docker	This service builds the containerized services, such as the streaming models, data ingestion, and identity resolution.
Hadoop	hadoop-hdfs-namenode	This service keeps track of where data is stored in the Hadoop Distributed File System.
hadoop-hdfs-datanode	This service stores data in the Hadoop Distributed File System.
hadoop-hdfs-secondarynamenode	This dedicated node in the HDFS cluster takes checkpoints of the file system metadata present on the hadoop-hdfs-namenode. It is not a backup namenode.
Hive Metastore	hive-metastore	This service stores the metadata for Hive/Impala tables and partitions in a relational database, and provides clients access to this information using the metastore service API.
Impala	impala-catalog	This internal service is used by the Apache Impala database engine.
impala-server	This service is the Apache Impala database server.
impala-state-store	This internal service is used by the Apache Impala database engine.
Job Agent	caspida-jobagent	This service manages all jobs in a multi node environment. This service is not applicable for single-node environments.
Job Manager	caspida-jobmanager	This service runs and manages jobs for Splunk UBA.
Kafka	kafka-server	This service acts as the message bus for Splunk UBA.
Kubelet	kubelet	This Kubernetes component makes sure that the containers are running in pods.
Offline Rule Executor	caspida-offlineruleexec	This service runs custom threats and anomaly rules.
Output Connector Server	caspida-outputconnector	This service is the outbound connection to external data sources such as Splunk Enterprise Security, Email, or ServiceNow.
PostgreSQL	postgresql	This service stores the Splunk UBA system metadata.
Realtime Rule Executor	caspida-realtimeruleexec	This service runs anomaly action rules on generated anomalies.
Redis	redis-server	This service is the in-memory store that caches model metadata and system-wide configuration parameters.
Spark	spark-history	This service monitors the Apache Spark history server.
spark-master	This service monitors the Apache Spark master server.
spark-server	This service submits Spark jobs to the Spark backend.
spark-worker	This service monitors the Apache Spark workers running in the cluster.
System Monitor	caspida-sysmon	This service monitors the Splunk UBA system.
Splunk	splunkd	This service monitors the status of the Splunk forwarder when enabled. A Splunk forwarder is needed to send data from Splunk UBA to the Splunk UBA Monitoring App.
Time Series DB	influxdb	This service stores time series data.
UBA ETL Service	etl	This service parses events and runs identity resolution for devices and users from IR cache. It also runs all active decorators such as geolocation, threat intel, and entity validation.
UBA Identity Resolver Service	identityresolver	This service processes events to build IR data.
UBA Streaming Models devicetopic-modelgroupxx	devicetopic-modelgroupxx	This model runs in the streaming workflow. Models reading from the same topic are grouped together for performance reasons.
UBA Streaming Models domaintopic-modelgroupxx	domaintopic-modelgroupxx	This model runs in the streaming workflow. Models reading from the same topic are grouped together for performance reasons.
UBA Streaming Models eventtopic-modelgroupxx	eventtopic-modelgroupxx	This model runs in the streaming workflow. Models reading from the same topic are grouped together for performance reasons.
UBA UI	caspida-ui	This service is the Splunk UBA web interface.
Unusual Per Day Activity Time Model	uthourperusermodel-xx	This model detects unusual activity time during a day by a user based on his/her normal access profile.
Zookeeper	zookeeper-server	This service synchronizes services and manages global configurations.
View modules health
To view the health status of Splunk UBA modules, perform the following steps:

Select System > Health Monitor.
Click Modules.
Review the status of various modules that make up the Splunk UBA product. Determine what to do if you see error codes that appear on the Modules health dashboard.

Module name	Indicator	Description
Analytics Aggregator Service	Last Activity Check	This service checks for the last activity to determine if the service is working as expected.
Analytics View Builder Service	Last Activity Check	This service checks for the last activity to determine if the service is working as expected.
Analytics Writer Service	Time Lag on AnalyticsTopic	This service shows the time lag of the AnalyticsTopic.
Time Lag on IRTopic	This service shows the time lag of the IRTopic.
EPS on AnalyticsTopic	This service shows the number of events processed per second (EPS) by the service on AnalyticsTopic.
EPS on IRTopic	This service shows the number of events processed per second by the service on IRTopic.
Event Lag on AnalyticsTopic	This service shows the number of events waiting to be ingested on AnalyticsTopic.
Event Lag on IRTopic	This service shows the number of events waiting to be ingested on IRTopic.
Events dropped from Kafka on AnalyticsTopic	This service shows the percentage of events dropped from Kafka on AnalyticsTopic.
Events dropped from Kafka on IRTopic	This service shows the percentage of events dropped from Kafka on IRTopic.
Anomaly Aggregation Task	Time Lag on AnomalyTopic	This service shows the time lag of the AnomalyTopic.
EPS on AnomalyTopic	This service shows the number of events processed per second by the service on AnomalyTopic.
Event Lag on AnomalyTopic	This service shows the number of events waiting to be ingested on AnomalyTopic.
Events dropped from Kafka on AnomalyTopic	This service shows the percentage of events dropped from Kafka on AnomalyTopic.
Data Source	Assets data retrieval time	This service shows the last time assets data was retrieved.
Data Source Processing	This service shows the EPS of data sources in the Processing state.
Events Count by Data Format	This service shows the number of events processed by data format.
HR data retrieval time	This service shows the last time HR data was retrieved.
Overall EPS of all Datasources	This service shows the aggregated EPS of all data sources in the Processing state.
Percentage of Events skipped	This service shown the percentage of events skipped.
Splunk Data Source Lag	This service monitors the data ingestion search lag for all Splunk data sources, including Kafka data ingestion. The lag is defined as the duration between search submission time and the search's latest time. If lag is beyond 3600 seconds, warning message is displayed.
Splunk Data Source Search Status Check	This service Monitors the health of data ingestion into Splunk UBA by tracking errors in the Splunk data source searches, including Kafka data ingestion.
Kafka Broker	
Kafka Broker indicators are only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

All topics bytes in	The number of bytes per second being received by the Kafka broker.
All topics bytes out	The number of bytes per second being read by the consumers.
Request handler idle ratio	The percentage of time that the brokers' request handlers are idle.
Request process time - 99th Percentile	The time in milliseconds that it takes for the brokers to fully process 99% of requests per request type. Click on the View <number> Values link to see more information.
Request process time - Average	The average time in milliseconds that it takes for the brokers to fully process requests per request type. Click on the View <number> Values link to see more information.
Topic bytes in	The rate in bytes per second of the message traffic each topic is receiving from producing clients. Click on the View <number> Values link to see more information.
Topic bytes out	The rate in bytes per second of the message traffic consumed by clients of each topic. Click on the View <number> Values link to see more information.
Topic partition count	The number of partitions per topic. Click on the View <number> Values link to see more information.
Topic size on disk	The size each topic occupies on disk. Click on the View <number> Values link to see more information.
Total size on disk	The total size of all the Kafka topics.
Model Store	
Model Store indicators are only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

Average Deserialization Delay	Average deserialization delay for each model. Click on the View <number> Values link to see more information.
Average Size of Stored Models	Average size of each stored model. Click on the View <number> Values link to see more information.
Maximum Size of Stored Models	Maximum size for each model. Click on the View <number> Values link to see more information.
Number of Models Stored	Number of models stored. Click on the View <number> Values link to see more information.
Offline Models	
All Offline Models indicators except for Last Execution Time per Model are only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

Completed Stages	Number of completed stages for each offline model in the latest execution. Click on the View <number> Values link to see more information.
Completed Tasks	Number of completed tasks for each offline model in the latest execution. Click on the View <number> Values link to see more information.
Disk Bytes Spilled	Number of disk bytes spilled by each offline model in the latest execution. Data that does not fit in the memory is "spilled" to the disk. Click on the View <number> Values link to see more information.
Execution Duration	Amount of time it took for each offline model to run. Click on the View <number> Values link to see more information.
Failed Stages	Number of failed stages for each offline model in the latest execution. Click on the View <number> Values link to see more information.
Failed Tasks	Number of failed tasks for each offline model in the latest execution. Click on the View <number> Values link to see more information.
Last Execution Time Per Model	The last time each offline model was run. Click on the View <number> Values link to see more information.
Longest Stage Duration	The longest stage duration for each offline model during its last execution. Click on the View <number> Values link to see more information.
Shuffle Read Bytes	Number of shuffle read bytes for each offline model. Click on the View <number> Values link to see more information.
Shuffle Read Records	Number of shuffle read records for each offline model. Click on the View <number> Values link to see more information.
Shuffle Write Bytes	Number of shuffle write bytes for each offline model. Click on the View <number> Values link to see more information.
Shuffle Write Records	Number of shuffle write records for each offline model. Click on the View <number> Values link to see more information.
Skipped Stages	Number of stages skipped for each offline model. Click on the View <number> Values link to see more information.
Skipped Tasks	Number of tasks skipped for each offline model. Click on the View <number> Values link to see more information.
Total Jobs	Total number of jobs for each offline model. Click on the View <number> Values link to see more information.
Total Stages	Total number of stages for each offline model. Click on the View <number> Values link to see more information.
Total Tasks	Total number of tasks for each offline model. Click on the View <number> Values link to see more information.
Offline Rule Executor	Threat Revalidation	This service checks whether the average time to revalidate threats since the last system restart is within the normal range.
Output Connector Server	Anomalies dropped from Kafka	This service shows the percentage of anomalies dropped from Kafka.
Anomalies Time Lag	This service shows the time lag of the Output Connector Server on the anomaly input queue.
Audit and control events dropped from Kafka	This service shows the percentage of audit and control events dropped from Kafka.
Email Failure Percentage	This service shows the percentage of email attempts that failed.
This indicator is only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

Events Time Lag	This service shows the time lag of the Output Connector Server on the events input queue.
Sending Threats to Enterprise Security is halted	This service monitors whether or not Splunk UBA is able to send threats to Splunk ES.
Postgre SQL	Number of Suppressed Anomalies	This service shows the total number of anomalies in the system which have been suppressed either manually or by anomaly action rules.
Realtime Rule Executor	Anomalies dropped from Kafka	This service shows the percentage of anomalies that were dropped from Kafka.
Time Lag	This service shows the time lag of the anomalies being processed by Kafka.
Rate of Anomaly Generation	This service checks whether the average number of anomalies processed per second over the last 10 minutes is within the normal range.
This indicator is only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

Threat Computation Task	Graph-based Threat Computation	This service shows OK if graph-based threat computation is running in a timely manner.
Threat Computation	This service shows OK if overall threat computation is completing successfully.
Threat Computation Duration	This service shows OK if overall threat computation is completing in the expected amount of time.
UBA ETL Service	Time Lag on RawDataTopic	This service shows the time lag of the RawDataTopic.
EPS on RawDataTopic	This service shows the EPS by the service on RawDataTopic.
Event Lag on RawDataTopic	This service shows the number of events waiting to be ingested on RawDataTopic.
Events dropped from Kafka on RawDataTopic	This service shows the percentage of events dropped from Kafka on RawDataTopic.
Latest Event Time on RawData Topic	This service monitors the time of the last event processed on the RawData topic.
This indicator is only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

Time Difference on RawData Topic	This service monitors the maximum time difference among the latest processed events of each of the Splunk UBA raw ETL parsers.
This indicator is only available by adding ?system to The Splunk UBA URL. For example: https://ubaserver1/?system#Y2FzcGlk==.

UBA Identity Resolver Service	Time Lag on IRTopic	This service shows the time lag of the IRTopic.
Time Lag on PreIREventTopic	This service shows the time lag of the PreIREventTopic.
EPS on IRTopic	This service shows the EPS of the service on IRTopic.
EPS on PreIREventTopic	This service shows the EPS of the service on PreIREventTopic.
Event Lag on PreIREventTopic	This service shows the number of events waiting to be ingested on PreIREventTopic.
Events dropped from Kafka on IRTopic	This service shows the percentage of events dropped from Kafka on IRTopic.
Events dropped from Kafka on PreIREventTopic	This service shows the percentage of events dropped from Kafka on PreIREventTopic.
UBA Pipeline	NewAnomalyTopic	This service shows the status of the NewAnomalyTopic.
UBA Streaming Models devicetopic-modelgroupnn	Time Lag on DeviceTopic	This service shows the time lag of the DeviceTopic.
EPS on DeviceTopic	This service shows the EPS of the service on DeviceTopic.
Event Lag on DeviceTopic	This service shows the number of events waiting to be ingested on DeviceTopic.
Events dropped from Kafka on DeviceTopic	This topic shows the percentage of events dropped from Kafka on DeviceTopic.
UBA Streaming Models domaintopic-modelgroupnn	Time Lag on DomainTopic	This service shows the time lag of the DomainTopic.
Time Lag on DomainTopic	This service shows the time lag of the DomainTopic.
EPS on DomainTopic	This service shows the EPS of the service on DomainTopic.
Event Lag on DomainTopic	This service shows the number of events waiting to be ingested on DomainTopic.
Events dropped from Kafka on DomainTopic	This service shows the percentage of events dropped from Kafka on DomainTopic.
UBA Streaming Models eventtopic-modelgroupnn	Time Lag on EventTopic	This service shows the time lag of the EventTopic.
EPS on EventTopic	This server shows the EPS of the service on EventTopic.
Event Lag on EventTopic	This service shows the number of events waiting to be ingested on EventTopic.
Events dropped from Kafka on EventTopic	This service shows the percentage of events dropped from Kafka on EventTopic.
Unusual Per Day Activity Time Model	Time Lag on EventTopic.	This service shows the time lag of the EventTopic.
EPS on EventTopic	This server shows the EPS of the service on EventTopic.
Event Lag on EventTopic	This service shows the number of events waiting to be ingested on EventTopic.
Events dropped from Kafka on EventTopic	This service shows the percentage of events dropped from Kafka on EventTopic.
View data quality Metrics
To view data quality information, perform the following steps:

Select System > Health Monitor.
Click Data Quality.
Review metrics about the quality of data in your system. If system issues affect the quality of data, errors appear on this page.

Module	Indicator	Description
Data Source	Data Source EPS on Splunk	This service shows the average number of events processed per second by each data source on Splunk in the last hour.
Percentage of Events dropped by EventFilters	This service shows the percentage of events dropped by EventFilters on the UI.
Percentage of Events with no entity	This service shows the percentage of events that have no entity.
Percentage of Events with no Relevant Data	This service shows the percentage of events that have no relevant data.
Splunk Direct Data Source Enum Check	This service monitors the Splunk Direct input enum field data quality and tracks the mismatch rate (percentage) in each data source.
Offline Rule Executor	Average Execution Time Per Rule	This service shows the average execution time of each custom threat, anomaly action rule, or anomaly rule.
Last Execution End Time per Rule	This service shows the last time each custom threat, anomaly action rule, or anomaly rule finished running.
Last Execution Failure per Rule	This service shows the last time at which a custom threat, anomaly action rule, or anomaly rule failed to run.
Last Execution Start Time per Rule	This service shows the start time of the most recent run of each custom threat, anomaly action rule, or anomaly rule.
Number of Execution Failures per Rule	This service shows the number of consecutive times each rule failed to run, or 0 if no failures have occurred.
Number of Executions per Rule	This service shows the total number of times that each custom threat, anomaly action rule, or anomaly rule has attempted to run. Both successful and failed attempts are counted.
Output Connector Server	Number of Threats Sent to Output Connector	This service shows the total number of threats sent to the output connector for forwarding to Splunk Enterprise Security or other external destinations in the time since Splunk UBA was last restarted.
Total New Anomalies	This service shows the number of new anomalies received by the output connector server in this session. You can compare this number to the number of anomalies in the receiving system, such as Splunk Enterprise Security, to determine if all anomalies are successfully being processed by the system to which Splunk UBA is sending anomalies.
PostgreSQL	Number of Inactive Anomalies	This service shows the total number of anomalies currently processing in the system. Contact Splunk Support if the number is consistently above several thousand or the number continues to increase.
Number of Suppressed Anomalies	This service shows the total number of anomalies in the system that have been suppressed manually or by anomaly action rules.
Real-time Rule Executor	Average New Anomalies Completed	This service shows the average number of anomalies processed per second since the last restart of the Realtime Rule Executor.
Dropped New Anomalies	This service shows the total number of dropped anomalies that were not duplicates since the last restart of the Realtime Rule Executor.
Duplicate New Anomalies	This service shows the total number of duplicate anomalies since the last restart of the Realtime Rule Executor.
New Anomalies Received	This service shows the total number of new anomalies created by Splunk UBA since the last restart of the Realtime Rule Executor.
New Anomalies Completed	This service shows the total number of new anomalies processed since the last restart of the Realtime Rule Executor.
New Anomalies in Process	This service shows the number of new anomalies currently being processed by Splunk UBA.
Number of Active Anomalies	This service shows the total number of anomalies activated by the Realtime Rule Executor after the last restart of the real-time rule executor. Activated anomalies are anomalies that were not suppressed or permanently deleted by anomaly action rules.
Threat Computation Task	Threat Computation Start Time	This service shows the last time the threat computation task was started.
Monitor system health with the health check script
The health check script captures the state of a running system and highlights areas of concern such as event processing lags, system slowness, and errors in services like Apache Kafka.

You can schedule the script to run regularly as a cron job and email the output as an attachment. See Configure email alerts to your Splunk UBA deployment administrators.

The uba_health_check.sh script is stored in the /opt/caspida/bin/utils directory of Splunk UBA. Log in as the caspida user on the management server using SSH to run the script.

Output from the script is saved in a plain text file in the /var/log/caspida/check/ directory with a file name that includes the host name of the server and the time stamp. You can also download the script output from Splunk UBA. Select the Scripts module. See Collect diagnostic data from your Splunk UBA deployment.

Monitor server health with SNMP
You can use an SNMP monitoring tool to track statistics related to CPU usage, memory, and disk utilization on any server that has Splunk UBA installed.


Health Monitor status code reference
Use the status codes provided by Splunk UBA in the Health Monitor dashboard to help troubleshoot issues in your deployment. See Monitor the health of your Splunk UBA deployment for information about the dashboard.

Health monitor indicators do not stop any processes from running. For example, if a rule's execution time has exceeded a threshold and a warning is generated, its execution is not interrupted.

Analytics Writer Service (ANL_WRT)
Splunk UBA displays several analytics writer (ANL_WRT) status codes to help you troubleshoot any issues with the analytics writer services.

ANL_WRT-1-AnalyticsTopic
Status	WARN
Error message	Indicator is in WARN state because of one of the two reasons:
1. One/more of the instances have not consumed events since <x mins/hrs>
2. TimeLag of one/more instances is x% of topic's retention <topicRetention>) for <x mins/hrs>. Consumer may start missing events soon.

What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.ANL_WRT.AnalyticsTopic.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.ANL_WRT.AnalyticsTopic.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
ANL_WRT-3-AnalyticsTopic
Status	WARN
Error message	Instance has not consumed events for <x mins/hrs>.
What is happening	The instance has not consumed any events in the specified period of time. This warning is triggered when the instance's timeLag on the topic (min(latestRecordTimes) across its partitions with eventLag > 0) has remained constant for warnPollCount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.ANL_WRT.AnalyticsTopic.instance.timeLag.warn.pollcount property to customize the threshold for when a warning is triggered. The default is 12 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
ANL_WRT-1-IRTopic
Status	WARN
Error message	Indicator is in WARN state because of one of the two reasons:
1. One/more of the instances have not consumed events since <x mins/hrs>
2. TimeLag of one/more instances is x% of topic's retention <topicRetention>) for <x mins/hrs>. Consumer may start missing events soon

What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.ANL_WRT.IRTopic.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.ANL_WRT.IRTopic.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
ANL_WRT-3-IRTopic
Status	WARN
Error message	Instance has not consumed events for <x mins/hrs>.
What is happening	The instance has not consumed any events in the specified period of time. This warning is triggered when the instance's timeLag on the topic (min(latestRecordTimes) across its partitions with eventLag > 0) has remained constant for warnPollCount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.ANL_WRT.IRTopic.instance.timeLag.warn.pollcount property to customize the threshold for when a warning is triggered. The default is 12 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
Data Sources (DS)
Splunk UBA displays several data source (DS) status codes to help you troubleshoot any issues with the data source services.

DS-1
Status	ERROR
Error message	EPS of one/more Data Sources was 0 for over a day.
What is happening	Zero events per second were processed for one or more low-frequency data sources for more than 5 days, or high-frequency data sources for more than 1 day.
Each data source is categorized as low-frequency or high-frequency during a training period of 10 days, where it is polled once per hour (240 polls). Configure this threshold using the ubaMonitor.DS.datasourceEPS.training.pollCount property.

If the EPS is below dsEpsFreqThreshold consistently during the training period, the data source is categorized as a low-frequency data source. If not, the data source is categorized as a high-frequency data source.
After the training period, if the EPS of a high-freguency data source is 0 for a specific number of polls (highFreqBadPollCount), this error is generated.
After the training period, if the EPS of low-frequency data source is 0 for a specific number of polls (lowFreqBadPollCount), this error is generated.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
dsEpsFreqThreshold: ubaMonitor.DS.datasourceEPS.freq.threshold (default is 50)
highFreqBadPollCount: ubaMonitor.DS.datasourceEPS.highFreq.bad.pollCount (default is 24 polls, or 1 day)
lowFreqBadPollCount: ubaMonitor.DS.datasourceEPS.lowFreq.bad.pollCount (default is 120 polls, or 5 days)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Restart data source(s) in BAD state, making sure events are sent to Splunk UBA. Contact Splunk Support if the status does not return to OK.
DS-2
Status	WARN
Error message	EPS of one/more Data Sources was 0 for over 6 hours.
What is happening	Zero events per second were processed for one or more low-frequency data sources for more than 3 days or high-frequency data sources for more than 6 hours.
Each data source is categorized as low-frequency or high-frequency during a training period of 10 days, where it is polled once per hour (240 polls). Configure this threshold using the ubaMonitor.DS.datasourceEPS.training.pollCount property.

If the EPS is below dsEpsFreqThreshold consistently during the training period, the data source is categorized as a low-frequency data source. If not, the data source is categorized as a high-frequency data source.
After the training period, if the EPS of a high-freguency data source is 0 for a specific number of polls (highFreqWarnPollCount), generate this error.
After the training period, if the EPS of low-frequency data source is 0 for a specific number of polls (lowFreqWarnPollCount), generate this error.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
dsEpsFreqThreshold: ubaMonitor.DS.datasourceEPS.freq.threshold (default is 50)
highFreqWarnPollCount: ubaMonitor.DS.datasourceEPS.highFreq.warn.pollCount (default is 6 polls, or 6 hours)
lowFreqWarnPollCount: ubaMonitor.DS.datasourceEPS.lowFreq.warn.pollCount (default is 72 polls, or 3 days)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 18 hours. If the status does not go back to OK or keeps fluctuating, restart data source(s) in WARN state. Check that events are sent to Splunk UBA.
DS-3
Status	ERROR
Error message	Number of skipped events is more than the configured threshold.
What is happening	The number of skipped events (for example, Unknown, EpochTooLow, EpochTooHigh, EpochDefault, EventBlacklisted, EventFiltered, EventHasNoEntities, IpAddressOutOfRange, AllUsersUnknown, AD3DigitGroupEventCode, PANIgnoredSesssionEvent, DuplicateEvent, EventNotRelevant) exceeds the configured threshold.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.DS.skippedEvents.bad.threshold property to customize the threshold for which an error is triggered. The default configured threshold is 75.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	In Splunk UBA, go to Manage > Data Sources. Under each data source, determine why the events are skipped. Contact Splunk Support if the reason is neither UI event filters nor HR data.
DS-5
Status	ERROR
Error message	No events were processed for one or more data formats.
What is happening	For each data format, the average frequency of events stored in Impala per poll is calculated in the default training period of 10 polls. You can customize this number of polls using the ubaMonitor.DS.eventsPerFormat.trainingPeriod property.
If the frequency is above 1000, the data format is categorized as high frequency. Otherwise, it is categorized as low frequency. You can customize this threshold using the ubaMonitor.DS.eventsPerFormat.freq.threshold property.
After the training period, if number of events for any high-frequency data format is 0, generate this error.
After the training period, if number of events for any low frequency data format is 0 for ubaMonitor.DS.eventsPerFormat.lowFreq.NOK number of consecutive polls, generate this error. The default is 7.
What to do	Verify that the Splunk platform is ingesting data. If data is being ingested:
Restart the data source for that format.
Contact Splunk Support if no events are being processed for the data format after data source restart.
If data is not being ingested by the Splunk platform, fix data on-boarding on the Splunk platform.

DS-6
Status	WARN
Error message	HR data ingestion has warnings.
What is happening	Possible causes:
The HR data source is not defined.
Ingestion of HR data has not been executed for more than the configured ubaMonitor.DS.HR.poll.warn.threshold. The default threshold is 48 hours.
The last HR data job took longer than the configured ubaMonitor.DS.HR.poll.max.duration. The default threshold is 3 hours.
The percentage of events that failed to be parsed by any of the jobs is equal or greater than ubaMonitor.DS.HR.poll.max.failed.perc. The default threshold is 0.05, or 5 percent.
What to do	Verify that the HR data source is configured properly and is ingesting events by going to the Data Sources page in Splunk UBA. See Validate HR data configuration before adding other data sources.
If events are being properly ingested, restart the HR data source. If the warning persists, contact Splunk Support.
If events are not being ingested, fix the HR data source ingestion on the Splunk platform.
DS-7
Status	BAD
Error message	HR data ingestion has errors.
What is happening	The amount of time since the latest execution of any of the jobs is more than ubaMonitor.DS.HR.poll.bad.threshold. The default threshold is 72 hours.
What to do	Restart the HR data source. If the warning persists, contact Splunk Support.
DS-8
Status	WARN
Error message	Asset data ingestion has warnings.
What is happening	Possible causes:
Ingestion of assets data has not been executed for more than the configured ubaMonitor.asset.HR.poll.warn.threshold. The default threshold is 48 hours.
The last asset data job took longer than the configured ubaMonitor.DS.asset.poll.max.duration. The default threshold is 3 hours.
The percentage of events that failed to be parsed by any of the jobs is equal or greater than ubaMonitor.DS.asset.poll.max.failed.perc. The default threshold is 0.05, or 5 percent.
What to do	Verify that the assets data source is configured properly and is ingesting events by visiting Data Sources page in Splunk UBA.
If events are being properly ingested, restart the assets data source. If the warning persists, contact Splunk Support.
If events are not being ingested, fix the assets data source ingestion on the Splunk platform.
DS-9
Status	BAD
Error message	Asset data ingestion has errors.
What is happening	The amount of time since the latest execution of any of the jobs is more than ubaMonitor.DS.asset.poll.bad.threshold. The default threshold is 72 hours.
What to do	Restart the assets data source. If the warning persists, contact Splunk Support.
ENUM_MISMATCH_WARN
Status	WARN
Error message	enum mismatch beyond warn threshold.
What is happening	The ratio of bad events over total events is between 0.1 and 0.2.
What to do	Stop the affected data source and make sure Splunk UBA is able to understand enum fields. Take one of two actions:
Modify the SPL to make sure values in enum fields match what is expected in normalize.rules file.
Update normalize.rules to enable Splunk UBA to understand incoming data.
For more information, see Monitor the quality of data sent from the Splunk platform in Get Data into Splunk User Behavior Analytics.

ENUM_MISMATCH_BAD
Status	BAD
Error message	enum mismatch beyond error threshold.
What is happening	The ratio of bad events over total events is greater than 0.2.
What to do	Stop the affected data source and make sure Splunk UBA is able to understand enum fields. Take one of two actions:
Modify SPL to make sure values in enum fields match what is expected in normalize.rules file.
Update normalize.rules to enable Splunk UBA to understand incoming data.
For more information, see Monitor the quality of data sent from the Splunk platform in Get Data into Splunk User Behavior Analytics.

DS_LAGGING_WARN
Status	WARN
Error message	Data source micro-batch search lag is over the threshold.
What is happening	Data source services monitor the Splunk data source ingestion search lag, including Kafka data ingestion. The lag is defined as the duration between search submission time and the search's latest time. If lag is beyond 3600 seconds, warning message is displayed.
See Configure Kafka data ingestion in the Splunk UBA Kafka Ingestion App manual.

Configure this threshold by adding or editing the splunk.kafka.ingestion.search.max.lag.seconds property.

What to do	Stop the affected data source and try to split it into multiple sources to keep each data source's EPS small.
DS_SEARCH_STATUS_BAD
Status	BAD
Error message	Data source micro-batch search has returned an error.
What is happening	Data source services monitor the Splunk data source ingestion search status, including Kafka data ingestion. This indicator tracks if the search issued by the data source is healthy. An alert is triggered when the number of times the search returns any error is more than ubaMonitor.DS.search.highFreq.bad.pollCount times in a row. The default is 3 times in a row.
Configure this threshold by adding or editing the ubaMonitor.DS.search.highFreq.bad.pollCount property.

What to do	Stop the affected data source and check on the Splunk platform's job inspector for more debug information.
Kafka Broker (KAFKA)
Splunk UBA displays one Kafka status code to help you troubleshoot any issues with the Kafka broker.

KAFKA-1
Status	ERROR
Error message	Kafka topics are not receiving events.
What is happening	There may be an issue in kafka-server resulting in events not writing to their topics. This error is triggered when data is being ingested but the indicator item values are null (when there are exceptions from KafkaJMXClient) for badPollCount number of polls. To check if data is being ingested, check if the difference between eventsIngested by connectors during first poll required for status change(last pollCount #) and the latest poll is greater than eventsIngestedDiffThreshold.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
badPollCount: ubaMonitor.kafka.bytesIn.bad.pollCount (default is 3 polls)
eventsIngestedDiffThreshold: ubaMonitor.kafka.bytesIn.eventsIngested.diff.threshold (default is 100)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Monitor for an hour, and if the status does not return to OK, restart kafka-server. If Kafka topics continue to not receive events, contact Splunk Support.
Offline Rule Executor (ORE)
Splunk UBA displays several offline rule executor (ORE) status codes to help you troubleshoot any issues with the offline rule executor services.

ORE-1
Status	WARN
Error message	One or more rules have failed to run.
What is happening	One or more custom threats, anomaly action rules, or anomaly rules failed to run in the current session.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.ore.rulesFailures.warn.ruleCount property to to set the number of rules that must fail to run before a warning is generated. The default is 1.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Monitor the number of failed runs compared to the total number of runs for the rule over the following days. If the number of failed runs increases or remains steady, contact Splunk Support. There is no need to contact Splunk Support if the number of failed runs decreases.
ORE-2
Status	ERROR
Error message	One or more rules consistently failed to run.
What is happening	One or more custom threats, anomaly action rules, or anomaly rules have consistently failed to run in the current session. This error is triggered when more than 20 percent of the executions of at least 1 rule have failed in the current session.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.ore.rulesFailures.bad.rulePerc property to customize the threshold for when an error is triggered. The default is 20 percent.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-offlineruleexec restart
What to do	Contact Splunk Support.
ORE-3
Status	WARN
Error message	Average execution time per rule
What is happening	At least one custom threat, anomaly action rule, or anomaly rule has exceeded the warning threshold.
What to do	If the rules in WARN status are executing without failures, edit the ubaMonitor.ore.ruleDuration.warn.duration property to increase the time threshold before a warning is generated. The default is 30 minutes. After setting the property:
Sync the cluster.
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf/
Restart the offline rule executor.
sudo service caspida-offlineruleexec restart
Click on the link in the Average Execution Time per Rule on the Data Quality Indicators page to check the rule execution times.

ORE-4
Status	WARN
Error message	Threat revalidation is slower than normal.
What is happening	During the current Realtime Rule Executor session, it is taking longer than a specified threshold of time to revalidate threats based on changes to anomalies, such as new scores, deleted anomalies, or suppressed anomalies.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.rre.threatReval.warn.duration property to customize the threshold for when a warning is triggered. The default is 20 minutes.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-offlineruleexec restart
What to do	Contact Splunk Support.
Realtime Rule Executor (RRE)
Splunk UBA displays several realtime rule executor (RRE) status codes to help you troubleshoot any issues with the realtime rule executor services.

RRE-1
Status	WARN
Error message	New anomalies are being processed slowly and the processing speed is slowing down.
What is happening	The average events per second for processing new anomalies is slowing. High numbers of suppressed anomalies affects processing new anomalies. This warning is triggered when the 10 minutes average EPS keeps dropping (or increasing by less than 0.2) for the last number of (pollCount) polls and in all polls it is below the WARN configurable threshold. If no threshold is specified, this status is disabled.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
threshold: ubaMonitor.RRE.movingAvgNewAnomEPS.warn (initialized with 1)
pollCount: ubaMonitor.RRE.movingAvgNewAnomEPS.warn.polls (default is 12)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-realtimeruleexec restart
What to do	Restart the Realtime Rule Executor service.
On the Services page of the Health Monitor dashboard, locate the server node with the Realtime Rule Executor service installed.
Log in to that server as the caspida user using SSH.
Run the following commands:
sudo service caspida-realtimeruleexec stop
sudo service caspida-realtimeruleexec start
If the error continues to appear, check the number of suppressed anomalies on the Data Quality Metrics page on the Health Monitor dashboard. If the number of suppressed anomalies is in the millions, delete some of the suppressed anomalies. See Delete anomalies in Splunk UBA.

RRE-2
Status	ERROR
Error message	New anomalies are no longer being processed.
What is happening	The processing speed of new anomalies is low enough that no anomalies were processed in the last 10 minute period. This error is triggered when the 10 minutes average EPS keeps dropping (or increasing by less than 0.2) for the last number of (pollCount) polls and in all polls it is below the BAD configurable threshold. If no threshold is specified, this status is disabled.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
threshold: ubaMonitor.RRE.movingAvgNewAnomEPS.bad (initialized with 0.1)
pollCount: ubaMonitor.RRE.movingAvgNewAnomEPS.bad.polls (default is 12)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-realtimeruleexec restart
What to do	Restart the Realtime Rule Executor service.
On the Services page of the Health Monitor dashboard, locate the server node with the Realtime Rule Executor service installed.
Log in to that server as the caspida user using SSH.
Run the following commands:
sudo service caspida-realtimeruleexec stop
sudo service caspida-realtimeruleexec start
If the error continues to appear, check the number of suppressed anomalies on the Data Quality Metrics page on the Health Monitor dashboard. If the number of suppressed anomalies is in the millions, delete some of the suppressed anomalies. See Delete anomalies in Splunk UBA.

RRE-3
Status	WARN
Error message	Minor anomaly loss has been detected.
What is happening	Kafka has started to drop events from NewAnomalyTopic. This warning is triggered when the percentage of missed events is between from% (inclusive) and to% (not inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
from: ubaMonitor.RRE.missed.warn.threshold (default is 1)
to: ubaMonitor.RRE.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.RRE.missed.warn.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-realtimeruleexec restart
What to do	Restart the Realtime Rule Executor. The indicator's status will reset and its status recalculated in the next hour. If the status returns to WARN, contact Splunk Support.
To restart the Realtime Rule Executor, perform the following steps:

On the Services page of the Health Monitor dashboard, locate the server node with the Realtime Rule Executor service installed.
Log in to that server as the caspida user using SSH.
Run the following commands:
sudo service caspida-realtimeruleexec stop
sudo service caspida-realtimeruleexec start
RRE-4
Status	ERROR
Error message	A significant percentage of anomalies is being dropped by Kafka.
What is happening	Kafka is dropping a significant number of events from NewAnomalyTopic. This error is triggered when the percentage of missed events exceeds the limit% (inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
limit: ubaMonitor.RRE.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.RRE.missed.bad.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-realtimeruleexec restart
What to do	Restart the Realtime Rule Executor. The indicator's status will reset and its status recalculated in the next hour. If the status returns to WARN, contact Splunk Support.
To restart the Realtime Rule Executor, perform the following steps:

On the Services page of the Health Monitor dashboard, locate the server node with the Realtime Rule Executor service installed.
Log in to that server as the caspida user using SSH.
Run the following commands:
sudo service caspida-realtimeruleexec stop
sudo service caspida-realtimeruleexec start
PostgreSQL (PSQL)
Splunk UBA displays one PostgreSQL (PSQL) status code to help you troubleshoot any issues with the PostgreSQL service.

PSQL-1
Status	WARN
Error message	The number of suppressed anomalies is too high.
What is happening	A high volume of suppressed anomalies has led to slow system performance. This warning is triggered when the number of suppressed anomalies in PostgreSQL surpassed a configurable threshold. If no threshold is specified, this status is disabled.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.postgres.supprAnom.warn property to customize the threshold for when a warning is triggered. The default is 5 million.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Reduce the number of suppressed anomalies to improve performance. Delete anomalies from the anomalies trash. See Delete anomalies in Splunk UBA.
Offline Models (OML)
Splunk UBA displays several offline model (OML) status codes to help you troubleshoot any issues with the offline model services.

OML-1
Status	WARN
Error message	One or more models have not executed for (x) hours.
What is happening	One or more models have not executed successfully in the past 2 days.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
pollTime: ubaMonitor.OML.exec.time.poll.hour (default is 22, or 10:00PM)
warn pollCount: ubaMonitor.OML.exec.time.warn.poll.count (default is 1 poll)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 48 hours. If the OK status does not return during that time, contact Splunk Support.
OML-2
Status	ERROR
Error message	One or more models have not executed for the past 3 days.
What is happening	One or more models have not executed successfully in the specified interval of time.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
pollTime: ubaMonitor.OML.exec.time.poll.hour (default is 22, or 10:00PM)
bad pollCount: ubaMonitor.OML.exec.time.bad.poll.count (default is 3 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Contact Splunk Support.
Output Connector Service (OCS)
Splunk UBA displays several output connector service (OCS) status codes to help you troubleshoot any issues with the output connector services.

OCS-4
Status	ERROR
Error message	Percentage of email failures is more than the configured threshold.
What is happening	The percentage of email failures exceeds the configured threshold.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.OCS.emailFailures.bad.pollcount property to customize the percentage of email failures before an error is triggered. The default is 80 percent.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	In Splunk UBA, go to Manage > Output Connectors and verify that the Email Connector is correctly configured.
OCS-5
Status	WARN
Error message	Minor anomaly loss has been detected.
What is happening	Kafka is dropping events from AnomalyTopic. This warning is triggered when the percentage of missed anomalies is between from% (inclusive) and to% (not inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
from: ubaMonitor.OCS.anomalies.missed.warn.threshold (default is 1)
to: ubaMonitor.OCS.anomalies.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.OCS.anomalies.missed.warn.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	Restart the Output Connector Server. The indicator's status will reset and its status will recalculate in the next hour. If the status returns to WARN, contact Splunk Support.
To restart the Output Connector Server:

Find the node on which it runs from the Cluster Services page.
Log in to this node as caspida user and run the following commands:
sudo service caspida-outputconnector stop
sudo service caspida-outputconnector start
OCS-6
Status	ERROR
Error message	A significant percentage of anomalies is being dropped from Kafka.
What is happening	Kafka is dropping a significant percentage of events from AnomalyTopic. This error is triggered when the percentage of missed anomalies exceeds limit% (inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when the error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
limit: ubaMonitor.OCS.anomalies.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.OCS.anomalies.missed.bad.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	Restart the Output Connector Server. The indicator's status will reset and its status will recalculate in the next hour. If the status returns to WARN, contact Splunk Support.
To restart the Output Connector Server:

Find the node on which it runs from the Cluster Services page.
Log in to this node as caspida user and execute:
sudo service caspida-outputconnector stop
sudo service caspida-outputconnector start
OCS-8
Status	WARN
Error message	Minor event loss has been detected.
What is happening	Kafka is dropping events from OutputConnectorTopic. This warning is triggered when the percentage of missed events is between from% (inclusive) and to% (not inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
from: ubaMonitor.OCS.events.missed.warn.threshold (default is 1)
to: ubaMonitor.OCS.events.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.OCS.events.missed.warn.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	Restart the Output Connector Server. The indicator's status will be reset and in the next hour, its status will be recalculated. If the status goes back to WARN again, contact Splunk Support.
To restart the Output Connector Server:

Find the node on which it runs from the Cluster Services page.
Log in to this node as caspida user and execute:
sudo service caspida-outputconnector stop
sudo service caspida-outputconnector start
OCS-9
Status	ERROR
Error message	A significant percentage of events are dropping from Kafka.
What is happening	Kafka is dropping a significant percentage of events from OutputConnectorTopic. This error is triggered when the percentage of missed events exceeds limit% (inclusive) for the last number of (pollCount) polls.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
limit: ubaMonitor.OCS.events.missed.bad.threshold (default is 5)
pollCount: ubaMonitor.OCS.events.missed.bad.polls (default is 4)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	Restart the Output Connector Server. The indicator's status will reset and its status will recalculate in the next hour. If the status returns to WARN, contact Splunk Support.
To restart the Output Connector Server:

Find the node on which it runs from the Cluster Services page.
Log in to this node as caspida user and execute:
sudo service caspida-outputconnector stop
sudo service caspida-outputconnector start
OCS-11
Status	ERROR
Error message	The latest threat feed has been halted.
What is happening	Splunk UBA has not been able to send threats to Splunk for the past hour. This error is triggered when the same batch of threats has failed to be sent to Splunk ES for the last number of (badPollCount) polls.
You can perform the following tasks to customize the thresholds for when an error is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.OCS.threats.stuck.bad.polls property to customize the thresholds to trigger the error. The default is 12 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-outputconnector restart
What to do	
Verify that Splunk ES is up and running.
Restart the Output Connector Server:
Find the node on which the Output Connector Server is running from the Cluster Services page.
Log in to this node as caspida user and execute:
sudo service caspida-outputconnector stop
sudo service caspida-outputconnector start
The indicator's status will reset and its status will recalculate in the next hour. If the status returns to ERROR, contact Splunk Support.

Streaming Models (SML)
Splunk UBA displays several streaming model (SML) status codes to help you troubleshoot any issues with the streaming model services.

SML-1
Status	WARN
Error message	Time lag of the consumer exceeded x% of topic's retention(<topicRetention>) for <x mins/hrs>. Consumer may start missing events soon.
What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events. This warning is triggered when either of the following conditions are met:
One or more of instance time lag indicators on the topic is WARN.
The timeLag is greater than warnThreshold % of the topic's retention for warnPollCount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.SML.<topic>.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.SML.<topic>.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
SML-3
Status	WARN
Error message	Instance has not consumed events for <x mins/hrs>.
What is happening	The instance has not consumed any events in the specified interval of time. This warning is triggered when instance's timeLag on the topic (min(latestRecordTimes) across its partitions with eventLag > 0) has remained constant for warnPollCount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.SML.<firstModelInGroup>.<topic>.instance.timeLag.warn.pollcount property to customize the threshold for when a warning is triggered. The default is 24 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
Threat Computation Task (TC)
Splunk UBA displays several threat computation (TC) status codes to help you troubleshoot any issues with the threat computation task.

TC-1
Status	WARN
Error message	Threat computation is taking more than a certain number of minutes to complete.
What is happening	The last time that the threat computation process ran, it took more than duration number of minutes to compute threats in Splunk UBA. High system loads can contribute to a longer time period for threat computation.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.threatComputation.duration.warn property to customize the threshold for when a warning is triggered. The default is 120 minutes.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
The threat computation task is run as part of caspida-jobmanager and no restart is required for property changes to take effect.

What to do	Monitor this indicator for the next day or two. If it continues to appear, contact Splunk Support.
TC-2
Status	WARN
Error message	Graph computation and threat calculation is taking more than a certain number of minutes to complete.
What is happening	Graph computation is slowing the process of threat computation. High system load can contribute to a longer time period for graph computation of threats. This warning is triggered when the graph computation part of the latest invocation of the task took more than graphDuration minutes to complete.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.threatComputation.graphDuration.warn property to customize the threshold for when a warning is triggered. The default is 90 minutes.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
The threat computation task is run as part of caspida-jobmanager and no restart is required for property changes to take effect.

What to do	Monitor this indicator for the next day or two. If it continues to appear, contact Splunk Support.
TC-3
Status	WARN
Error message	The most recent threat computation failed.
What is happening	The most recent process to compute new threats failed to run.
What to do	The task runs hourly. If threat computation fails regularly or often, contact Splunk Support.
UBA ETL Service (ETL)
Splunk UBA displays several ETL status codes to help you troubleshoot any issues with the ETL services.

ETL-1-RawDataTopic
Status	WARN
Error message	Indicator is in WARN state for one of two reasons:
1. One/more of the instances have not consumed events since <x mins/hrs>
2. TimeLag of one/more instances is x% of topic's retention <topicRetention>) for <x mins/hrs>. Consumer may start missing events soon

What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events. This warning is triggered when timeLag > warnThreshold % of topic's retention for a specific number of polls (pollcount).
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.ETL.RawDataTopic.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.ETL.RawDataTopic.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
ETL-3-RawDataTopic
Status	WARN
Error message	Instance has not consumed events for <x mins/hrs>.
What is happening	The instance has not consumed any events during the specified period of time. This warning is triggered when the timeLag on the topic (min(latestRecordTimes) across its partitions with eventLag > 0) has remained constant for warn.pollcount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.ETL.RawDataTopic.instance.timeLag.warn.pollcount property to customize the threshold for when a warning is triggered. The default is 12 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
UBA Identity Service (IR)
Splunk UBA displays several identity resolution (IR) status codes to help you troubleshoot any issues with the IR services.

IR-1-PreIREventTopic
Status	WARN
Error message	Indicator is in WARN state because of one of the two reasons:
1. One/more of the instances have not consumed events since <x mins/hrs>
2. TimeLag of one/more instances is x% of topic's retention <topicRetention>) for <x mins/hrs>. Consumer may start missing events soon

What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events. This warning is triggered when the timeLag > warnThreshold % of topic's retention for warn.pollcount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.IR.PreIREventTopic.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.IR.PreIREventTopic.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
IR-3-PreIREventTopic
Status	WARN
Error message	Instance has not consumed events for <x mins/hrs>.
What is happening	The instance has not consumed any events during the specified period of time.
This warning is triggered when instance's timeLag on the topic (min(latestRecordTimes) across its partitions with eventLag > 0) has remained constant for warnPollCount number of polls.

You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the ubaMonitor.IR.PreIREventTopic.instance.timeLag.warn.pollcount property to customize the threshold for when a warning is triggered. The default is 12 polls.
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.
IR-1-IRTopic
Status	WARN
Error message	Indicator is in WARN state because of one of the two reasons:
1. One/more of the instances have not consumed events since <x mins/hrs>
2. TimeLag of one/more instances is x% of topic's retention <topicRetention>) for <x mins/hrs>. Consumer may start missing events soon

What is happening	Consumers in Splunk UBA consume events from topics in the Kafka queue. When an event has stayed on its topic for the configured retention period, it is dropped to make room for newer events. This warning is triggered when timeLag > warnThreshold % of topic's retention for warnPollCount number of polls.
You can perform the following tasks to customize the thresholds for when a warning is triggered:

Edit /etc/caspida/local/conf/uba-site.properties and add or edit the following properties:
warnThreshold: ubaMonitor.IR.RawDataTopic.timeLag.warn.threshold (default is 80 percent)
warnPollCount: ubaMonitor.IR.RawDataTopic.timeLag.warn.pollcount (default is 6 polls)
Synchronize the cluster in distributed Splunk UBA deployments:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the required services by running the following command on the management node:
sudo service caspida-sysmon restart
What to do	Keep monitoring for the next 4 hours. If the OK status does not return during that time, contact Splunk Support.


Collect diagnostic data from your Splunk UBA deployment
Collect diagnostic data from your Splunk UBA deployment to share with Splunk Support. You can download diagnostic data from all services (or specific services) in Splunk UBA for a specific time range.

In distributed deployments, Splunk UBA collects logs in the following manner:

If a service is running on a different node from where you are requesting the diagnostic data, Splunk UBA obtains the log file from the appropriate node.
For services that are running on multiple nodes, Splunk UBA collects the logs from all nodes where the service is running.
Download Splunk UBA diagnostic data from the web interface
Perform the following tasks to download Splunk UBA diagnostic data from the web interface:

In Splunk UBA, select System > Download Diagnostics.
Type the number of days into the past for which you want to collect and download diagnostic data into the Past Days field. For example, type 1 to download data from the previous 24 hours.
Select whether to download data from all services or only selected services.
(Optional) Select the check boxes for the services that you want to download data from.
Click OK to collect the diagnostic files as a tar.gz file.
After a few minutes a message appears prompting you to download the Splunk UBA diagnostics file.
Depending on the time period for which you want to download diagnostics, the process might take up to thirty minutes to return results. If your session ends before the diagnostic file becomes available, download diagnostic data from the command line instead. See Download Splunk UBA diagnostic data from the command line.

You cannot download Apache Spark events from Splunk UBA because it takes more than ten minutes to collect the logs. Download diagnostic data from the command line instead.

Download Splunk UBA diagnostic data from the command line
Perform the following tasks to download Splunk UBA diagnostic data from the command line:

Log in to the Splunk UBA management node as an admin user or a user who has permissions to access log files.
Create a configuration file defining the following parameters:
Parameter	Required?	Description
outputFolder	Yes	The location where the extracted tarball is written.
retentionPeriodInDays	No	The number of days worth of logs files to extract. The default is 5, meaning that log files older than 5 days are not extracted.
diskUsageThreshold	No	The disk usage limit as a percentage. Specify a value 0-100. The default is 80, meaning that if the following exceeds 80% then the log extraction is not performed:
(<size of all logs you want to extract, before compression> + <current used disk space>) / <total disk space>
modules	No	The modules whose logs you want to extract. If no modules are defined, then logs for all modules are extracted.
The /opt/caspida/conf/caspida-log-extractor-module-log-folder.json file defines all the Splunk UBA modules and where their logs are located.
The /opt/caspida/conf/caspida-log-extractor-service-to-modules.json file defines the mapping from Splunk UBA services listed in /opt/caspida/conf/deployment/caspida-deployment.conf to Splunk UBA modules.
Below is an example configuration file. CaspidaGeneral, CaspidaJobManager, and Spark logs from the last 5 days are extracted. The disk usage threshold is set to 90%:

{
    "outputFolder": "/home/caspida/log_extraction_test",
    "retentionPeriodInDays": 5,
    "diskUsageThreshold": 90,
    "modules": [
        {
            "name": "CaspidaGeneral"
        },
        {
            "name": "CaspidaJobManager"
        },
        {
            "name": "Spark"
        }
    ]
}
Run the following command:
python /opt/caspida/bin/log_extractor/CaspidaLogExtractor.py --config <path_to_configuration_file>
To extract the log files only on the host where CaspidaLogExtractor.py is run, use the --local parameter:

python /opt/caspida/bin/log_extractor/CaspidaLogExtractor.py --local --config <path_to_configuration_file>
The --local parameter causes the script to ignore the outputFolder field in the configuration file. The generated archive of diagnostic data is located in the /tmp/<ip_of_current_machine>_<YYMMDDHHMMSS>.tar.gz file.



Audit user activity in Splunk UBA
You can audit user activity in Splunk UBA by reviewing the audit logs in Splunk UBA or by sending audit logs to the Splunk platform for analysis.

User activity logged by Splunk UBA
Audit logs contain the name of the action performed, the type of the action performed, the username of the user performing the action, and the time of the action. In addition, if the action performed affects an entity such as a user or device, Splunk UBA logs the entity type, entity ID, and the URL for the affected entity details page. To see more details about an entity, search the entity ID in the table relevant to the entity type. For example, search an entity ID for a device in the Devices table.

By default, Splunk UBA retains three months of user activity audit logs.

Audit log example
For example, audit logs for the user jgonzalez downloading diagnostic data and then viewing the device details page for a device look as follows:

Action	Name	Details	Username	Time
Download Diagnostics		modules: All Modules
retentionPeriodInDays: 2	jgonzalez	Jun 13, 2017 2:14 PM
View Device Details	acme-61669202		jgonzalez	Jun 13, 2017 5:33 PM
The device name is a link to the device details page.

Types of user activity logged by Splunk UBA
Splunk UBA logs several types of activity for auditing. All actions performed by users in Splunk UBA are logged for auditing, including visits to dashboards and pages within the application.

Activity category	Specific behavior logged
Access and authentication activity	User logged in or out.
User account created, modified, or deleted.
Data source changes	Data sources added, modified, or deleted.
HR data configuration created
HR data configuration reset
Configure an output connector.
Installation activity	Install a content pack.
Install a new or updated license file.
Splunk UBA user activity	Navigation and filter activity in the user table.
Navigation and filter activity in the device table.
Navigate to the Models page.
Navigate to the Health Monitor page.
Threat review activity	Add a threat to a watchlist.
Add a threat to an allow list.
Anomaly review activity	Delete or restore an anomaly.
Change the score of an anomaly.
Add or remove an anomaly from a watchlist.
Deny list and allow list changes	Add or remove an entry from the deny list.
Add or remove an entry from the allow list.
Custom threat rule changes	Create a custom threat rule.
Modify or delete a custom threat rule.
PII masking behavior	Mask PII in Splunk UBA.
Unmask PII in Splunk UBA.
Review the audit logs in Splunk UBA
You can view the audit logs from the past three months in Splunk UBA. Select System > Audit Logs to open the audit logs.

Filter by time, action, username, or entity type to reduce the scope of the audit logs for review. For example, you can examine the activity of a specific user over a period of time, identify the users that performed a sensitive action in a period of time, locate the user that interacted with a specific threat in Splunk UBA, or show all activity for a narrow period of time.

Send audit logs to the Splunk platform for analysis
See Send Splunk UBA audit events to Splunk ES in Send and Receive Data from the Splunk Platform for complete instructions.

Manage the number of threats and anomalies in your environment
The Offline Rule Executor in Splunk UBA runs nightly to process the scheduled anomaly and threat rules, and also performs threat revalidation in real time when there are rule changes, anomalies are removed from the system, or anomaly scores are changed. Threat revalidation can take a long time and cause memory issues on your system depending on a variety of factors, including the types and age of the anomalies involved in the threat, the number or anomalies and entities involved in the threat, and any custom threat rules active in the system.

Perform regular maintenance of your Splunk UBA deployment by managing the number of threats and anomalies in your system.

When deleting large number of anomalies, do not delete more than 200,000 anomalies at a time.

Perform regular cleanup of anomalies more than 90 days old. See Delete anomalies in Splunk UBA.
Close unwanted threats. See Close threats in Splunk UBA.
Monitor the total number of anomalies in your environment.
If your deployment is fewer than 10 nodes, do not exceed 800,000 anomalies.
If your deployment is 10 nodes or more, do not exceed 1.5 million anomalies.
Monitor the number of rule-based threats in your environment.
If your deployment is fewer than 10 nodes, do not exceed 1,000 rule-based threats.
If your deployment is 10 nodes or more, do not exceed 2,000 rule-based threats.
The Offline Rule Executor times out in 15 minutes, meaning that if a threat rule takes longer than 15 minutes to complete, or threat revalidation takes longer than 15 minutes, some computations are lost and not generated in Splunk UBA. If a threat rule is taking longer than 15 minutes to complete, you can edit the rule parameters to try to shorten the time. See Monitor policy violations with custom threats.



## Customize Splunk UBA




## Send Data from Splunk UBA

## Troubleshoot Splunk UBA Event Processing

Troubleshoot Splunk UBA event processing
This section contains information to help you analyze activity and diagnose problems with event processing in your Splunk UBA deployment.

Identify all sourcetypes in your data
Run the following search to identify the sourcetypes in the data being ingested by the Splunk platform. Identifying sourcetypes is useful when you want to verify that you have the necessary data for Splunk UBA to function or to unlock desired use cases.

| metasearch index=*
| stats count by index, sourcetype
| table sourcetype, index, count

Identify all available indexes, sourcetypes, and EPS
Identify all available indexes, sourcetypes, and average events per second (EPS). The EPS value is important to make sure you are sizing your Splunk UBA cluster correctly. See Plan and scale your Splunk UBA deployment.

| tstats count as eps where index=* earliest=-30d@d group by index, sourcetype _time span=1s
| stats count as NumSeconds max(eps) perc99(eps) perc90(eps) avg(eps) as avg_eps by index, sourcetype
| addinfo
| eval PercentageOfTimeWithData = NumSeconds / (info_max_time -info_min_time)
| fields - NumSeconds info*
| eval EffectiveAverage = avg_eps * PercentageOfTimeWithData
| fieldformat PercentageOfTimeWithData = round(PercentageOfTimeWithData*100,2) . "%"

Events from a data source do not appear in Splunk UBA Web
Events from a data source are being processed but do not appear in Splunk UBA Web.

Cause	Solution
There might be a delay of up to 5 minutes before any information about processed events appears in Splunk UBA Web.	To view event processing details, add ?system into the URL.
In Splunk UBA Web, select Manage > Data Sources.
Select a data source. The URL in Splunk UBA Web might be something like:
https://ubaserver1/#Y2FzcGlk==
Add ?system into the URL. For example:
https://ubaserver1/?system#Y2FzcGlk==
Reload the page with the updated URL.
Additional information is displayed for that data source, such as EPS trend, events categorized by view or model, and connector statistics.

Active Directory events are not being parsed
You notice that some Active Directory (AD) events are not being parsed.

Cause	Solution
Invalid values are present in the EntityValidations.json file.	Invalid values cause the AD token resultCode to not be populated. This value is important for categorizing AD events. Open the /etc/caspida/local/conf/etl/configuration/EntityValidations.json file and see if 0x0 is present in the generic section. If so, remove it. If you do not have any customized values for invalidValues, remove the entire section or keep it empty, as shown below:
"invalidValues" : { }
If you edit the file, use proper JSON syntax with your edits.

Error messages when viewing contributing events
When viewing the contributing events for an anomaly, you receive an error message like the following:

Cannot find search head definition for endpoint https://<host>:<port> in splunk_search_head.json
To resolve this, check the following:

Make sure DNS is configured correctly on your system. All nodes in your Splunk UBA deployment must point to the same DNS server. If DNS is not configured correctly, you may see this error when you are trying to view contributing events over a VPN connection.
Verify that the following host names are an exact match. Use a fully qualified domain name (FQDN) in both of the following places:
Configuring Splunk UBA when you specify a Splunk platform host name. See Connect Splunk UBA to the Splunk Platform to view supporting events.
Configuring a data source in Splunk UBA. See Add data from the Splunk platform to Splunk UBA.

