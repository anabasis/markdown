# Get Data into Splunk User Behavior Analytics

## Introduction

Understand data flow in Splunk UBA
Get data into Splunk UBA from the Splunk platform (Splunk Enterprise or Splunk Cloud). Splunk UBA uses this data along with automated machine learning algorithms to generate anomalies and threats about the users, accounts, devices, and applications in your environment. Some common use cases include detecting compromised accounts and devices, detecting data exfiltration, detecting insider abuse, including privileged accounts abuse, and providing context and information for investigations.

This image shows how Splunk UBA uses data from the Splunk platform to generate anomalies and threats. Splunk UBA can send these anomalies and threats back to the Splunk platform for further analysis using integrated products such as Splunk Enterprise Security. Each element in the image is described in the table immediately following the image.

This image shows how data flows through and is used by Splunk UBA. The elements in the image are described in the text immediately following this image.

Step	Process	Description
1	Raw or Parsed Data from the Splunk platform	Splunk UBA can ingest parsed, CIM-compliant data or raw events from the Splunk platform. You must choose the proper connector type when getting data in to Splunk UBA. Use the Splunk Direct connector to parse events on the Splunk platform before ingesting them into Splunk UBA. Use the Splunk Raw Events connector to use Splunk UBA's native parsers instead of parsing the data on the Splunk platform.
See Use connectors to add data from the Splunk platform to Splunk UBA for more information about the connector types.
See How data gets into Splunk UBA for more information about how data gets ingested into Splunk UBA from the Splunk platform.
See Use connectors to add data from the Splunk platform to Splunk UBA for instructions.
2	Account Normalization and Identity Resolution	Splunk UBA analyzes the data from the Splunk platform and maps the fields in the data to Splunk UBA-specific fields. Properly mapped data fields enables Splunk UBA to do the following:
Normalize device and domain names, and associate all accounts identified in your HR data with a single human user.
Perform identity resolution to find the real-time association between IP addresses, host names, and users, and also maintain these associations over time.
3	Views Specific to each Data Type	Splunk UBA provides a level of abstraction so that you only see important, relevant data. For example, firewall logs can come in a variety of formats depending on the source vendor. Splunk UBA abstracts the variety in the data and shows you only the data relevant to firewall-related activity, such as IP, port, and information about the firewall action. This metadata is tagged with an appropriate view such as Firewall and stored in data cubes.

You can also configure generic data sources to bring raw events into Splunk UBA. These raw events belong to the generic view and can also stored in data cubes and are processed for anomalies. See What is the custom use case framework? in Develop Custom Content in Splunk User Behavior Analytics.

4	Anomaly Models	Anomaly models analyze the data in Splunk UBA and create anomalies based on a variety of factors.
Streaming models analyze ingested data in real time and determine the impact of those events over a short time window, such as the past hour. Based on this analysis, streaming models can produce a multitude of items in Splunk UBA, such as anomalies, indicators of compromise (IoCs), or analytics data.
Batch models and anomaly rules analyze ingested data over a larger time window, such as the last 24 hours, typically running overnight due to the need to process large amounts of data. All threat models in Splunk UBA run as batch models, taking into account the aggregation of data in Splunk UBA including the data cataloged by the streaming models. There are two types of batch models in Splunk UBA:
Rare event models generate anomalies by detecting unusual, rare, or first time activity.
Time series models generate anomalies by tracking specific activities over a period of time.
You can create new batch anomaly models or clone existing models using the custom use case framework. See What is the custom use case framework? in Develop Custom Content in Splunk User Behavior Analytics.


Anomalies are grouped into various categories such as Exfiltration, Infection, or Expansion. These categories typically correspond to stages of the kill chain and make it possible for your threat logic to place anomalies into the correct sections of the chain. Anomaly models also add metadata to anomalies. These metadata are specific to individual anomaly types and are used to filter the anomalies during investigation or anomaly hunting.

5	Anomalies	Anomalies are notable findings in the data, such as deviations from typical behavior or the detection of interesting patterns, like beaconing. A Splunk UBA operator can view anomalies and take further action as needed. Anomalies vary in scope and complexity, ranging from simple highlights of a useful alarm generated by an external product, such as a security endpoint solution or a firewall, to stealthy data exfiltration attempts requiring advanced statistical and machine learning models to detect. Anomalies are generated by the streaming models, batch models, and anomaly rules.
You can use anomaly action rules in Splunk UBA to manage existing anomalies in the system as part of the anomaly creation flow. For example, you can delete or restore anomalies, modify the score, or add anomalies to a watchlist. See Take action on anomalies with anomaly action rules.
You can also customize anomaly scoring rules to provide a level of control and consistency across specific anomaly types. See Customize anomaly scoring rules.
Anomalies have both types and categories. Types are specific descriptive names of anomalies, while categories are generic descriptions for anomalies. Multiple anomaly types can share a category, and one anomaly type can have multiple categories. Create an anomaly action rule or a custom threat with either anomaly types or anomaly categories, depending on how specific you want the rule or threat to be.

6	Threat Models and Custom Threat Rules	Threat models in Splunk UBA build dynamic threats based on the data and anomalies in the system.

You can also create custom threat rules to identify verifiable threats in your network, like specific activities that you want to monitor for policy compliance. Create, edit, enable, and manage the custom threats that are most useful for your organization. You can create custom threats that apply to users, devices, or sessions. Several custom threats are included with Splunk UBA.

Threat models take into account the aggregation of data in Splunk UBA, including the data cataloged by the streaming models, to generate threats. All threat models in Splunk UBA run as batch models.
Threat rules generate threats by looking for specific anomaly patterns within a specific window of time. A threat is generated each time the anomaly pattern is found. Each rule runs on a pre-defined schedule, depending on the nature of the rule.
7	Threats	A threat is a collection of one or more anomalies that form a clearly defined security use case, such as Data Exfiltration. Threats are often correlated with indicators of compromise (IoC) and other supporting evidence to provide a detailed description of a series of events.

Threats can be computed in the following ways:

Kill-chain threats examine all anomalies for a specific user or device for patterns that align with the kill-chain stages. Example kill-chain threats are Data Exfiltration by Suspicious User or Device or Data Exfiltration by Compromised Account.
Graph-based threats are computed based on groups of similar anomalies rather than anomalies grouped by user or device. Example graph-based threats are Public-facing Website Attack or Fraudulent Website Activity.
Data-driven threats collect data about anomalies and users or devices to determine the likelihood of a threat. Insider threats are computed by the data-driven computation process. Example data-driven threats are Lateral Movement and Data Exfiltration.
Rule-based threats are raised from custom threat rules when a specific set of conditions are met. Example rule-based threats are Brute Force and Data Exfiltration after Data Staging.

How data gets from the Splunk platform to Splunk UBA
Data is ingested into Splunk UBA from the Splunk platform in the following ways:

Splunk UBA performs time-based searches against the Splunk platform to pull data in to Splunk UBA. See Time-based search.
Splunk UBA performs real-time indexed queries against the Splunk platform to pull data in to Splunk UBA. See Real-time search.
The Splunk platform pushes data to Splunk UBA using Kafka ingestion. See Direct to Kafka.
Time-based search
Splunk UBA performs micro-batched queries in 1-minute intervals against the Splunk platform to pull in events. This is the default method for getting data into Splunk UBA.

Using time-based search enables Splunk UBA to provide monitoring services for the status of your data ingestion. To monitor the status of your data ingestion:

See the Splunk Data Source Lag indicator in View modules health.
See the error messages and descriptions in Data Sources (DS).
Monitor your Splunk UBA instance directly from Splunk Enterprise with the Splunk UBA Monitoring app. See About the Splunk UBA Monitoring app.
To configure the properties of the queries:

In the /etc/caspida/local/conf/uba-site.properties file, add or edit the properties in the table.
Run the following command to synchronize the cluster in distributed deployments:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
Run the following commands to stop and restart Caspida:
/opt/caspida/bin/Caspida stop
/opt/caspida/bin/Caspida start
Property	Description
splunk.live.micro.batching.delay.seconds	The point in time when Splunk UBA begins data ingestion. The default is 180 seconds (3 minutes) earlier than the start of the current minute. For example, if data ingestion is enabled at 10 seconds past 1:02 PM, then the beginning of the minute is 1:02 PM. Specifying a delay of 120 seconds means that the first batch query begins processing events at 1:00 PM. The query runs on the events within the specified interval of time defined by splunk.live.micro.batching.interval.seconds.
splunk.live.micro.batching.interval.seconds	The length of the time in seconds for each batch query.
The default is 60 seconds, meaning that a query is run every 60 seconds for 60 seconds worth of events, starting from the time defined by splunk.live.micro.batching.delay.seconds.
If you specify 120 seconds as the interval, then a query is run every 120 seconds for 120 seconds worth of events.
Do not configure the interval to exceed 4 minutes.

connector.splunk.max.backtrace.time.in.hour	The window of time that determines when to begin data ingestion after a data source is stopped and then restarted. The default backtrace time is 4 hours.
If a data source is stopped for a longer period of time than the configured connector.splunk.max.backtrace.time.in.hour interval, some events will be lost. For example, if a data source was stopped at 12:00AM and not restarted again until 6:00AM, and the connector.splunk.max.backtrace.time.in.hour is 4 hours, Splunk UBA will ingest events that occurred at 2:00AM. The events between 12:00AM and 2:00AM cannot be recovered.
If a data source is restarted inside the window of time configured by connector.splunk.max.backtrace.time.in.hour, Splunk UBA will continue to ingest events where it left off before the data source was stopped and attempt to catch up so there is no more lag. This is described in the text immediately below the table.
The search windows in Splunk UBA's micro-batch queries are expanded to ingest more events to compensate for lags during data ingestion. Searches are run every minute and for each search that takes less than 60 seconds, the search window is increased by 3 minutes to ingest a greater number of events. This enables Splunk UBA to gradually overcome a data ingestion lag, up to the point where data ingestion is back to the configured initial delay.

If any search takes more than 60 seconds to complete, the search window is reduced by 3 minutes, and the next search is issued immediately at the conclusion of the previous search. This is continued until the search can complete again in less than 60 seconds.

Consider the timeline In the following example, where a data source is stopped at 12:00AM and then restarted again at 1:00AM.

Search Start Time	Search Duration	Search Time Window	Description of data ingestion
1:00:00AM	4 seconds	1 minute	Ingest events occurring between 12:00AM - 12:01AM. Splunk UBA detects that there is a lag in the data ingestion. Since this search takes less than 60 seconds to complete, so the next search window is increased by 3 minutes.
1:01:00AM	6 seconds	4 minutes	Ingest events occurring between 12:01AM - 12:05AM. This search takes less than 60 seconds to complete, so the next search window is increased by 3 minutes.
1:02:00AM	22 seconds	7 minutes	Ingest events occurring between 12:05AM - 12:12AM. This search takes less than 60 seconds to complete, so the next search window is increased by 3 minutes.
1:03:00AM	67 seconds	10 minutes	Ingest events occurring between 12:12AM - 12:22AM. This search takes longer than 60 seconds to complete:
The next search is issued immediately after this search is completed, at 1:04:07AM.
The next search window is decreased by 3 minutes.
1:04:07AM	61 seconds	7 minutes	Ingest events occurring between 12:22AM - 12:29AM. This search takes longer than 60 seconds to complete:
The next search is issued immediately after this search is completed, at 1:05:08AM.
The next search window is decreased by 3 minutes.
1:05:08AM	26 seconds	4 Minutes	Ingest events occurring between 12:29AM - 12:33AM. This search takes less than 60 seconds to complete:
The next search is issued at the normal interval, 1 minute from the time the current search is issued.
The search window is increased by 3 minutes.
1:06:08AM	31 seconds	7 Minutes	Ingest events occurring between 12:33AM - 12:40AM.
This process continues until there is no more lag in the data ingestion, at which point the search window is returned to the default interval of 1 minute.

If a data source is stopped for a longer period of time than the configured connector.splunk.max.backtrace.time.in.hour interval, some events will be lost. For example, if a data source was stopped at 12:00AM and not restarted again until 6:00AM, and the connector.splunk.max.backtrace.time.in.hour is 4 hours, Splunk UBA will ingest events that occurred at 2:00AM. The events between 12:00AM and 2:00AM cannot be recovered.

Real-time search
Splunk UBA can perform real-time indexed queries against the Splunk platform to pull in events.

Use this method if you are experiencing problems with the default method of time-based data ingestion.

Set the following property and value in the /etc/caspida/local/conf/uba-site.properties file:
splunk.live.micro.batching=false
Synchronize the cluster in distributed deployments:
/opt/caspida/bin/Caspida sync-cluster /etc/caspida/local/conf
This method does not provide any monitoring services for your data ingestion. Only the default time-based search provides data ingestion health monitoring via the health monitor and Splunk UBA Monitoring app.

Direct to Kafka
Use this to push data from the Splunk platform to Splunk UBA when you have a single data source with EPS numbers in excess of 10,000.

See Send data from the Splunk platform directly to Kafka in the Splunk UBA Kafka Ingestion App manual.

How Splunk UBA handles data from different time zones
Splunk UBA uses the _time field as the timestamp for all events ingested from the Splunk platform. By default, the Splunk platform stores the UTC epoch time of the event in the _time field. See How timestamp assignment works in the Splunk Enterprise Getting Data In manual.

If the time zone on the Splunk platform is not configured with UTC epoch time in the _time field, you may see cases where anomalies and threats are generated later than expected.

See Add file-based data sources to Splunk UBA for information about How Splunk UBA handles time zones for file-based data sources.

Use connectors to add data from the Splunk platform to Splunk UBA
You can send events from the Splunk platform (Splunk Enterprise or Splunk Cloud) to Splunk UBA for analysis. Integrate Splunk UBA with the Splunk platform so that you can retrieve events and take advantage of the field extractions and data indexing that the Splunk platform performs.

Prerequisites for adding data from the Splunk platform to Splunk UBA
Before you can add data from the Splunk platform to Splunk UBA, make sure that your setup meets the minimum requirements. See Requirements for connecting to and getting data from the Splunk platform in Install and Upgrade Splunk User Behavior Analytics.

Add data from the Splunk platform using the Splunk Direct and Splunk Raw Events connectors
Splunk UBA provides two connectors for Splunk UBA to get data from the Splunk platform: Splunk Direct and Splunk Raw Events. The following table summarizes when to use which connector.

Your Data	Splunk UBA Connector Type	Where events are parsed	Documentation
Your data is verified CIM compliant. CIM-compliant data enables Splunk UBA to find expected fields, tags, and event types and recognize the type of data that is being ingested. Using CIM-compliant data also enables Splunk UBA to integrate with other Splunk tools such as Splunk Enterprise Security (ES) and Splunk IT Service Intelligence (ITSI) that also rely on CIM-compliant data.

Map your data to the CIM using the add-on builder or manually using tags. See Map to data model in the Splunk Add-on Builder User Guide or Use the CIM to normalize data at search time in the Splunk Common Information Model Add-on Manual.

Splunk Direct	The Splunk platform performs field extractions and data indexing before sending the data to Splunk UBA.	
See Ingest these data sources using the Splunk Direct connector for supported data sources.
See Use Splunk Direct to add a CIM-compliant data source to Splunk UBA for instructions.
Your data is partially or non-CIM compliant, but Splunk UBA has a native parser to process the data.	Splunk Raw Events	Raw events are sent to Splunk UBA. The events are parsed by Splunk UBA using its native parsers.	
See Ingest these data sources using the Splunk Raw Events connector for supported data sources.
See Add raw events from Splunk Enterprise to Splunk UBA.
Your data is not CIM compliant and Splunk UBA does not have a native parser to support the data format. Use this format to get data into Splunk UBA and generate custom content. See What is the custom use case framework? in the Develop Custom Content in Splunk User Behavior Analytics manual.	Splunk Direct	Use the Splunk platform to perform field extractions and data indexing before sending the data to Splunk UBA.	See Add custom data to Splunk UBA using the generic data source.
Which connector should I use for a particular data source?
The connector you use to get data into Splunk UBA depends on the type of data.

Splunk UBA requires human resources (HR) data and assets data from your Splunk platform before any other data is onboarded. Splunk UBA provides dedicated connectors for these specific data sources.
See Required data sources for Splunk UBA for information about required data sources.
See Ingest HR data and assets data using a dedicated data source type for information about how to ingest HR data and assets data.
After you get your HR data and assets data in to Splunk UBA, you can use the Splunk Direct and Splunk Raw Events connectors for additional data sources.
See Ingest these data sources using the Splunk Direct connector
See Ingest these data sources using the Splunk Raw Events connector
Ingest HR data and assets data using a dedicated data source type
The following data sources have a dedicated data source type in Splunk UBA used for ingesting data.

Data Source	How does Splunk UBA use this data?	How to Ingest
HR data from your HR system	HR data is required and must be the first data source ingested in Splunk UBA. See Why Splunk UBA requires HR data for more information.	Create a data source for your HR data. See Get HR data in to Splunk UBA.
Assets data from your CMDB, Enterprise Security, or Active Directory	Assets data is required and must be the second data source onboarded, immediately after HR data. See Identify assets in your environment for more information.	Create a data source for Splunk Assets. See Perform asset identification by using the Splunk Assets data source.
Ingest these data sources using the Splunk Direct connector
Ingest the following data source types using the Splunk Direct connector in Splunk UBA. See Add CIM-compliant data from the Splunk platform to Splunk UBA for instructions.

Data Source	How does Splunk UBA use this data?
Authentication	Splunk UBA uses authentication logs to unlock use cases such as Account Misuse and Compromised User Account.
Badge Access	Splunk UBA uses badge access data to unlock use cases such as Account Misuse and Compromised User Account.
Cloud Storage	Splunk UBA uses cloud storage data to unlock use cases such as Account Misuse, Compromised User Account, or Data Exfiltration.
Database	Splunk UBA uses database logs to perform database-related detections such as Excessive Database Records Deleted or Excessive Database Records Modified.
DHCP	DHCP data associates IP addresses to physical MAC addresses. Splunk UBA requires this data to perform identity resolution and to unlock use cases such as Compromised or Infected Machine, Data Exfiltration, and External Attack.
DLP	Splunk UBA uses DLP logs to unlock Data Exfiltration use cases.
DNS	DNS data provides associations between IP addresses and device names. Splunk UBA requires this data to perform identity resolution and to unlock use cases such as Compromised or Infected Machine.
Email	Splunk UBA uses this data to unlock uses cases such as Data Exfiltration, External Attack, or Account Misuse.
Endpoint	Splunk UBA uses endpoint data source to unlock use cases such as Lateral Movement, Compromised User Account, or Compromised or Infected Machine.
External Alarms	This category includes Splunk Enterprise Security (ES) notables, IPS/IDS, DLP, Malware, and Antivirus. Splunk UBA uses these data sources to unlock use cases such as Compromised or Infected Machine, Compromised User Account, Account Misuse, Lateral Movement and External Attack.

External alarms must be properly categorized so that they can be used by Splunk UBA. Each external alarm must belong to one of the anomaly categories listed in Filter the anomaly table by anomaly category in the Use Splunk User Behavior Analytics manual.

Firewall	Splunk UBA uses firewall data to unlock use cases such as Compromised or Infected Machine and Data Exfiltration. Events from both allowed and blocked traffic are analyzed.
Host AV	Splunk UBA uses host anti-virus data to unlock use cases such as Compromised or Infected Machine or Compromised User Account.
Network IDS/IPS	Splunk UBA uses intrusion detection and protection logs to unlock uses cases such as Compromised or Infected Machine or Compromised User Account.
Printer	Splunk UBA uses printer data to unlock use cases such as Data Exfiltration.
VPN	Splunk UBA requires this data to unlock use cases such as Account Misuse, Compromised User Account, or Data Exfiltration.
Web Proxy	Splunk UBA uses proxy data to unlock use cases such as Compromised or Infected Machine and Data Exfiltration.
Windows event logs in XML format	Splunk UBA requires Windows event data to unlock use cases such as Account Creation or Misuse, Compromised Machines or Accounts, and Lateral Movement. See Add Windows events to Splunk UBA.
Ingest custom data sources using the Splunk Direct connector
Use the Splunk Direct connector to onboard data that is not CIM compliant and or which Splunk UBA does not have a native parser to support the data format. Use this format to get data in to Splunk UBA for the custom use case framework.

See What is the custom use case framework?
See Add custom data to Splunk UBA using the generic data source for instructions.
Ingest these data sources using the Splunk Raw Events connector
Ingest the following data source types using the Splunk Raw Events connector in Splunk UBA. See Add raw events from the Splunk platform to Splunk UBA for instructions.

Data Source	How does Splunk UBA use this data?	Use this raw parser
Windows event logs in multiline format	Splunk UBA requires Windows event data to unlock use cases such as Account Creation or Misuse, Compromised Machines or Accounts, and Lateral Movement. See Add Windows events to Splunk UBA.	See Add Windows events to Splunk UBA.
Windows PowerShell logs	Log PowerShell activity and analyze the commands with Splunk UBA to identify indicators of compromise corresponding to malicious activity by a user or malware. PowerShell provides access to Windows API calls that attackers can exploit to gain elevated access to the system, avoiding antivirus and other security controls in the process. PowerShell is also internally utilized by popular hacking tools. See Configure PowerShell logging to see PowerShell anomalies in Splunk UBA.	See Add Windows events to Splunk UBA.
USB logs	USB logs unlock use cases such as Compromised or Infected Machine and Data Exfiltration. Only USB logs from Windows platforms are supported.	Symantec Endpoint Protection (SEP) AV/BEHAVIOR/RISK/SCAN/USB
Netflow logs	Netflow logs provide traffic flow information, showing where network traffic is coming from and going to and providing information about the volume of traffic being generated. Splunk UBA requires this data source to unlock use cases such as Lateral Movement.	NetFlow (nfdump)
Cisco logs	Cisco logs such as Cisco ASA provide firewall-related data used by use cases such as Data Exfiltration, Suspicious Communications, and Network Scanning. Cisco VPN logs provide data about login attempts, failed login attempts, and identity resolution.	Netflow-IPFix

Which data sources do I need?
Before adding data sources to Splunk UBA, review the tables to find which data source types you may need to unlock desired use cases and detections.

Required data sources for Splunk UBA to identify users and devices
Data sources for Splunk UBA to perform identity resolution
Data source types for use cases in Splunk UBA
Data source types for model-based anomalies in Splunk UBA
Data source types for rule-based anomalies in Splunk UBA
Required data sources for Splunk UBA to identify users and devices
Human resources (HR) data and assets data are required for Splunk UBA to generate high-fidelity anomalies and threats.

Data Source	How does Splunk UBA use this data?
HR data from your HR system	HR data is required and must be the first data source ingested in Splunk UBA. HR data contains information about the accounts being tracked by Splunk UBA. HR data is required by Splunk UBA to identify accounts and categorize account types, then associate each account with a human user. See Why Splunk UBA requires HR data for more information.
Assets data from your configuration management database (CMDB), Splunk Enterprise Security (ES), or Active Directory (AD)	Assets data is required and must be the second data source onboarded, immediately after HR data. Assets data contains information about the devices in your environment. Assets data is required by Splunk UBA to track the behavior of assets in your system, display additional metadata for known entities, and allow blacklisting of devices that should not be associated with users. Splunk UBA requires assets data with DNS to properly perform device identify resolution. See Identify assets in your environment for more information.
See Ingest HR data and assets data using a dedicated source type for information about how to ingest these data sources.

Data sources for Splunk UBA to perform identity resolution
Splunk UBA performs identity resolution to find the real-time associations between IP addresses, host names, and users. Splunk UBA maintains these associations over time and also allows you to prevent anomalies from being generated for specific users and devices. See Exclude identity resolution for devices or users for more information.

The most accurate identity resolution is achieved by having all of the data sources in the table, and you must have at least one. The absence of a data source, such as DNS, does not prevent Splunk UBA from performing identity resolution, but may affect whether or not entities are properly mapped or whether the mappings are maintained over time.

Splunk UBA uses the following data sources to perform identity resolution:

Data Source	How does Splunk UBA use this data?
Authentication	Splunk uses login events in authentication data to perform the following entity mappings:
IP addresses to hostnames
IP addresses to user accounts
Hostnames to user accounts
DNS	Splunk UBA uses DNS query response data to map IP addresses to hostnames.
DHCP	Splunk UBA uses log entries from new, renewed, or released leases to perform the following mappings:
IP address to MAC address
IP address to hostname
VPN	Splunk UBA uses login and logout events in VPN data to map IP addresses to users.
See Which connector should I use for a particular data source? for information about how to ingest each data source.

Data source types for use cases in Splunk UBA
After the required data sources are in Splunk UBA, ingest additional data sources to unlock detections for a variety of use cases in Splunk UBA. Splunk UBA provides the following use cases by default.

Splunk UBA Use Case	Description	Typical Contributing Factors and Data Sources
Account Misuse	Accidental misuse and deliberate abuse of superuser privileges yield critical compliance and privacy risks with potentially severe financial consequences and damage to your company's reputation. Splunk UBA baselines the regular behavior of each accounts and identifies abnormalities that may indicate excessive usage, rare access, potential sabotage, or covering tracks. Splunk UBA's confidence grows as a user's activity deviates from the user's peer group profile and the enterprise profile. The higher the confidence, the higher the risk. Examples of such detections include using service accounts to do VPN or interactive logins, data snooping, deleting audit logs, and accessing confidential information.	Data sources such as:
Authentication
Badge Access
Cloud Data
Email
Endpoint
External Alarm
VPN
AD (Windows Security Events). See Add Windows events to Splunk UBA.
Compromised User Account	Splunk UBA identifies situations where user credentials have been stolen and are being used by someone other than the authorized human user or application. This use case can also detect shared account usage and generic account abuse. Splunk UBA uses behavior modeling to identify any deviation of user activity from normal thereby indicating that someone other than the legitimate owner is operating the account. Detection encompasses identifying unusual or malicious AD activity such as operations on self, terminated users, disabled accounts, and account recovery.	Data sources such as:
Authentication
Badge Access
Cloud Data
Endpoint
External Alarm
VPN
AD (Windows Security Events). See Add Windows events to Splunk UBA.
Compromised and Infected Machine	Splunk UBA can identify compromised network endpoints that are infected by malware or are otherwise behaving suspiciously. This differs from the Compromised User Account use case in that malicious activity might be detected on a host but not necessarily linked to a specific user account. For example, command and control traffic can be identified from a system where no user is currently logged in. Behavior-based modeling enables Splunk UBA to identify malware activity irrespective of the delivery mechanism of initial infection. The detection techniques include tracking changes in communication patterns of devices, the nature of communication with external domains or IPs, or characteristics of the domains.	Data sources such as:
DLP
DNS
External Alarm
Firewall
HTTP
Network IDS/IPS
AD (Windows Security Events). See Add Windows events to Splunk UBA.
Contextual Intelligence	Splunk UBA learns a lot about users and entities in the organization to identify anomalies that could be linked to threats. This information is extremely useful for analysts performing alert triage and incident investigations. For example, if an analyst suspects that an endpoint has been compromised, the analyst can use Splunk UBA to learn about that desktop's users, their regular behavior, and even the role of that endpoint in the network. For example, is the endpoint a server or a workstation, and is it used for system administration or business functions?	Identity resolution, device profiler models, and data sources such as:
VPN
DHCP
DNS
Data Exfiltration	Unauthorized or malicious data exfiltration may occur even by action of authorized users. As a result, this use case is focused on identifying this type of activity, which is necessary even when the ability to detect compromised accounts and endpoints is in place. Splunk UBA detects loss or theft of private and confidential data out of enterprise across multiple threat vectors such as network security infrastructure including firewall and proxies, online cloud storage, attached storage including USB devices, and email.	Data sources such as:
Cloud Data
DLP
Email
Firewall
HTTP
Network IDS/IPS
Printer
VPN
Lateral Movement	Lateral movement involves a trusted insider scanning and expanding access across multiple resources. Detection techniques such as rare access or expanding resource usage are used to identify lateral movement. Resources here can be machines, network file shares, box folders etc. Accesses can either be network scans, brute force logins or legitimate logins.	Data sources such as:
External Alarm
Network IDS/IPS
AD (Windows Security Events). See Add Windows events to Splunk UBA.
Suspicious Behavior / Unknown Threats	In cases when there are not enough pre-defined signatures or correlations to cover some scenarios, Splunk UBA can effectively identify unknown scenarios by identifying anomalies based on deviations in the user or device activity in comparison with self or peer group baselines, suspicious or malicious activity, and alerts from external tools and correlating them into a threat. These suspicious account activities and unknown threats often demand further investigation and can lead to other potential threats such as malvertising, account compromise, account misuse, policy violations, or misconfiguration. The Suspicious Behavior / Unknown Threats use case is often used for content building. When an unknown scenario is detected, the scenario can be written into correlation search or threat rules for deterministic detection.	A combination of high scores or large number of anomalies associated with entities.
Data source types for model-based anomalies in Splunk UBA
Before adding data sources to Splunk UBA, review this table to find which types of anomalies can be generated for certain types of data. Click on a column header to sort the table by that column topic.

Anomaly rules typically have underscore characters in their names, while models do not. For example:

audit_log_cleared is an anomaly rule
Unusual Volume of Bytes Written to USB per User Model is an anomaly model
Data entering Splunk UBA is tagged with a view, which is sort of like a category in terms of how Splunk UBA interprets the data. For example, a network event from your CIM compliant IDS/IPS logs is tagged with the Network view by Splunk UBA. See Understand data flow in Splunk UBA.

The value of the specific destination device in this event can be extracted by Splunk UBA's rules and models using view.Network.DestinationDevice. This table identifies the specific fields whose values are used by Splunk UBA's anomaly rules and models to generate anomalies. See Understanding Splunk UBA data cubes in Develop Custom Content in Splunk User Behavior Analytics for more information about extracting the values of specific fields.

Anomaly	Model	Data Sources	View	Cube	Fields and Filters
Anomalous USB Activity	Unusual Volume of Bytes Written to USB per User Model	DLP
Endpoint
External Alarm

DLP	dlpsummary_s	view.*.user
view.*.user.uuid
view.data.portableDevice.deviceType
view.network.bytesFromClient
Filter:
view.data.portableDevice.deviceType='USB'

Unusual Volume of File Operations to USB per User Model	DLP
Endpoint
External Alarm

DLP	dlpsummary_s	numEvents
view.*.user
view.*.user.uuid
view.data.portableDevice.deviceType
Filter:
view.data.portableDevice.deviceType='USB'

Blacklisted Application	Fixed Patterns in Network Traffic Model	Firewall	AD	N/A	view.HTTP.getURL
view.Network.getDestinationDevice
view.Network.getSourceDevice
Filter:
Only incoming traffic is analyzed.

Blacklisted Domain	Blacklisted Entity Model	HTTP
DNS

DNS
HTTP
Network	semiaggr_s	view.*.user.id
view.*.user.name
view.*.source.id
view.*.source.name
view.*.source.scope
view.network.destination.id
view.network.destination.name
view.network.destination.scope
view.http.url.uui
view.http.url.domainName
view.*.externalAction
numEvents
Filter:
destinationScope == 'External' or sourceScope == 'External'

Blacklisted IP Address	Blacklisted Entity Model	Network IDS/IPS	DNS
HTTP
Network	semiaggr_s	
Download From Internal Server	Unusual Volume of Data Downloaded from Internal Server Per User Model	Firewall	Firewall	semiaggr_s	view.*.source.isPermanent
view.*.user
view.*.user.uuid
view.network.bytesToClient
view.network.destination.isPermanent
view.network.destination.scope
view.network.transfer
Filter:
(view.network.destination.scope is not null) AND (view.*.source.isPermanent = true) AND (view.network.destination.isPermanent = true)

Excessive Box Downloads	Unusual Volume of Box Downloads per User Model	Cloud Data	CloudData	fileaccess_s	event.eventClass
event.format
view.*.resource.size
view.*.srcUser
view.*.srcUser.uuid
Filter:
event.format = 'Box'

Excessive Data Printed	Unusual Volume of Data Printer per User Model	Printer	Printer	printerdata	view.*.User
view.*.User.uuid
view.printer.fileSize

Excessive Data Transmission	Unusual Volume of Data Uploaded per User Model	Network IDS/IPS
Firewall

Firewall	semiaggr_s	view.*.user
view.*.user.uuid
view.http.url.domainName
view.network.bytesFromClient
view.network.destination
view.network.destination.scope
Filter:
(view.http.url.domainName is not null) OR (view.network.destination is not null)

Unusual Volume of Data Uploaded per User Model (uses Connection Profiling)	Network IDS/IPS
Firewall

Firewall	semiaggr_s	view.*.user
view.*.user.uuid
view.network.bytesFromClient
view.network.destination.scope
view.network.transfer
Filter:
view.network.destination.scope is not null

Unusual Volume of Data Uploaded per Device Model	Network IDS/IPS
Firewall

Firewall	semiaggr_s	view.*.source
view.*.source.isPermanent
view.*.source.uuid
view.http.url.domainName
view.network.bytesFromClient
view.network.destination
view.network.destination.scope
Filter:
(view.http.url.domainName is not null OR view.network.destination is not null) AND view.*.source.isPermanent = true

Unusual Volume of Data Uploaded per Device Model (uses Connection Profiling)	Network IDS/IPS
Firewall

Firewall	semiaggr_s	view.*.source
view.*.source.isPermanent
view.*.source.uuid
view.network.bytesFromClient
view.network.destination.scope
view.network.transfer
Filter:
(view.network.destination.scope is not null) AND view.*.source.isPermanent = true

Excessive Database Administration Tasks	Unusual Volume of Admin commands per User Model	Database	Database	databasesummary	numEvents
view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
Filter:
view.database.commandName in ('Abort', 'UCAbort')

Excessive Database Help Actions	Unusual Volume of Help commands per User Model	Database	Database	databasesummary	numEvents
view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
Filter:
view.database.commandName like '%Help%'

Excessive Database Permission Grants	Unusual Volume of Grants per User Model	Database	Database	databasesummary	numEvents
view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
Filter:
view.database.commandName like '%Grant%'

Excessive Database Records Deleted	Unusual Volume of Database Records Deleted per User Model	Database	Database	databasesummary	view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
view.database.recordsAffected
Filter:
view.database.commandName rlike '^(Drop|Delete|Truncate).*$'

Excessive Database Records Modified	Unusual Volume of Database Records Modified per User Model	Database	Database	databasesummary	view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
view.database.recordsAffected
Filter:
view.database.commandName in ('Alter Table', 'Merge Into', 'Replace View', 'Update')

Excessive Database Records Read	Unusual Volume of Database Records Read per User Model	Database	Database	databasesummary	view.database.commandName
view.database.databaseUser
view.database.databaseUser.uuid
view.database.recordsAffected
Filter:
view.database.commandName in ('Select', 'Show')

Excessive Downloads via VPN	Unusual Volume of VPN Traffic per User Model	VPN	Network	semiaggr_s	view.*.user
view.*.user.uuid
view.authentication.loginServerType
view.network.bytesToClient
Filter:
view.authentication.loginServerType = 'VPN'

Excessive File Size Change	Excessive File Size Change Model	Cloud Data
Network IDS/IPS
Authentication

CloudData	fileaccess_s	view.*.srcUser.name
view.*.srcUser.id
view.*.resource.name
view.*.resource.size
view.*.resource.id
view.*.resource.type
view.*.parentPath.fileName
view.*.parentPath.id
event.eventClass
view.*.dstUser.name
view.*.dstUser.id

External Alarm Activity. See About the External Alarm and External Alarm Activity anomalies in Splunk UBA for more information.	External Alarm Analysis Model	External Alarm	ExternalAlarm	externalalarms	event.eventClass
view.*.riskClassification
view.*.externalAction
view.*.timeSlot
numEvents
view.*.alarmCategories
view.*.user.id
view.*.user.name
view.*.origin
view.*.origin.id
view.*.destination.id
view.*.destination.name
view.*.application.id
view.*.application.name

External Website Attack	Suspicious Patterns in Incoming Web Traffic Model	HTTP	Network	N/A	view.Network.source.{name, scope}
view.Network.destination.{name, scope}
view.HTTP.URL.host
Filter:
Only events with Network view are analyzed.

Land Speed Violation	Land Speed Violation Model	Authentication	Authentication	N/A	view.Network.source.{name, scope}
view.Network.destination.{name, scope}
view.Authentication.isLoginEvent
view.Authentication.ServerType
Filter:
Only incoming events with Authentication View, isLoginEvent = True, and ServerType = VPN

Machine Generated Beacon	Web Beaconing Detection Model	HTTP	AD	windowsevents	view.HTTP.URL
view.HTTP.URL.Host
view.HTTP.URL.Address
view.HTTP.URL.Path
event.TimeInMilliSeconds
view.HTTP.Method
view.HTTP.ClientIp
view.HTTP.BytesSent
view.HTTP.PeeringHost
view.HTTP.MediaType
view.HTTP.Application
view.HTTP.ExternalAction
view.HTTP.ClientIp.UUId
Filter:
view.http.URL.Host != null. AND view.http.URL != null AND view.http.ClientIp.DeviceScope == Internal

Malicious AD Activity	Fixed Patterns in Microsoft Windows Logs Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	N/A	event.getToken("evid")
view.*.user
view.Authentication.getLoginType
view.*.returnCode
event.getToken("authenticationpackage")
event.getToken("targetaccountname")
event.getToken("accountname")
view.*.accountDomain
view.*.application
Filter:
data format is "AD"

Multiple Authentication Errors	Unusual Volume of Authentication Failure Events per User Model	Authentication	Authentication	authenticationevents	view.*.user
view.*.user.uuid
view.authentication.isFailedLogin
view.authentication.isLogin
Filter:
view.authentication.isLogin > 0

Multiple Authentications	Unusual Volume of Authentication Events per User Model	Authentication	Authentication	authenticationevents	event.eventClass
view.*.user
view.*.user.uuid
view.authentication.isFailedLogin
view.authentication.isLogin
Filter:
view.authentication.isLogin > 0 AND event.eventClass != 'FailedLogin'

Multiple Badge Accesses	Unusual Volume of Badge Accesses per User Model	Badge Access	BadgeAccess	badgeaccess	numEvents
view.*.user
view.*.user.uuid

Multiple Box Login Errors	Unusual Volume of Box Login Failure Events per User Model	Cloud Data	CloudData	semiaggr_s	event.eventClass
event.format
view.*.user
view.*.user.uuid
view.authentication.isFailedLogin
view.authentication.isLogin
Filter:
event.format = 'Box' AND view.authentication.isLogin > 0 AND (event.eventClass = 'FailedLogin' OR event.eventClass = 'Login')

Multiple Box Logins	Unusual Volume of Box Login Events per User Model	Cloud Data	CloudData	semiaggr_s	event.eventClass
event.format
view.*.user
view.*.user.uuid
view.authentication.isLogin
Filter:
event.format = 'Box' AND view.authentication.isLogin > 0 AND event.eventClass = 'Login'

Multiple Box Operations	Unusual Volume of Box Events per User Model	Cloud Data	CloudData	fileaccess_s	event.eventClass
event.format
numEvents
view.*.srcUser
view.*.srcUser.uuid
Filter:
event.format = 'Box' AND event.eventClass != 'Sync' AND event.eventClass != 'UnSync'

Multiple External Alarms	Unusual Volume of External Alarms per Device Model	External Alarm	ExternalAlarm	externalalarm	numEvents
view.*.riskClassification
view.*.source.isPermanent
view.*.user
view.*.user.uuid
Filter:
view.*.source.isPermanent = true AND view.*.view.*.riskClassification is not null AND view.*.view.*.riskClassification != 'None'

Multiple File Operations	Unusual Volume of File Access Related Events per User Model	Cloud Data	CloudData	fileaccess_s	event.eventClass
view.*.resource.fileName
Multiple Login Errors	Unusual Volume of Failed Login Events per User Model	Authentication	Authentication	authenticationevents	view.*.user
view.*.user.uuid
view.authentication.isFailedLogin
view.authentication.isUserLogin
Filter:
view.authentication.isUserLogin > 0

Multiple Logins	Unusual Volume of VPN login Events per User Model	Authentication	Authentication	authenticationevents	view.*.user
view.*.user.uuid
view.authentication.isFailedLogin
view.authentication.isUserLogin
view.authentication.loginServerType
Filter:
view.authentication.isUserLogin > 0 AND view.authentication.isFailedLogin = 0 AND view.authentication.loginServerType = 'VPN'

Multiple Outgoing Connections	Unusual Volume of Outgoing Connections per Device Model	Firewall			numEvents
view.*.source
view.*.source.isPermanent
view.*.source.uuid
view.network.destination.scope
Filter:
(view.network.destination.scope = 'External') AND view.*.source.isPermanent = true

Unusual Volume of Outgoing Connections per User Model	Firewall			numEvents
view.*.user
view.*.user.uuid
view.network.destination.scope
Filter:
view.network.destination.scope = 'External'

Multiple Sessions Denial	Unusual Volume of Blocked Connections per User Model	Firewall			numEvents
view.*.externalAction
view.*.user
view.*.user.uuid
view.network.destination.scope
Filter:
view.network.destination.scope = 'External' AND view.*.externalAction = 'Denied'

Unusual Volume of Blocked Connections per Device Model	Firewall			numEvents
view.*.externalAction
view.*.source
view.*.source.isPermanent
view.*.source.uuid
view.network.destination.scope
Filter:
view.network.destination.scope = 'External' AND view.*.externalAction = 'Denied' AND view.*.source.isPermanent = true

Period with Unusual Windows Security Event Sequences	Active Directory Markov-Chain Correlation Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	AdPstIOC
AdPstModelReady

Potential Data Staging	Unusual Volume of Data Uploaded to DMZ Devices per User Model	External Alarm
Network IDS/IPS
Firewall

view.*.source.isPermanent
view.*.user
view.*.user.uuid
view.network.bytesFromClient
view.network.destination.scope
view.network.transfer
Filter:
(view.network.destination.scope is not null) AND view.*.source.isPermanent = true

Potential Webshell Activity	Web Shell Model	HTTP	HTTP	N/A	view.HTTP.URL.Host
view.HTTP.URL.Name
view.HTTP.URL.Path
event.TimeInMilliSeconds
view.HTTP.ClientIp
view.HTTP.BytesReceived
view.HTTP.PeeringHost
view.HTTP.MediaType
view.HTTP.ExternalAction
Filter:
view.http.URL != null && view.http.URL.Host != null

Scanning Activity	Network Scanning Detection Model	Firewall	Network	semiaggr_s	
Suspicious Account Lockout	Suspicious Account Lockout Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	view.ad.targetAccount
view.ad.eventClass
view.ad.eventStatus
view.ad.eventSubStatus
Filter:
events with event ID 4740 (account lockout), 4723 (password change), 4625 with substatus 0xC0000071 (password expiry) and 4724 (password reset)

Suspicious Data Access	Box Pattern Model	Cloud Data
Network IDS/IPS
Authentication

CloudData	fileaccess_s	view.*.srcUser
event.eventClass
view.*.resource.id
view.*.resource.name
view.*.resource.size
view.*.resource.type
view.*.parentPath.fileName
view.*.parentPath.id
view.*.destUser.name
view.*.source
view.event.application

O365 File Access Pattern Model	Cloud Data
Network IDS/IPS
Authentication

CloudData	fileaccess_s	view.*.srcUser
event.eventClass
view.*.resource.id
view.*.resource.name
view.*.resource.size
view.*.resource.type
view.*.parentPath.fileName
view.*.parentPath.id
view.*.destUser.name
view.*.source
view.event.application

Suspicious Data Movement	Device Exfiltration Model	Firewall	Firewall	semiaggr_s	view.*.application
view.firewall.destinationZone
view.network.bytesFromClient
view.data.portableDevice.deviceId
Filter:
destinationScope == 'External' and sourceScope == 'Internal'

User Exfiltration Model	Firewall	Firewall	semiaggr_s	view.*.application
view.network.destination.country
view.network.bytesFromClient
view.data.portableDevice.deviceId
Filter:
destinationScope == 'External' and sourceScope == 'Internal'

Suspicious Domain Communication	Malware Communication Model	Firewall
HTTP
DNS
External Alarm

Firewall
HTTP
DNS
External Alarm	N/A	view.DNS.Query
view.HTTP.URL.Host
view.HTTP.ApplicationType
view.DNS.ServerDevice.Port
view.HTTP.URL.Port
view.Network.DestinationDevice.Port
view.DNS.TTL
view.Network.BytesSent
view.Network.BytesReceived
view.Network.DestinationDevice
view.Network.SourceDevice
view.*.UserIds
view.HTTP.Referrer.Host
view.HTTP.BrowserInfo
view.HTTP.Method
view.HTTP.MediaType
view.HTTP.EventReturnCode
view.HTTP.ProtocolVersion
view.HTTP.URL.getPath
view.HTTP.ContentType
Filter:
NetworkView.getDestinationDevice is external

Suspicious Domain Name	Malware Communication Model	HTTP
DNS

HTTP
DNS	N/A	view.DNS.Query
view.HTTP.URL.Host
view.HTTP.ApplicationType
view.DNS.ServerDevice.Port
view.HTTP.URL.Port
view.Network.DestinationDevice.Port
view.DNS.TTL
view.Network.BytesSent
view.Network.BytesReceived
view.Network.DestinationDevice
view.Network.SourceDevice
view.*.UserIds
view.HTTP.Referrer.Host
view.HTTP.BrowserInfo
view.HTTP.Method
view.HTTP.MediaType
view.HTTP.EventReturnCode
view.HTTP.ProtocolVersion
view.HTTP.URL.getPath
view.HTTP.ContentType
Filter:
NetworkView.getDestinationDevice is external

Suspicious Email	Suspicious Email Detection Model	Email	Email	emailsummary	view.email.sender
view.email.senderDomain
view.email.recipients
view.email.hasAttachment
view.email.subject
view.email.attachmentSize

Suspicious HTTP Redirects	Browser Exploitation Model	HTTP	HTTP	N/A	
Suspicious Network Connection	Network Transport Model	Network IDS/IPS	Everything from the Network view	N/A	None
Suspicious Network Exploration	Users Increasing Device Access Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	view.ad.eventId
view.authentication.logonProcess
view.*.processName
view.ad.targetAccount.accountName
view.ad.sourceAccount.accountName
view.network.destination
view.ad.sourceDomain
view.ad.targetDomain
view.ad.rawSourceAddress
view.ad.rawTargetAddress
view.ad.rawSourceWorkstation
Filter:
dataFormat == "AD"

Suspicious New Access	New Access Model for Box	Cloud Data	CloudData	fileaccess_s	view.Data.*.srcUser
view.Data.*.destUser
view.Data.*.resource.fileName
event.application
event.eventClass
Filter:
Only events with log format BOX are analyzed.

Suspicious Powershell Activity	Powershell Detection Offline Model	Endpoint	Network	PowerShellEvent	
Suspicious Privilege Escalation	Suspicious Privilege Escalation Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	view.ad.targetAccount
view.ad.sourceAccount
view.ad.adEnabledPrivileges
view.network.destination
view.ad.eventId
Filter:
eventId = '4672' AND targetAccountName is not null

Unusual Activity Time	Unusual Per Day Activity Time Model, Unusual Per Week Activity Time Model	Authentication
Network IDS/IPS

Authentication	N/A	event.TimeInMilliSeconds
event.getAnyUser
Filter:
Authentication.isUserLoginEvent = True && Authentication.isLogoutEvent = False && Authentication.getAnyUser not null

Unusual Application Scope	Rare Egress Application Model	External Alarm
Firewall
Network IDS/IPS

Firewall	semiaggr_s	view.network.source.scope
view.firewall.possibleServerPort
view.http.applicationType
view.network.destination.scope
Filter:
view.network.source.isPermanent = true && view.network.source.scope = 'Internal' && view.network.destination.scope is not null

Unusual Database Activity	Rare Database Activity Model	Database	Database	databasesummary	Everything from the Database view
Unusual Entry Type Badge Reader Access	Rare Badge Reader Access Model	Badge Access	BadgeAccess	badgeaccess	
Unusual Error	Rare Microsoft Windows Events Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Event	Rare Microsoft Windows Events Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual External Alarm	Rare External Alarms Model	External Alarm	ExternalAlarm	externalalarms	Everything from the External Alarm view.
Filter:
view.externalalarm.source.isPermanent = true

Unusual File Access	Rare File Access Model	Cloud Data
Network IDS/IPS
Authentication

CloudData	fileaccess_s	event.eventClass
view.event.application
view.data.srcUser
view.data.destUser

Unusual Firewall Alarm	Frequent Pattern Mining of Firewall Alarms	Firewall	ExternalAlarm	semiaggr_s	view.*.source.uuid
view.*.source.scope
view.*.source.isPermanent
view.network.destination.uuid
view.network.destination.scope
view.network.destination.isPermanent
view.network.interactive
view.network.machine
event.eventClass
view.*.application.uuid
view.*.riskClassification
view.*.externalAction
Filter:
(view.*.source.scope = 'Internal' && view.*.source.isPermanent = true) OR (view.network.destination.scope = 'Internal' && view.network.destination.isPermanent = true)

Unusual Geolocation of Communication Destination	Rare Destination IP Geolocation Model	VPN	Firewall	semiaggr_s	
Unusual Login Domain	Rare Microsoft Windows Device Access Model Using Login Data	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Login Error	Rare Microsoft Windows Device Access Model Using Login Data	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Login Process	Rare Microsoft Windows Device Access Model Using Login Data	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Login Type	Rare Microsoft Windows Device Access Model Using Login Data	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Machine Access	Rare Microsoft Windows Device Access Model Using Login Data	Authentication
Network IDS/IPS

AD	windowsevents	view.ad.targetAccount.accountName
view.authentication.loginType
view.authentication.logonProcess
view.*.processName
view.ad.returnCode

Rare Microsoft Windows Device Access Model Using Authentication Data	Authentication
Network IDS/IPS

AD	windowsevents	view.ad.targetAccount.accountName
view.ad.returnCode

Unusual Network Activity	Rare Port for Application Model	Firewall	Firewall	remodelfeatures	view.firewall.possibleServerPort
view.firewall.application
view.firewall.sourceZone
view.firewall.destinationZone
view.firewall.source
view.network.destination
Filter:
view.firewall.source.isPermanent = true

Rare Destination IP Geolocation Model	Network IDS/IPS	Firewall	semiaggr_s	view.network.destination.country
view.network.user
view.network.source
Filter:
view.network.destination.scope = 'External' && view.*.source.isPermanent = true

Unusual Network Activity	Rare Port for Application Model	Firewall	Firewall	remodelfeatures	
Unusual Network Application	Rare Port for Application Model	Firewall	Firewall	remodelfeatures	
Unusual Network Port	Rare Port for Application Model	Firewall	Firewall	remodelfeatures	
Unusual Network Zone	Rare Port for Application Model	Firewall	Firewall	remodelfeatures	
Unusual Process or Process Path	Rare Microsoft Windows Events Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Resource Type	Rare Microsoft Windows Events Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	
Unusual Time of Badge Access	Unusual Time of Badge Access Model	Badge Access	BadgeAccess	badgeaccess	
Unusual USB Activity	USB Activity Model	Endpoint
External Alarm

DLP	dlpsummary_s	view.*.endpoint
view.*.user
view.Data.portableDevice.deviceType
view.Data.portableDevice.vendor
view.*.externalAction
Filter:
deviceType = USB and externalAction != Blocked

Unusual VPN Connection Sources	Unusual Change in Ratio of Users per Remote Source in Successful VPN Authentication Events	Authentication
Cloud Data
Endpoint
External Alarm
Firewall
HTTP, Network IDS/IPS
AD (Windows Security Events). See Add Windows events to Splunk UBA.

Authentication
Network	authenticationEvents	view.network.server.uuid
view.network.server
view.*.destUser.uuid
view.*.destUser
view.network.source.uuid
view.network.source
view.authentication.loginServerType
view.authentication.isSuccessfulLogin
Filter:
view.authentication.loginServerType = 'VPN' AND view.authentication.isSuccessfulLogin

Unusual VPN Login Geolocation	Rare VPN Login Location Model	Authentication
Network IDS/IPS

Firewall	semiaggr_s	view.authentication.source.country
view.authentication.user
view.authentication.source
Filter:
view.authentication.loginServerType = 'VPN' && view.network.source.scope = 'External'

Unusual Web Browser	Rare User Agent String Model	HTTP	HTTP	httpsummary_s	view.http.userAgentString
view.http.clientIp
Filter:
view.http.clientIp.isPermanent = true && view.http.clientIp = 'Internal'

Unusual Windows Security Event	Rare Microsoft Windows Events Model	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	view.ad.processName
view.ad.eventClass
view.ad.processPath
view.ad.returnCode
view.ad.targetAccount.accountName
view.ad.resource.resourceType
Filter:
view.network.source.isPermanent = true

Unusually Long VPN Session	Unusual VPN Duration Model	VPN	Network	semiaggr_s	
Data source types for rule-based anomalies in Splunk UBA
Anomaly	Rule	Data Sources	View	Cube	Fields and Filters
AD Audit Log Cleared	audit_log_cleared	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	eventId
AD Recovery Account	ad_recovery_account	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
Admin Change to Self	admin_changes_on_self	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	eventId
targetuser
username
useraccount

AmplificationDOS	amplification_dos_pan	Firewall	Firewall	semiaggr_s	destination
bytesin
bytesout
application

Confidential Print	potential_confidential_documents_printed	Printer	Printer	printerdata	fileName
Disabled Account Activity	disabled_account_activity	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	eventId
substatus
returncode
DLP Changed Name	dlp_changed_name	DLP	DLP	dlpsummary_s	sourcefile
destinationfile
type

DLP File Access Peer	pga_fileaccess	DLP	DLP	dlpsummary_s	sourcefile
DLP FIle Multiple Vectors	dlp_source_multiple_types	DLP	DLP	dlpsummary_s	eventTypeId
DLP Multiple Files	dlp_multiple_sourcefile	DLP	DLP	dlpsummary_s	sourcefile
DLP Multiple Vectors	dlp_multiple_types	DLP	DLP	dlpsummary_s	eventTypeId
DLP Print Violations	dlp_print_multiple_policy	DLP	DLP	dlpsummary_s	eventTypeId
policy

DLP Social and Credit	dlp_ssn_and_cc	DLP	DLP	dlpsummary_s	sourcefile
DLP Unusual Vector Peer	pga_dlptype	DLP	DLP	dlpsummary_s	N/A
DLP Web Personal	dlp_web_personal	DLP	DLP	dlpsummary_s	destinationpath
Email Attachment Size	data_transfer_over_email	Email	Email	emailsummary	attachmentSize
Email to Competitor	email_to_competitor	Email	Email	emailsummary	N/A
Email to Self	email_to_self	Email	Email	emailsummary	N/A
Failed Badge Accesses on Multiple Doors	Failed_Badge_Entry_Multiple_Doors	Badge Access	BadgeAccess	badgeaccess	objectName
High DLP Matches	daily_user_dlpmatches_anomaly	DLP	DLP	dlpsummary_s	matches
High File Writes	daily_user_dlp_file_transfer_anomaly	DLP	DLP	dlpsummary_s	destinationfile
High Print Job Count	daily_user_prints_anomaly	Printer	Printer	printerdata	eventTypeId
High Print Jobs Peer	pga_number_of_print_jobs	Printer	Printer	printerdata	N/A
High Printer Usage Peer	pga_number_of_pages	Printer	Printer	printerdata	totalPages
High USB Bytes	daily_user_usb_data_transfer_anomaly	DLP	DLP	semiaggr_s	deviceType
internalaction
eventname

High USB Denials	daily_user_usb_denies_anomaly	DLP	DLP	semiaggr_s	devicetype
internalaction

High USB Writes	daily_user_usb_file_write_anomaly	DLP	DLP	semiaggr_s	deviceType
internalaction
eventname

Host Data Deletion	csendpoint_high_datadeleton	Firewall	Firewall	semiaggr_s	N/A
Host Infection	csendpoint_high_infection	Firewall	Firewall	semiaggr_s	N/A
Host Lateral Movement	csendpoint_high_lateralmovement	Firewall	Firewall	semiaggr_s	N/A
HTTP Blacklisted Domain	download_from_suspicious_blacklisted_domain	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin
clientip
source
domain

HTTP Exfiltration Domain	http_transfer_to_storage_site	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin

HTTP Job Domain	job_search_proxy	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
HTTP Malware Domain	downlaod_from_suspicious_infection_domain	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin
clientip
source
domain

HTTP Phishing Domain	download_from_suspicious_credentialacces_domain	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin
clientip
source
domain

HTTP Policy Domain	download_from_suspicious_policyviolation_domain	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin
clientip
source
domain

HTTP Proxy Domain	usage_of_proxy_anonymizer	HTTP	HTTP	httpsummary_s	applicationtype (URL Category)
bytesout
bytesin

Local Account Created	local_account_creation	Windows Security Events (AD), Windows Security Events (Workstation)	AD	windowsevents	ComputerName
AccountDomain

Multiple Failed Entry Attempts	disabled_badge_access	Badge Access	BadgeAccess	badgeaccess	objectName
Multiple Password Resets	password_policy_circumvention	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	N/A
Multiple Users Failed Access	failed_access_multiple_users	Badge Access	BadgeAccess	badgeaccess	objectName
New AD Account	new_account_detected2	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
PAN Evasion Domain	suspicious_defenseevasion_uri_pan	Firewall	Firewall	semiaggr_s	category
PAN High Risk Domain	suspicious_policyviolation_uri_pan	Firewall	Firewall	semiaggr_s	category
PAN Job Search	job_search_pan	Firewall	Firewall	semiaggr_s	category
PAN Malware Domain	malicious_infection_uri_pan	Firewall	Firewall	semiaggr_s	category
PAN Phishing Domain	malicious_credentialaccess_uri_pan	Firewall	Firewall	semiaggr_s	category
PAN Unwanted Domain	suspicious_blacklisted_uri_pan	Firewall	Firewall	semiaggr_s	category
Print Unusual Extension Peer	pga_file_extension_printed	Printer	Printer	printerdata	fileName
Resume Sent	email_resume	Email	Email	emailsummary	hasAttachments
subject

Service Account AD	service_account_login_ad	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
Service Account VPN	service_account_login_vpn	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
Short Lived Account	account_creation_deletion_in_short_span	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	eventid
username
targetuser

Short Lived Security Membership	member_added_removed_in_short_span	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
Targeted Group Phishing	spear_phishing	Email	Email	emailsummary	evcls
Terminated Account Usage	terminated_user_activity	Any	Any	semiaggr_s	userstatus (from HR data)
Unauthorized Login Device	unauthorized_machine_login	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	N/A
Unauthorized Login Time	unauthorized_activity_time	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	N/A
Unauthorized Login Type	unauthorized_logintype	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	windowsevents	N/A
Unusual AD Event - Peer Group	pga_unusualadevent	AD (Windows Security Events). See Add Windows events to Splunk UBA.	AD	semiaggr_s	N/A
Unusual Cloud Storage Deletions	cloud_high_number_of_deletions	Cloud Data	CloudData	fileaccess_s	N/A
Unusual Cloud Storage Downloads	cloud_high_number_of_downloads	Cloud Data	CloudData	fileaccess_s	N/A
Unusual File Extension	cloud_unusual_fileextension_access	Cloud Data	CloudData	fileaccess_s	N/A
Unusual Printer Usage	potential_confidential_documents_printed	Printer	DLP	dlpsummary_s	
Unusual USB Device Plugged In	unusual_usb_plugin	DLP	DLP	semiaggr_s	deviceType
internalaction
deviceid

Unusual Web Protocol Exfiltration	suspicious_file_transfer	HTTP	HTTP	httpsummary_s	protocol
applicationtype (URL Category)

USB Storage Attached an Unusually High Number of Times	multiple_usb_plugs	DLP	DLP	semiaggr_s	deviceid
About the External Alarm and External Alarm Activity anomalies in Splunk UBA
In Splunk UBA releases earlier than 4.1, the External Alarm anomaly is raised when a notable event or external alarm category event from Splunk ES is ingested by Splunk UBA. In order for the anomaly to be triggered, the event's severity must be critical. The External Alarm anomaly was generated by a streaming model.

In Splunk UBA release 4.1 and later, the External Alarm anomaly is replaced by the External Alarm Activity anomaly. The External Alarm Activity anomaly is generated from the External Alarm Analysis Model offline model, and is triggered when the total number of notable events or external alarm category events from Splunk ES with a critical severity exceeds a certain threshold. You can view details for this anomaly in Data source types for model-based anomalies in Splunk UBA.

The External Alarm Activity uses alert grouping in both detection logic and presentation, meaning that there is not a one-to-one correspondence between the number of notable events and the number of External Alarm Activity anomalies for a user. For example, the Summary of external alarm activity panel on the Anomaly Details page for the External Alarm Activity anomaly may show that a user has only one External Alarm Activity anomaly associated with that user. Click on the event to expand the view and see that multiple External Alarm Activity anomalies are associated with that user.

Follow the instructions in Pull notable events from Splunk ES to Splunk UBA in the Send and Receive Data from the Splunk Platform manual to get notable events from Splunk ES to SplunK UBA.

Add Windows events to Splunk UBA
Windows security events from endpoints such as desktop systems or laptops are used by Splunk UBA to provide insight into system activity. You can also use Windows event data to associate IP addresses to device names and human users.

Windows events can be logged in many formats, with native multiline or XML being the most command formats. Splunk UBA can ingest Windows logs in both multiline and XML formats. A different method of ingestion is required for each, as described below:

Multiline format is the native format for Windows event logs. See How to get multiline Windows events in to Splunk UBA for instructions.
XML format is commonly found when interacting with Splunk Enterprise Security (ES), the Common Information Model (CIM), or later versions of the Splunk Add-on for Microsoft Windows. See How to get XML Windows events in to Splunk UBA for instructions.
How to get multiline Windows events in to Splunk UBA
Perform the following steps to get multiline Windows events in to Splunk UBA:

Verify that your Windows events are in multiline format. See What does a multiline Windows event look like?
Follow the steps in Use the Splunk Raw Events connector to get multiline Windows events in to Splunk UBA.
What does a multiline Windows event look like?
An example multiline Windows event is shown below:

11/18/2020 2:49:32 PM
LogName=Security
SourceName=Microsoft Windows security auditing.
EventCode=4624
EventType=0
Type=Information
ComputerName=ubanode.exampledomain.local
TaskCategory=Logon
OpCode=Info
RecordNumber=989284571
Keywords=Audit Success
Message=An account was successfully logged on.
Subject:
    Security ID:        NULL SID
    Account Name:       -
    Account Domain:     -
    Logon ID:       0x0
Logon Type:         3
Impersonation Level:        Impersonation
New Logon:
    Security ID:        EXAMPLEDOMAIN\ad_user1
    Account Name:       ad_user1
    Account Domain:     EXAMPLEDOMAIN
    Logon ID:       0xF13AE
    Logon GUID:     {3134bb44-1592-fc31-6404-b4b820e7507e}
Process Information:
    Process ID:     0x0
    Process Name:       -
Network Information:
    Workstation Name: 
    Source Network Address: -
    Source Port:        -
Detailed Authentication Information:
    Logon Process:      Kerberos
    Authentication Package: Kerberos
    Transited Services: -
    Package Name (NTLM only):   -
    Key Length:     0
Use the Splunk Raw Events connector to get multiline Windows events in to Splunk UBA
Perform the following steps to get your multiline Windows events in to Splunk UBA. See Add raw events from the Splunk platform to Splunk UBA for detailed instructions about how to add data sources using the Splunk Raw Events connector.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select Splunk as the data source type.
Click Next.
Specify a name for the data source, such as Splunk.
Type a connection URL that matches the URL for your Splunk platform search head and management port. For example, https://splunksearchhead.splunk.com:8089. If you have search head clustering configured and a load balancer is available, you can specify the load balancer host name to avoid a single point failure. Ensure that port 8089 is accessible on the load balancer.
Type the user name and password for the Splunk platform account.
Select a Connector Type of Splunk Raw Events.
Click Next.
Select a time range, such as Live and All time.
Click Next.
Click Splunk Query and add the name of your index as the query. For example:
index=<your_multiline_Windows_index_name>

Select Single Format, then click in the drop-down list and select Windows Event Log (Multiline).
Click Next.
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK.
How to get XML Windows events in to Splunk UBA
Perform the following steps to get multiline Windows events in to Splunk UBA:

Verify that your Windows events are in XML format. See What does an XML Windows event look like?
Follow the steps in Use the Splunk Direct connector to get XML Windows events in to Splunk UBA.
What does an XML Windows event look like?
An example XML Windows event is shown below:

<?xml version="1.0"?>
<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">
  <System>
    <Provider Name="Microsoft-Windows-Security-Auditing" Guid="{54849625-5478-4994-A5BA-3E3B0328C30D}"/>
    <EventID>4624</EventID>
    <Version>2</Version>
    <Level>0</Level>
    <Task>12544</Task>
    <Opcode>0</Opcode>
    <Keywords>0x8020000000000000</Keywords>
    <TimeCreated SystemTime="2015-11-12T00:24:35.079785200Z"/>
    <EventRecordID>211</EventRecordID>
    <Correlation ActivityID="{00D66690-1CDF-0000-AC66-D600DF1CD101}"/>
    <Execution ProcessID="716" ThreadID="760"/>
    <Channel>Security</Channel>
    <Computer>WIN-GG82ULGC9GO</Computer>
    <Security/>
  </System>
  <EventData>
    <Data Name="SubjectUserSid">S-1-5-18</Data>
    <Data Name="SubjectUserName">WIN-GG82ULGC9GO$</Data>
    <Data Name="SubjectDomainName">WORKGROUP</Data>
    <Data Name="SubjectLogonId">0x3e7</Data>
    <Data Name="TargetUserSid">S-1-5-21-1377283216-344919071-3415362939-500</Data>
    <Data Name="TargetUserName">Administrator</Data>
    <Data Name="TargetDomainName">WIN-GG82ULGC9GO</Data>
    <Data Name="TargetLogonId">0x8dcdc</Data>
    <Data Name="LogonType">2</Data>
    <Data Name="LogonProcessName">User32</Data>
    <Data Name="AuthenticationPackageName">Negotiate</Data>
    <Data Name="WorkstationName">WIN-GG82ULGC9GO</Data>
    <Data Name="LogonGuid">{00000000-0000-0000-0000-000000000000}</Data>
    <Data Name="TransmittedServices">-</Data>
    <Data Name="LmPackageName">-</Data>
    <Data Name="KeyLength">0</Data>
    <Data Name="ProcessId">0x44c</Data>
    <Data Name="ProcessName">C:\\Windows\\System32\\svchost.exe</Data>
    <Data Name="IpAddress">127.0.0.1</Data>
    <Data Name="IpPort">0</Data>
    <Data Name="ImpersonationLevel">%%1833</Data>
    <Data Name="RestrictedAdminMode">-</Data>
    <Data Name="TargetOutboundUserName">-</Data>
    <Data Name="TargetOutboundDomainName">-</Data>
    <Data Name="VirtualAccount">%%1843</Data>
    <Data Name="TargetLinkedLogonId">0x0</Data>
    <Data Name="ElevatedToken">%%1842</Data>
  </EventData>
</Event>
Use the Splunk Direct connector to get XML Windows events in to Splunk UBA
Perform the following steps to get your XML Windows events in to Splunk UBA. See Add CIM-compliant data from the Splunk platform to Splunk UBA for detailed instructions about how to add data sources using the Splunk Direct connector. The procedure for adding XML Windows events into Splunk UBA is the same as adding a CIM-compliant data source, except that you will not select the CIM Compliant checkbox during the procedure.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select a data source type of Splunk.
Click Next.
Specify a name for the data source, such as Splunk.
Type a connection URL that matches the URL for your Splunk platform search head and management port. For example, https://splunksearchhead.splunk.com:8089. If you have search head clustering configured and a load balancer is available, you can specify the load balancer host name to avoid a single point failure. Ensure that port 8089 is accessible on the load balancer.
Type the user name and password for the Splunk platform account.
Leave or select Splunk Direct as the connector type.
Do not check the CIM Compliant check box.

Click Next.
Select a time range, such as Live and All time.
Click Next.
Select Splunk Query and enter the following search in the field. Replace <YOUR_INDEX_NAME> with the name of your XML Windows events index.
index="<YOUR_INDEX_NAME>" | eval uba_source_type="ad" | 
spath output='Event.System.Computer' path=Event.System.Computer |
spath output='Event.System.Level'    path=Event.System.Level |
spath output='Event.System.Version'  path=Event.System.Version |
rex "\<Provider Name=\'(?<SourceName>[^\']+)\'" |
eval
       Account_Domain=SubjectDomainName, 
       Account_Name=SubjectUserName, 
       Application_Name=if(isnotnull(Application), Application, app), 
       Authentication_Package=AuthenticationPackageName,
       Caller_Computer_Name=if(isnotnull(TargetDomainName), TargetDomainName, CallerComputerName), 
       Caller_Process_ID=if(isnotnull(CallerProcessId), CallerProcessId, ProcessId),
       Caller_Process_Name=if(isnotnull(CallerProcessName), CallerProcessName, ProcessName), 
       Client_Address=IpAddress, 
       Client_Port=IpPort, 
       commandArgs=CommandLine, 
       ComputerName=Computer,
       Creator_Process_ID=ProcessId, 
       Error_Code=Status, 
       EventCode=EventID, 
       EventType=if(isnotnull(EventType),EventType, "0"), 
       Failure_Code=Status, 
       Failure_Reason=FailureReason, 
       Group_Domain=TargetUserName, 
       Group_Name=TargetDomainName, 
       Handle_ID=HandleId, 
       Impersonation_Level=ImpersonationLevel, 
       LogName=Channel, 
       Logon_Account=TargetUserName, 
       Logon_GUID=LogonGuid, 
       Logon_ID=if(isnotnull(SubjectLogonId), SubjectLogonId, TargetLogonId), 
       Logon_Process=LogonProcessName, 
       Logon_Type=LogonType, 
       Network_Address=IpAddress, 
       New_Account_Name=TargetUserName, 
       New_Process_ID=NewProcessId, 
       New_Process_Name=NewProcessName, 
       New_Security_ID=TargetUserSid, 
       Object_Name=ObjectName, 
       Object_Server=ObjectServer, 
       Object_Type=ObjectType, 
       Operation_Type=OperationType, 
       Privileges=PrivilegeList, 
       Privileges_Used_for_Access_Check=PrivilegeList, 
       Process_Command_Line=CommandLine,  
       Process_ID=ProcessId, 
       Process_Name=ProcessName, 
       Protocol=Protocol, 
       Result_Code=Status, 
       Security_ID=SubjectUserSid, 
       Server=Computer, 
       Service_ID=ServiceSid, 
       Service_Name=ServiceName, 
       Share_Path=ShareLocalPath,   
       Source_Address=SourceAddress, 
       Source_Network_Address=IpAddress, 
       Source_Port=if(isnotnull(IpPort), IpPort, SourcePort), 
       Source_Workstation=Workstation, 
       Status=Status, 
       Sub_Status=SubStatus, 
       Supplied_Realm_Name=TargetDomainName, 
       Target_Server_Name=TargetServerName, 
       Ticket_Encryption_Type=TicketEncryptionType, 
       Ticket_Options=TicketOptions, 
       Token_Elevation_Type=TokenElevationType, 
       Transaction_ID=TransactionId, 
       User_ID=TargetSid, 
       Workstation_Name=if(isnotnull(WorkstationName), WorkstationName, Workstation_Name) 
   | fields 'Event.System.Computer',  
       'Event.System.Level', 
       'Event.System.Version',
       Access_Mask, 
       Accesses, 
       Account_Domain, 
       Account_Name, 
       Application_Name, 
       Authentication_Package, 
       Caller_Computer_Name, 
       Caller_Process_ID, 
       Caller_Process_Name, 
       Client_Address, 
       Client_Port, 
       commandArgs, 
       ComputerName, 
       Creator_Process_ID,
               Disabled_Privileges,
               Enabled_Privileges, 
       Error_Code, 
       EventCode, 
       EventType, 
       Failure_Code, 
       Failure_Reason, 
       Group_Domain, 
       Group_Name, 
       Handle_ID, 
       Impersonation_Level,
       Keywords, 
       LogName, 
       Logon_Account, 
       Logon_GUID, 
       Logon_Process, 
       Logon_Type,
       Network_Address,
       New_Account_Name,
       New_Process_ID,
       New_Process_Name,
       New_Security_ID,
       Object_Name,
       Object_Server,
       Object_Type,
       Operation_Type,
       Privileges,
       Privileges_Used_for_Access_Check,
       Process_ID,
       Process_Name,
       Protocol,
       Result_Code,
       Security_ID,
       Server,
       Service_ID,
       Service_Name,
       Share_Path,
       SourceName,
       Source_Address,
       Source_Network_Address,
       Source_Port,
       Source_Workstation,
       Status,
       Sub_Status,
       Supplied_Realm_Name,
       Target_Server_Name,
       Ticket_Encryption_Type,
       Ticket_Options,
       Token_Elevation_Type,
       Transaction_ID,
       User_ID,
       Workstation_Name, 
       action,
       privilege_id, 
       sourcetype, 
       uba_source_type

Click Next.
Select Single Format as the data format, then click in the drop-down list and select AD.
Click Next.
Remove everything in the Splunk Query field, and paste the same search from earlier in this procedure here.
Click Next.
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK to save the data source.
Which Windows events are used by Splunk UBA?
The raw parser in Splunk UBA doesn't look for specific Windows events, Rather, all Windows events are analyzed to find common field names such as account name or workstation. These field names are extracted from Windows events and stored in data cubes to be consumed by anomaly rules and models. Having the right Windows events in Splunk UBA can lead to meaningful detections so that the desired security use cases are unlocked.

See the following categories of Windows events used by Splunk UBA:

Highly recommended Windows events used by Splunk UBA
Recommended Windows events used by Splunk UBA
Nice to have Windows events used by Splunk UBA
Highly recommended Windows events used by Splunk UBA
Ingest the events listed in this table so that Splunk UBA can generate the proper anomalies and threats. See Which data sources to I need? to identify the anomalies and threats generated by Windows events. The absence of any of the listed events will prevent anomalies and threats from being generated.

Windows Event ID	Description
4624	An account was successfully logged on.
4776	The computer attempted to validate the credentials for an account.
4768	A Kerberos authentication ticket (TGT) was requested.
4769	A Kerberos service ticket was requested.
4625	An account failed to logon.
4634	An account was logged off.
Recommended Windows events used by Splunk UBA
It is recommended to log the following Windows event types so that Splunk UBA can generate anomalies and threats.

Windows Event ID	Description
1102	The audit log was cleared.
Nice to have Windows events used by Splunk UBA
The following Windows event types enhance the fidelity of your detections by providing additional evidence and clarity.

Windows Event ID	Description
Windows PowerShell events
4103	PowerShell Module Logging. See Configure PowerShell logging to see PowerShell anomalies in Splunk UBA.
4104	PowerShell Script Block Logging. See Configure PowerShell logging to see PowerShell anomalies in Splunk UBA.
Windows object and registry handling events
4657	A registry value was modified.
4691	Indirect access to an object was requested.
4692	Backup of data protection master key was attempted.
4693	Recovery of data protection master key was attempted.
4695	Unprotection of auditable protected data was attempted.
4907	Auditing settings on object were changed.
4911	Resource attributes of the object were changed.
5145	A network share object was checked to see whether client can be granted desired access.
Windows domain, trust, and authentication events
4706	A new trust was created to a domain.
4713	Kerberos policy was changed.
4715	The audit policy (SACL) on an object was changed.
4772	A Kerberos authentication ticket request failed.
4820	A Kerberos Ticket-granting-ticket (TGT) was denied because the device does not meet the access control restrictions.
Windows policy events
6273	Network Policy Server denied access to a user.
6276	Network Policy Server quarantined a user.
6277	Network Policy Server granted access to a user but put it on probation because the host did not meet the defined health policy.
Windows account handling events
4627	Group membership information.
4704	A user right was assigned.
4718	System security access was removed from an account.
4719	System audit policy was changed.
4727	A security-enabled global group was created.
4728	A member was added to a security-enabled global group.
4729	A member was removed from a security-enabled global group.
4730	A security-enabled global group was deleted.
4731	A security-enabled local group was created.
4732	A member was added to a security-enabled local group.
4733	A member was removed from a security-enabled local group.
4734	A security-enabled local group was deleted.
4735	A security-enabled local group was changed.
4737	A security-enabled global group was changed.
4744	A security-disabled local group was created.
4745	A security-disabled local group was changed.
4746	A member was added to a security-disabled local group.
4747	A member was removed from a security-disabled local group.
4750	A security-disabled global group was changed.
4754	A security-enabled universal group was created.
4755	A security-enabled universal group was changed.
4756	A member was added to a security-enabled universal group.
4757	A member was removed from a security-enabled universal group.
4758	A security-enabled universal group was deleted.
4759	A security-disabled universal group was created.
4760	A security-disabled universal group was changed.
4761	A member was added to a security-disabled universal group.
4763	A security-disabled universal group was deleted.
4767	A user account was unlocked.
4781	The name of an account was changed.
4782	The password hash an account was accessed.
4797	An attempt was made to query the existence of a blank password for an account.
4798	A user's local group membership was enumerated.
4799	A security-enabled local group membership was enumerated.
Windows device handling events
4800	The workstation was locked.
4801	The workstation was unlocked.
6416	A new external device was recognized by the system.
Windows security incidents events
4618	A monitored security event pattern has occurred.
4649	A replay attack was detected.
Windows firewall policy changes events
4946	A change has been made to Windows Firewall exception list. A rule was added.
4947	A change has been made to Windows Firewall exception list. A rule was modified.
4948	A change has been made to Windows Firewall exception list. A rule was deleted.
4950	A Windows Firewall setting has changed.





## Prepare to add data to Splunk UBA

Get data into Splunk UBA
Splunk UBA uses data from the Splunk platform to identify potential insider and external threats to your environment. Work with Splunk Professional Services to get started with importing important data sources and filtering events.

Add data sources to Splunk UBA
Complete the following steps to properly get data into Splunk UBA.

Verify you have the correct permissions. See Requirements for connecting to and getting data from the Splunk platform.
(Optional) See which data source types are supported in Splunk UBA. See View supported data source types and prepare to add data sources to Splunk UBA.
Get HR data into Splunk UBA. See Get HR data into Splunk UBA.
Get assets and identity data into Splunk UBA. See Identify assets in your environment.
Configure allow lists and deny lists in Splunk UBA for domains, IP addresses, or users. See Use allow and deny lists to generate or suppress anomalies.
Get data from the Splunk platform into Splunk UBA. See Use connectors to add data from the Splunk platform to Splunk UBA. You can get started with a smaller dataset before ingesting all of your data. See Get started with a small dataset.
Review and verify your data sources. See Verify that you successfully added the data source.
View supported data source types and prepare to add data sources to Splunk UBA
Before you add new data sources, review the types of data that you want to add and determine which ones Splunk UBA supports. See Which data sources do I need?.

Perform the following tasks to view the data source types supported by Splunk UBA:

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Review the data source types on the Data Source Type page. The supported data source types that can be added to Splunk UBA are listed on this page.
After you determine which data sources you can add, make sure that existing event filters do not affect the new data sources. Review the existing event filters to check for settings that negatively affect future data uploads. For example, an event filter that excludes source_IP data from one data source will affect the new data source. Modify the filters as needed as new data sources are added.

Splunk UBA provides support for English language logs only.

Get started with a small dataset
Get started with a smaller set of data before working in a full production environment. This is useful for verifying that the data coming into Splunk UBA is properly configured and mapped so that you see the desired anomalies and threats.

There are several ways to use a small dataset to get started in Splunk UBA:

You can add data from a file to test on a small scale. See Add file-based data sources to Splunk UBA.
You can add data from Splunk software to Splunk UBA in test mode, where Splunk UBA analyzes a sample set of data from the data source. See Add data sources to Splunk UBA in test mode.
You can create an event filter, which is useful for limiting or targeting the data you are analyzing. You can apply filters to include or exclude devices or users. See Filter events analyzed by Splunk UBA for anomalies.

Add file-based data sources to Splunk UBA
Add new file-based data sources to Splunk UBA. You can use file-based data sources for testing on a small scale.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select the type of data source you want to add.
Click Next.
Enter a Name to identify the data source in Splunk UBA.
Upload the file.
Click OK.
Because file-based data sources represent static data, you can write a script to create new files periodically, and then load this data into Splunk UBA.

When ingesting file-based events, Splunk UBA extracts the timestamp from events. In most cases, file-based events do not have a time zone associated with the events, so Splunk UBA uses UTC as the default time zone. If you do not want to use UTC as the time zone, perform the following tasks:

Log in to the management node of your Splunk UBA deployment as the caspida user.
Edit the /etc/caspida/local/conf/uba-site.properties file and add the parser.global.input_timezone property. For example, to set the property to Pacific Standard Time (Los Angeles):
parser.global.input_timezone=America/Los_Angeles
Synchronize the cluster if you have a distributed deployment:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Stop and start The Splunk UBA containers:
/opt/caspida/bin/Caspida stop-containers
/opt/caspida/bin/Caspida start-containers

Add data sources to Splunk UBA in test mode
Add data sources to Splunk UBA in test mode to validate that Splunk UBA is importing the data sources successfully and accurately. Test mode is most useful when validating data sent to Splunk UBA from other Splunk software, such as Splunk Enterprise or Splunk Enterprise Security.

Test mode does not work in the following situations:

File-based data sources such as event files
Human Resources (HR) data, because HR data does not contain events
Assets data, because assets data does not contain events
Do not clone a test mode data source. Instead, after you validate that the test mode data is satisfactory, create a new data source to add the desired data.
Test mode results with Kafka ingestion verifies the following:

The validity of the SPL
The events returned from the Splunk platform and how they are parsed
The views obtained form the parsed events
Test mode with Kafka data ingestion doesn't verify whether or not the indexer is writing to the Splunk UBA Kafka topic.

Add data sources in test mode
Test mode processes events for validity with the event parser but does not process the events for anomalies. Test mode imports the first 10,000 events or 5 minutes worth of events from a data source, whichever happens first. You cannot modify the time constraint.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select a data source format.
Fill out the required fields for the data source.
Click Next.
Leave the check box for Test Mode selected.
Click OK.
Review the results of data source validation
After data source test mode completes or stops, review the results of test mode validation. Allow five minutes or more for the results of data source test mode to appear. Test mode validation results do not appear for data sources in distributed mode.

Click the data source name in the list of data sources.
Review the Test Mode Views Validation to compare the number of processed events with the valid events for each view type.
This screen image shows the data source page for an example data source called SplunkEnterprise. Data from this data source was ingested in test mode. The Test Mode Views Validation section shows a table with the following columns: View Type, Processed Events, and Valid Events.
Click the parsed events icon (the parsed events icon) to review sample parsed events for errors in the event fields identified by the parser. Example validation errors include events that are missing required information and fields.
The events processed per second (EPS) does not show data during test mode.

Make changes to data sources as needed
Based on the results of the data source validation, make changes as needed.

Change the data source query that you are using to get events from the Splunk platform or Splunk ES to make sure that the data source query contains required information.
Make sure that all necessary fields are populated and properly mapped.
Click Start in Test Mode at any time to re-test event parsing after you make changes.

Start the data source in production mode
Once Splunk UBA parses the events from the data source correctly, click Start in the data source details page to start processing and parsing the events for anomalies.

Filter events analyzed by Splunk UBA for anomalies
Limit or target the data analyzed by Splunk UBA with filters. You can create two types of filters:

Event filters to restrict event analysis based on field values. Filtering occurs after the data is added to Splunk UBA, but before Splunk UBA analyzes the data.
HR data global filter to restrict event analysis to only events that contain users present in the HR data. Filtering occurs before you add data to Splunk UBA and is enabled by default.
For example, you can use filters to exclude all events containing the user name of a penetration testing account or the IP address of a machine used in a malware lab. Event filters apply to all data added to Splunk UBA. If a filter is no longer needed, delete it.

Event filters can only include or exclude the specific fields for a category. For example, you cannot create a filter that excludes events with the user "buttercup" but includes events with the user "daisymug".

Do not create filters that overlap. For example, a filter that includes all events from the AD group "Ponies" but excludes events of the user "buttercup", who is a member of the "Ponies" AD group, will result in unpredictable event filtering, depending on the order of execution of the filters.

Create an event filter
Create event filters after configuring data sources.

Select Manage > Data Sources.
Click Event Filters.
Select a filter type.
Select the users, devices, AD groups, or event fields to filter.
Select whether to include or exclude the selected users, devices, AD groups, or event fields.
Click OK to save your filters.
Stop and restart the containers, and also sync the cluster in multi-node deployments.
/opt/caspida/bin/Caspida stop-containers
/opt/caspida/bin/Caspida sync-cluster
/opt/caspida/bin/Caspida start-containers
Example: create an event filter involving users
Create a filter to exclude events associated with the user Simon Roma.

Select Manage > Data Sources.
Click Event Filters.
Select a filter type of Users.
Click Add Users to add a new user filter.
Search for the user name Simon Roma.
Check the checkbox next to the user name and click OK.
Select the Exclude the events for the specified users radio button.
This screen image shows the Event Filter dialog window. The Users option is selected in the left-side selector. The user Simon Roma appears in the window because it was already selected, and "Exclude the events for the specified users" is selected.
Click OK to save the filter.
Stop the containers, synchronize the cluster if needed, then start the containers.
Example: create an event filter involving devices
Create a filter to exclude internal devices in the RFC1918 IP address range.

Select Manage > Data Sources.
Click Event Filters.
Select a filter type of Internal Devices.
Click Add Internal Devices to add a new user filter.
Enter the CIDR or IP address of the devices you want to filter. In this example, we will filter the RFC1918 IP address ranges:
10.0.0.0/8
172.16.0.0/12
192.168.0.0/16
Select the Exclude the events for the specified internal devices radio button.
This screen image shows the Event Filter dialog window. The Internal Devices option is selected in the left-side selector. The RFC1918 IP address ranges listed immediately above this image are entered in the fields, and "Exclude the events for the specified users" is selected.
Click OK to save the filter.
Stop the containers, synchronize the cluster if needed, then start the containers.
Additional steps required for filters using AD groups
Perform the following additional steps if your event filter uses AD groups:

Stop the Splunk UBA containers.
/opt/caspida/bin/Caspida stop-containers
Check your system to see if the /etc/caspida/local/conf/attribution/Account.json exists. If not, copy /opt/caspida/conf/attribution/Account.json to /etc/caspida/local/conf/attribution/Account.json.
Edit /etc/caspida/local/conf/attribution/Account.json and change "useInModel": false to "useInModel": true in following JSON block:
{
   "name": "userGroups",
   "sourceNames": ["memberOf","groups"],
   "properties" : {
      "type": "ARRAY",
      "label": "AD Group",
      "pluralLabel": "AD Groups",
      "showInDetails": true,
      "showInFilters": true,
      "showInGroupBy": true,
      "useInModel": false
   }
},
Save your changes to the Account.json file.
Run the following commands to sync the cluster and restart the containers.
/opt/caspida/bin/Caspida sync-cluster
/opt/caspida/bin/Caspida start-containers
Filter events based on event fields
To filter events based on the fields in the events, you must know the name and value of each field that you want to filter. Any field that appears in the parsed list of fields of an event can be included or excluded using a filter. Use the following table to identify common fields and values that you might want to filter in your data. Work with Splunk Professional Services to add customized field-based filters.

Field name	Description	Example values
event.format	name of the event format	WebGateway
view.network.source	source IP address	10.1.2.3
view.network.destination	destination IP address	172.20.30.40
ad.user.srcuser	source user for an AD data source	jsmith
ciscosa.user.destuser	destination user in a Cisco ASA data source	administrators\superuser
Set the order in which event filters run
Event filters run in the same order every time for every event in a data source until a matching event is found. When a matching event is found, additional filters are not applied to the corresponding event.

By default, event filters are applied in the following order:

Field values to exclude
Field values to include
External devices to exclude
External devices to include
AD groups to exclude
AD groups to include
Internal devices to exclude
Internal devices to include
IP address filters
Filters defined with regex
Users to exclude
Users to include
The order in which event filters run can be set manually by copying properties from the uba-default.properties file and customizing them in the uba-site.properties file.

Open the /opt/caspida/conf/uba-default.properties and copy the decorator.filter.precedence property along with all the filters.
decorator.filter.precedence=FieldValuesExclude, FieldValuesInclude, ExternalDeviceExclude, ExternalDeviceInclude, GroupExclude, GroupInclude, InternalDeviceExclude, InternalDeviceInclude, IPAddress, Regex, UserExclude, UserInclude
Open the /etc/caspida/local/conf/uba-site.properties file and past the decorator.filter.precedence property and filters into the file.
Set the desired order of the filters.
Save your changes to the file.



## Add HR data to Splunk UBA

Why Splunk UBA requires HR data
Add human resources (HR) data, such as employee details and their account information, from Active Directory or other HR systems to Splunk UBA. HR data must be loaded before any other data is loaded into Splunk UBA.

Splunk UBA uses HR data to do the following with other data loaded into Splunk UBA:

Categorize accounts by type. Splunk UBA defines the normal, admin, service, and system account types by default. These account types are used by the various threat models in Splunk UBA to generate threats and anomalies. You can define additional account types as needed.
Identify human users (employees) for the accounts found in the attributes of log records, such as loginId or email.
Associate multiple accounts of a user to a single human user using a unique ID, such as an employee ID. For example, user John Smith may have a normal account called jsmith, and an admin account called adm_jsmith. Splunk UBA normalizes these separate accounts to a single unique ID jsmith which belongs to John Smith. All events generated by these two accounts are analyzed under a single user for anomaly detection.
Understand HR data resolution
When performing HR data resolution, Splunk UBA takes following steps to identify the human user for any account found in log events:

The account name is extracted from an event. The account name can be in a variety of formats, including Jsmith, acme\jsmith, acme\\jsmith, acme/jsmith, jsmith\acme, jsmith@acme, acme\jsmith@acmesecured, or jsmith@acme.com.
In this example, we will use jsmith@acme.com as the account name found in the log event.
The full account name jsmith@acme.com is searched against the HR data. If there is no match, the account name is normalized by removing the domain information and performing another search against the HR data. In this example, the @acme.com portion of the account name is removed and jsmith is searched against the HR data.
If the search for either jsmith@acme.com or jsmith returns a match, the account name is replaced with the name of the human user, such as John Smith.
If a human user is not found, the account name becomes an unknown user in Splunk UBA. By default, Splunk UBA drops all events associated with unknown users.
If your HR data contains the domain+loginId field, Splunk UBA can use an account name such as acme\jsmith to locate a human user when performing HR resolution.
The following example shows how HR data and HR data resolution are used by Splunk UBA to identify a data exfiltration use case. Suppose John Smith has been an employee at his present company for 10 years. He has multiple admin accounts created over those years. Using his multiple admin accounts, he performs the following tasks:

Uses his jsmith_admin account to access a development server and download data to his laptop.
Uses his adm_jsmith1 account to access a sales server and download data to his laptop.
Uses his jsmith_a33 account to access a cloud server and download data to his laptop.
Downloads all of the data he has obtained from his laptop onto a portable USB drive.
Without proper HR data configuration, this data exfiltration scenario is missed as individually each of John Smith's actions do not raise any anomalies in Splunk UBA. However, with proper HR data configuration linking John Smith to each of his admin accounts, Splunk UBA can stitch together this series of behaviors and raise the proper anomalies and threats.

This image shows how a single user named John Smith can use multiple accounts to perform data exfiltration undetected without proper HR data configuration. The scenario is described in the text immediately preceding this image.

HR data resolution using a configured domains list
Account name normalization can also be configured by defining the company's internal domains. See Define the AD domains in use for devices.

When you define any internal domains, Splunk UBA only normalizes account names with domains matching the configured domains list. For example, if a domains list contains the following domains:

domain1
domain2
The account jsmith@acme.com will not be normalized since acme.com is not one of the configured internal domains. The jsmith@acme.com account becomes an unknown user and all events for that account are dropped.

If an account such as domain1\jsmith is found in a log event, it is normalized to jsmith when Splunk UBA performs HR data resolution.

Resolving active or inactive users
Some organizations allow an email address to be reused when a new employee joins the company and that employee has the same name as a previous employee. For example, John Smith may be assigned the email address jsmith@acme.com, which is the same email as a John Smith who worked at the company previously and who has already been terminated.

IT departments usually create unique account names or login IDs for each employee. In this case, the terminated John Smith may have the account name jsmith and the current John Smith may have the account name jsmith1. When Splunk UBA finds jsmith@acme.com in a log event, it is not possible to immediately identify whether this email address is for the terminated John Smith or the active John Smith. However, Splunk UBA can use the account status and termination date in the HR data to identify that this email address belongs to the active John Smith with the account name jsmith1.

Get HR data into Splunk UBA
Add HR data to Splunk UBA by performing the following tasks:

Verify that the HR data filter is enabled
Set the HR data cache capacity
Prepare the HR data in Active Directory
Gather HR data from Active Directory
Use the HR data to classify account types and user accounts
Add HR data from Splunk Enterprise
As an option, you can add HR data from a CSV file to Splunk UBA for testing and validation. See Add HR data from a CSV file.

Verify that the HR data filter is enabled
Before adding HR data to Splunk UBA, verify that the HR data filter is enabled.

Do not disable this filter unless directed to do so by Splunk Customer Support.

Set the HR data cache capacity
For each HR data account, Splunk UBA uses loginId, domainLoginId, and email to perform identity resolution and stores an entry for each unique value in the HR cache. Set the value of identity.resolution.hrcache.capacity to three times the number of HR accounts being monitored by Splunk UBA to avoid potential performance issues. For example, if your Splunk UBA deployment is monitoring 250,000 accounts, set this property to 750,000:

Log in to the Splunk UBA management node as the caspida user.
Add or edit the identity.resolution.hrcache.capacity property to the /etc/caspida/local/conf/uba-site.properties file:
identity.resolution.hrcache.capacity = 750000
Save and exit the file.
In distributed deployments, synchronize the cluster:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Run the following command to verify that the number of lookup keys is less than the cache capacity:
/opt/caspida/bin/irscan -H -m
Below is a sample output for this command:

Number of lookup keys     :       0
Cache capacity            :  300000
Account Lookup attributes : [domainLoginId, loginId, email]        (partial lookup)  : []
User Lookup attributes    : []     (partial lookup)     : []
The default value of identity.resolution.hrcache.capacity is 300,000.

Prepare the HR data in Active Directory
Ensure that existing Active Directory (AD) data is as complete and consistent as possible.

Splunk UBA dashboards provide a rich reporting and filtering capabilities based on user and account attributes, such as a user's department, location, or status. Properly configured HR data is critical in making sure this information is available for analysts and hunters.

Fill out as many fixed fields as possible, such as a user's first and last name, department, and location. This information is used for reporting and incident review. A user's manager is also useful information in situations where review and escalation are required.

Variable fields such as employeeID or sAMAccountName are used to link multiple accounts together to a single unique ID. Provide as much of this information as possible. See Why Splunk UBA requires HR data.

Comprehensive Active Directory data helps Splunk UBA provide the most accurate and meaningful results when exploring events in your environment.

Gather HR data from Active Directory
Pull the HR data from AD into Splunk Enterprise and save the data to a lookup file.

Use SPL to obtain the HR data
On the Splunk search head, run the following search to obtain HR data from Active Directory, format the results into a table, and save it to a lookup file:

| ldapsearch domain=default search="(&(objectCategory=person)(objectClass=user)(sAMAccountName=*))"
  attrs="accountExpires,c,company,dc,department,displayName,distinguishedName,employeeID,
  employeeNumber,givenName,initials,l,mail,manager,memberOf,mobile,postalCode,sAMAccountName,sn,st,
  status,streetAddress,telephoneNumber,title,userAccountControl,whenChanged,whenCreated"
| table accountExpires,c,company,dc,department,displayName,distinguishedName,employeeID,
  employeeNumber,givenName,initials,l,mail,manager,memberOf,mobile,postalCode,sAMAccountName,sn,st,
  status,streetAddress,telephoneNumber,title,userAccountControl,whenChanged,whenCreated
| outputlookup mycompany_domain.csv

Replace domain=default with the short domain name (not the FQDN) from the SA-ldapsearch configuration on the search head.

The outputlookup command creates the lookup file.

The table command formats the results so that they can be recognized by Splunk UBA. The fields listed in this command are used by the data models in Splunk UBA to generate anomalies and threats. If you get your HR data from a source other than Active Directory or from a CSV file, you must format the results to match this table command.

Splunk UBA already maps certain fields by default, as shown in the following table. You can also view /opt/caspida/conf/attribution/User.json and /opt/caspida/conf/attribution/Account.json to view all available field names.

See Add custom attributes to your HR data for information about adding your own HR data fields.

UBA field name	AD/LDAP field name	Description	Required	Example
AD Groups	memberOf, groups	List of AD groups that the user is a member of. If there are no groups, leave the value blank.	No	pony_instructors, pony_riders
City	l, city	City (location) of the user.	No	San Francisco
Country	co, country	Country code of the user.	No	USA
Departing User	departingUser	Whether or not the user has decided to leave the company.	No	true, false
Display Name	displayName	The user's full name or a service account name. If this field is empty, the display name is created by using the values in the first name, middle name, and last name fields.	No	Shruti Michelle Buttercup
Domain and LoginId	domainLoginId	The user's domain + login ID. Supported formats:
adDomain\loginId
adDomain\\loginId
adDomain/loginId
loginId\adDomain
loginId@dnsDomain
adDomain\loginId@dnsDomain
No	domain1/smbuttercup
Email Address	mail, userPrincipalName, email	User's email address. In some cases, you may find this stored in the userPrincipalName field.	Yes	smbuttercup@example.com
Employee Type	employeeType, userType	The type of employee.	No	Contractor
Expiration Time	accountExpires	Valid formats:
Windows FileTime
yyyy-MM-dd'T'HH:mm:ss
 %Y-%m-%dT%H:%M:%S.%QZ
MM/dd/yyyy
yyyyMMddHHmmss.S'Z'
yyyyMMdd
No	07/09/2014
First Name	preferredName, givenName, firstname	The user's first name. This value is used to compute the display name field if the display name field is empty.	Yes	Shruti
High Risk User	highRiskUser	Whether or not the user is identified as a high risk user, such as an executive.	No	true, false
Hire Date	hireDate	Date the user was hired. Valid formats:
MM/dd/yyyy
yyyyMMddHHmmss.S'Z'
yyyMMdd
No	07/09/2014
Last Logon	lastLogon	Last time the user logged on. Valid formats:
Windows FileTime
yyyy-MM-dd'T'HH:mm:ss
 %Y-%m-%dT%H:%M:%S.%QZ
MM/dd/yyyy
yyyyMMddHHmmss.S'Z'
yyyMMdd
No	07/09/2014
Last Logon Timestamp	lastLogonTimestamp	Valid formats:
Windows FileTime
yyyy-MM-dd'T'HH:mm:ss
 %Y-%m-%dT%H:%M:%S.%QZ
MM/dd/yyyy
yyyyMMddHHmmss.S'Z'
yyyMMdd
No	07/09/2014
Last Name	sn, lastname	The user's last name. This value is used to compute the display name field if the display name field is empty.	Yes	Buttercup
Login Id	sAMAccountName, loginId	Login ID or username of an account associated with the user.	Yes	smbuttercup
Manager	manager, manageremployeeId	Name or ID of the user's manager.	No	Charlotte Arachnia
Middle Name	initials, MiddleName	The user's middle name. This value is used to compute the display name field if the display name field is empty.	Yes	Michelle
On Performance Improvement Plan	onPerformanceImprovementPlan, onPIP	Whether or not the user is on a performance improvement plan.	No	true, false
OU	department, ou	Organizational unit (department) or business unit of the user.	Yes	Pony Instructors
Phone	telephoneNumber, phone	Phone number of the user.	No	123-456-7890
State	st, state	State where the user resides.	No	CA
Termination Date	terminationDate	The user's last day of employment. Valid formats:
MM/dd/yyyy
yyyyMMddHHmmss.S'Z'
yyyMMdd
No	07/10/2014
Title	title	The user's title.	No	Senior pony instructor
Traveling	traveling, travelling	Whether or not the user is traveling.	No	true, false
User Account Control Code	userAccountControl, UAC	User account control code from AD. Use UAC when the value in your HR data is an ENUM value such as NORMAL_ACCOUNT. If a UAC value is not available, Splunk UBA calculates the UAC using the value of the userAccountControl, such as 512 for a NORMAL_ACCOUNT.	No	66050, ACCOUNT_DISABLED
Status	status	Active or inactive status of the user from the HR system.	No	Active/InActive
ZIP Code	postalCode, zip	ZIP code of the user.	No	94107
Scheduling LDAP searches to obtain HR data from Splunk Enterprise
The LDAP search to obtain HR data can affect the performance on the search head. After the lookup file is created, use it to create an SPL that will identify and classify all accounts in your HR data, and associate a single user with each of them. Do not attempt to do this using live searches on Splunk Enterprise.

In environments with multiple AD domains, make a separate search and lookup file for each domain. Use multiple LDAP searches scheduled at different times to help reduce the load on the search heads.

Splunk UBA refreshes HR data each morning at 2:00 AM.

Schedule your daily LDAP searches to complete before 2:00 AM, but also at a time when the search will not compete for critical system resources.
Schedule your LDAP search to time out after 7 days. Some LDAP searches can take several hours to complete, so setting a large timeout window ensures that the search is able to complete.
Use the HR data to classify account types and user accounts
Perform the following tasks to write and modify a search to identify and classify all account types and user accounts. Each section may contain example commands, or commands you must cut and paste without modifications.

Import the CSV file containing the HR data
Exclude system accounts from your HR data
Perform transformations required by Splunk UBA
Find and classify all account types
Determine user status to properly identify terminated accounts
Identify domain and login ID
Create a proper display name for each user
Link related accounts to a single user
See Example SPL to get HR data into Splunk UBA for a complete search example.

Import the CSV containing the HR data
Begin by importing the CSV file containing the HR data. For example, use the following command to import a CSV file called mycompany_domain.csv:

| inputlookup mycompany_domain.csv

If you have multiple domains, you can append additional domains using append. For example, the following command imports a CSV file called mycompany_domain_1.csv and appends the CSV file mycompany_domain_2.csv:

| inputlookup mycompany_domain_1.csv
| inputlookup append=t mycompany_domain_2.csv

Exclude system accounts from your HR data
Remove system accounts to reduce your licensing requirements and load on Splunk UBA. Cut and paste either of the following commands without making any changes:

| search sAMAccountName!="*$" sAMAccountName!="$*"

| search userAccountControl!=*WORKSTATION_TRUST_ACCOUNT*

Perform transformations required by Splunk UBA
The following replace, rename, and rex commands are required for transforming some data to a format recognized by Splunk UBA. Cut and paste all of the following commands and do not change or remove any commands.

| replace "NULL" with ""
| rename c as co
| rename userAccountControl as UAC
| rex field=manager "CN=(?<manager>.*?),OU="
| rex mode=sed field=manager "s/\\\//g"
| rex max_match=0 field=memberOf "CN=(?<memberOf>.*?),OU="
| eval memberOf=mvjoin(memberOf,",")

Find and classify all account types
Use the match command as needed to classify all of the account types in hrAccountType. By default, Normal, Service, and Admin accounts are defined in Splunk UBA, but you may have additional account types in your environment.

The following example identifies all accounts beginning with 99 followed by a combination of zero or lower-case letters as Admin accounts:

| eval hrAccountType=case(     
 (match(lower(sAMAccountName),"^99[0a-z].*")),"Admin",

Examine the remaining accounts to see if there are additional Admin accounts not categorized with this command. Write additional match statements as needed until add Admin accounts are accounted for. Then, write commands to find all the service accounts. The following example contains the previous search for Admin accounts, along with additional commands to identify Service accounts:

| eval hrAccountType=case(     
 (match(lower(sAMAccountName),"^99[0a-z].*")),"Admin",
 (match(lower(distinguishedName),".*ou=.*service accounts.*")), "Service",
 (match(lower(distinguishedName),".*cn=managed service accounts.*")), "Service",

You can have any many match statements as needed to classify all accounts in your system. For example, your normal email convention may be <first initial><last name>@mycompany.com but due to a conflict, you have some users who are <first name><last initial>@mycompany.com. This situation requires separate match statements to properly identify all accounts.

You may even need to have a single match statement to identify one single account.

Continue with this process until only human accounts remain classified as Normal accounts.

Determine user status to properly identify terminated accounts
Splunk UBA uses the status property to determine whether a human user is active or inactive. Any users who no longer has an employment relationship with the organization are considered terminated and have a status of inactive. Splunk UBA detections can raise anomalies for activities involving inactive users, such as scripts or automated tasks that are run with the inactive user's credentials.

Any user employed or contracted by the organization is considered active, regardless of how often they are in the office, their physical location, or whether they are traveling or on any type of extended leave.

Do not confuse the status of a human user with userAccountControl or UAC, which reflect the status of the user's accounts. The UAC for Admin, Service or System account types should not be used to set a user's status because these account types are not managed by HR systems. The IT departments of many organizations set the userAccountControl of a user's account to ACCOUNTDISABLE in cases where the employee is not permanently terminated and still has an employment contract with the organization. The following examples show how userAccountControl can be set to ACCOUNTDISABLE for a user who is not permanently terminated:

A user is on legal-hold pending an internal forensics investigation.
A user forgot their password upon returning from a long weekend or holiday.
A user is on PTO and the organization has set the user's accounts to ACCOUNTDISABLE.
In some cases, the UAC for Normal accounts can be an indicator that the user is terminated, but it's not the only condition. For example, the UAC for a user's Normal account is set to ACCOUNTDISABLE, but the user may or may not have other accounts that are not set to ACCOUNTDISABLE.

When onboarding HR data, you must accurately write your SPL to correctly identify terminated users and set their status=InActive. You must identify the way your organization is marking permanently terminated users, and write an SPL transformation so that only terminated users have a status of InActive. Incorrectly mapping active, disabled, or suspended accounts with status=InActive will result in a large number of false anomalies related to terminated user activity, when the user is in fact not terminated.

Identify domain and login ID
Use a search such as the one in the following example to properly identify an account's domain and login ID:

| eval domainLoginId=dc + "\\\\" + sAMAccountName

This statement would generate an example such as Corp\\userAccount. Splunk UBA resolves this to proper AD multiline format, such as Corp\userAccount.

Create a proper display name for each user
Create a proper display name for each user, consisting of the first, middle, and last name. Below is an example command:

| eval displayName=givenName." ".middleName." ".sn

Valid display names must use spaces to separate the first, middle, and last name, otherwise some models in Splunk UBA will not work properly.

Link related accounts to a single user
Use sAMAccountName to link related Admin accounts and Normal user accounts to a single employeeID. Below is an example command:

| eval employeeID=sAMAccountName
| rex field=employeeID "^99(?<employeeID>.*?)$"

See Why Splunk UBA requires HR data for more information about why all accounts must be associated with a single employeeID representing a human user.

Example SPL to get HR data into Splunk UBA
This example SPL inputs HR data from two domains and contains match statements to cover a variety of account types such as Service, SharedMailbox, Vendor, TestAccount, FaxAccount, and ExchangeSystemAccount. Accounts that are not identified by any of these statements are classified as Normal accounts.

The table command formats the results so that they can be recognized by Splunk UBA.

| inputlookup mycompany_domain_1.csv 			          
| inputlookup append=t mycompany_domain_2.csv 	
| search sAMAccountName!="*$" sAMAccountName!="$*" 
| replace "NULL" with ""
| rename c as co
| rename userAccountControl as UAC
| rex field=manager "CN=(?<manager>.*?),OU="
| rex mode=sed field=manager "s/\\\//g"
| eval hrAccountType=case(     
 (match(lower(sAMAccountName),"^99[0a-z].*")),"Admin",
 (match(lower(distinguishedName),".*ou=.*service accounts.*")), "Service",
 (match(lower(distinguishedName),".*cn=managed service accounts.*")), "Service", 
 (match(lower(distinguishedName), ".*ou=shared mailboxes.*")), "SharedMailbox",     
 (match(lower(distinguishedName), ".*ou=vendor.*")), "Vendor",
 (match(lower(distinguishedName), ".*ou=testbank.*")), "TestAccount",     
 (match(lower(sAMAccountName),".*fax.*")), "FaxAccount",      
 (match(lower(sAMAccountName), "^healthmailbox.*")), "ExchangeSystemAccount",
 (match(lower(sAMAccountName), "^sm_.*")), "ExchangeSystemAccount",     
 (match(lower(sAMAccountName), "^systemmailbox.*")), "ExchangeSystemAccount", (1==1), "Normal"  )
| eval domainLoginId=dc + "\\\\" + sAMAccountName
| eval displayName=givenName." ".middleName." ".sn
| eval employeeID=sAMAccountName
| rex field=employeeID "^99(?<employeeID>.*?)$"
| table status, employeeID, domainLoginId, hrAccountType, sAMAccountName, displayName, 
  distinguishedName, givenName, initials, sn, title, mail, company, department, streetAddress, 
  l, st, postalCode, co, telephoneNumber, mobile, manager, UAC, whenCreated, whenChanged, 
  accountExpires, memberOf

Add HR data from Splunk Enterprise
Use the finalized search from Use the HR data to classify account types and user accounts to pull live HR data from Splunk Enterprise into Splunk UBA.

From the Splunk UBA menu, select Manage > Data Sources.
Click New Data Source.
Select the Splunk HR Data data source and click Next.
Type a connection Name, such as SplunkHR. The data source name must be alphanumeric with no spaces.
Add the URL of your Splunk search head and management port.
For example, https://10.10.123.45:8089.
Type a user name and password for Splunk Enterprise. The user account must have the admin_all_objects capability.
Click Next.
Type a Query to query the HR data from Splunk Enterprise. Use the complete query you configured in Use the HR data to classify account types and user accounts. The query output can be in JSON or CSV format.
Select a Frequency for how often you want Splunk UBA to run the search and retrieve the HR data, such as Daily.
Select Hourly to run a search now, then again every hour from now. The first search will run when you click OK at the end of this procedure.
Select Daily to run the first search at the next available 2 AM, then again each morning at 2 AM. For example, if you create the search at 9 AM on Monday May 1, the first search will run at 2 AM on Tuesday May 2. Subsequent searches will run each morning at 2 AM. If you want to run a search before the scheduled time, click OK at the end of this procedure, then click on the data source and click Start at the top of the data source page.
Select Weekly to run the first search at 2 AM one week from you create the search, then again each week at 2 AM. For example, if you create the search at 9AM on Monday May 1, the first search will run at 2 AM on Monday May 8. Subsequent searches will run each Monday at 2 AM. If you want to run a search before the scheduled time, click OK at the end of this procedure, then click on the data source and click Start at the top of the data source page.
Select One Time to run the search this one time only. The search will run when you click OK at the end of this procedure.
Click OK to add the data source.
Add HR data from a CSV file
As an option, you can export your HR data to a CSV file, add then add the HR data in the CSV file to Splunk UBA. This is useful for testing and verifying HR data ingestion using a smaller set of data. See Validate HR data configuration before adding other data sources.

Add HR data to Splunk UBA from a CSV file by performing the following tasks:

Export your HR data into a CSV file with headers that correspond to the fields in the table in Use SPL to obtain the HR data.
From the Splunk UBA menu, select Manage > Data Sources.
Click New Data Source.
Select HR File as the type of data source and click Next.
Choose the .csv file to upload.
Click OK to add the data source.

Add custom attributes to your HR data
Enrich your HR data by adding custom attributes. You can use custom attributes to filter users in Splunk UBA and create custom anomaly action rules using your custom attributes. You can create or modify your custom attributes as needed, and your changes will be reflected in the system the next time your HR data is updated.

See Get HR data into Splunk UBA to see the HR data fields already supported by Splunk UBA.

Procedure and requirements for creating custom attributes for HR data
To create custom attributes for HR data, perform the following tasks:

From the command line, SSH to the management server as caspida.
Perform one or both of the following tasks:
Copy the /opt/caspida/conf/attribution/User.json file to the /etc/caspida/local/conf/attribution directory. Modify this file to add user-related attributes.
Copy the /opt/caspida/conf/attribution/Account.json file to the /etc/caspida/local/conf/attribution directory. Modify this file to add account-related attributes.
Add your desired custom attributes to either User.json or Account.json as desired. See Custom attribute schema for information about the schema.
Use a JSON validator to make sure there are no errors in your JSON syntax. You can use any online JSON validator, or use the following command:
python -m json.tool <filename>.json
This command just be run from the directory where the JSOn file is located. If there are no errors, the full contents of the JSON file are displayed. If there are errors, a message appears with the line number in the file where the error is located.
Stop your HR data sources.
Run the /opt/caspida/bin/utils/customize_attribution.sh -u command to update the HR data tables in the system. All existing HR data is removed.
Synchronize the cluster and restart the Splunk UBA web interface. Do this before re-importing your HR data so that the new attributes are visible in Splunk UBA.
Re-import your HR data. You must update the SPL you use to obtain HR data from Splunk Enterprise so that the new HR data attributes are included. After your HR data is available again, you can view your custom attributes in Splunk UBA.
Custom attribute schema
Use this schema to define a new HR data attribute in /etc/caspida/local/conf/attribution/User.json or /etc/caspida/local/conf/attribution/Account.json:

{
  "name": <newAttribute>,
  "sourceNames": [<sourceNameOfAttribute>],
  "properties": {
    "type": <dataTypeOfAttribute>,
    "label": <LabelToBeDisplayedOnUI>,
    "showInDetails": true,  // Set to true, if we want to see it in User page
    "showInGroupBy": true,  // Set to true, if we want to include this attribute to UI group-by 
    "showInFilters": true   // Set to true, if we want to include this attribute in UI filters
  }
}
The fields in the schema are described in this table:

Field	Description
name	The name of the attribute in Splunk UBA.
sourceNames	The name of the fields in your HR data containing the desired value for the Splunk UBA name. Use a comma to separate multiple field names.
type	The data type, such as STRING or BOOLEAN.
label	The name of the attribute as you want it to appear in the Splunk UBA, such as on the HR Data details page.
showInDetails	Set to true if you want this attribute to appear on the User Details page.
showInGroupBy	Set to true if you want this attribute to appear as a grouping attribute when creating a new widget. For example:
In Splunk UBA, select Analytics > Custom Dashboards.
Click New Widget or select Actions > New Widget.
Enter a widget name and select Users Count.
Click Next. The custom attribute will appear on the Widget Grouping page.
showInFilters	Set to true if you want this attribute to appear in the filters on the user pages. For example:
In Splunk UBA, select Manage > Users to view the users table.
Click Add Filter to view all user attributes.
Example: Add a new HR data attribute
In this example, we will walk through how to do the following:

Create a new HR data attribute
Verify the attribute in Splunk UBA
Use the new attribute to create an anomaly action rule
Create a new attribute called Full Time
Suppose we want to create a new attribute called Full Time to designate full-time employees. This is not a field tracked by default in Splunk UBA. Since it is a user-related property, we will add it to Users.json.

Log in to the management server as the caspida user.
Navigate to the /etc/caspida/local/conf/attribution directory. If this directory does not already exist, create it. For example:
cd /etc/caspida/local/conf
mkdir attribution
cd attribution
Create a copy of the /opt/caspida/conf/attribution/User.json file in the /etc/caspida/local/conf/attribution directory.
cp /opt/caspida/conf/attribution/User.json .
Edit the User.json file and add the following schema to the file:
{
    "name": "fulltime",
    "sourceNames": ["fulltime"],
    "properties": {
        "type": "BOOLEAN",
        "label": "Full Time",
        "showInDetails": true,
        "showInGroupBy": true,
        "showInFilters": true
   }
}
Save and exit the file.
Use a JSON validator to make sure there are no errors in your JSON syntax.
python -m json.tool User.json
If you see the full content of the file, then no syntax errors are found.
In Splunk UBA, go to Manage > Data Sources and stop any HR data sources that are currently running.
Run the following command to update the user attributes in your HR data:
/opt/caspida/bin/utils/customize_attribution.sh -u
This command reformats your HR data database tables to add the new attribute and removes all HR data from your system.
In distributed deployments, synchronize the cluster.
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the Splunk UBA web interface:
sudo service caspida-ui restart
Re-import your HR data. You must update the SPL you use to obtain HR data from Splunk Enterprise so that the new HR data attributes are included. See Get HR data into Splunk UBA.
Verify the new Full Time attribute
Check in Splunk UBA to verify that the new Full Time attribute was added.

In Splunk UBA, select Manage > HR Data.
On the HR Users page, select a user.
Verify that the attribute appears at the top of the page with the other HR data attributes.

Create an anomaly action rule using the Full Time attribute
Follow the instructions in Create an anomaly action rule in Splunk UBA to create a new anomaly action rule using this new user property. You can create an anomaly filter to identity users with this new Full Time attribute and add them to a watchlist, change the anomaly score, or delete anomalies associated with these users.

Validate HR data configuration before adding other data sources
After adding the HR data, return to the HR data page to make sure that the account names and account types are populated and associated with the correct user.

From Splunk UBA, select Manage > HR data.
Review the HR Users and HR Accounts tables.
If the configuration is inaccurate:
Click Reset HR Data to remove the HR data.
Update the HR data configuration. See Get HR data into Splunk UBA.
Add the HR data again.
Repeat this process as needed until you verify that the HR data in Splunk UBA associates the account names and account types with the correct user.

You can also use the /opt/caspida/bin/irscan -H command in the CLI to verify the HR account data for a specific user.

Log in to the management node as the caspida user.
Run the /opt/caspida/bin/irscan -H command.
When prompted, enter the user name you want to verify.
The following example output shows an HR account lookup for the user abogle:

caspida@uba001:~$ /opt/caspida/bin/irscan -H
{}
Loading HR data in memory.
-------------- top output for this process: [  1927] ------------------------------
top - 14:22:20 up 25 days, 12:11,  2 users,  load average: 4.97, 3.95, 2.08
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  6.5 us,  2.1 sy,  0.0 ni, 91.1 id,  0.1 wa,  0.0 hi,  0.2 si,  0.0 st
KiB Mem : 65975524 total, 20264584 free, 12461616 used, 33249324 buff/cache
KiB Swap:  4575228 total,  4078512 free,   496716 used. 51991108 avail Mem 


  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 1927 caspida   20   0 17.917g 124804  33332 S  13.3  0.2   0:02.39 java
-----------------------------------------------------------------------------------


Enter id/account to resolve >> 
abogle
Lookup account: [abogle], resolution-status[Resolved]
       Matched: [abogle]
          User: id[ -746877122015991365], name[Aaron Bogle], type[Human], idType[IR]
       Account: id[-8738048929199146334], name[abogle], type[Normal], status:[null]
After you verify that your HR data is onboarded correctly, you are ready to add assets, identities, and threat intel to Splunk UBA. See Identify assets in your environment.

Make changes to your HR data
Splunk UBA updates HR data daily. Because HR data is used to assign users and IDs to events processed from all other data sources, you cannot make changes to HR data once you start adding data sources. Changing the HR data configuration after data sources are added causes duplicate user IDs to appear in Splunk UBA. If you need to modify your HR data configuration after you have ingested events from other data sources, you must do the following:

Remove all metadata from Splunk UBA.
Ingest and verify your HR data again.
Ingest events from your data sources again.



## Add Assets data and identify assets to exclude from detections

Identify assets in your environment
Asset data refers to information about the devices that are owned by your company. Splunk UBA ingests asset data from Splunk Enterprise daily using asset lookup queries. Splunk UBA uses this predefined device information in the following ways:

An in-memory cache is used to store some of the asset lookup results, which are used by Splunk UBA to perform device resolution. See Device resolution in Splunk UBA in Use Splunk User Behavior Analytics for more information about how Splunk UBA uses asset data to resolve device names.
Exclude devices such as domain controllers, exchange servers, file servers, print servers or proxy servers that are not associated with a specific user.
Display additional metadata for devices in the system.
You can update the asset data information in Splunk UBA using one of the following methods:

Perform asset identification by using the Splunk Assets data source to perform queries against the Splunk platform.
Perform asset identification by using a CSV file when you are unable to perform direct searches on the Splunk platform.
Prerequisites for performing asset identification
You must perform asset identification after HR data is loaded into Splunk UBA, but before any event data is loaded.

In addition, verify the following on Splunk Enterprise:

The ldapsearch command must be available and capable of accessing the LDAP server. The ldapsearch command is used by the asset domain controller query to identify and exclude the domain controllers in your environment. See Perform asset identification by using the Splunk Assets data source.
Splunk UBA cannot obtain domain controller information in Splunk Cloud environments.

If you have Splunk Enterprise Security (ES), the asset table must be reachable through Splunk Enterprise. Access to the asset table is required to access the asset database.
References to indexes and sources of Windows Security events in Splunk Enterprise must be available. Splunk UBA's asset proxy query makes use of Windows events 4624 and 4769 to identify and exclude proxy servers in your environment. See Perform asset identification by using the Splunk Assets data source.
Not all data at your site might be properly processed. In some cases, you may receive an error message in Splunk UBA, and in others, only in the log file.

Asset data fields
Assets in Splunk UBA can be searched using the fields below.

Field	Data Type	Description	Example
hostname	string	Required. The hostname of the device.	server1
blackListDeviceIr	boolean	Recommended. Indicates whether or not any IP addresses are associated with the MAC address for this device. Set to true to prevent any IP addresses from being associated with the MAC address for this device. See Exclude identity resolution for devices or users.	false
blackListUserIr	boolean	Recommended. Indicates whether or not any users are associated with this device. Set to true to prevent any users from being associated with this device. See Exclude identity resolution for devices or users.	false
app	string	The application name.	Database
asset_tag	string	The asset ID on the physical asset tag such as a sticker that is typically placed on each device in your organization.	123456
bunit	string	The business unit that the device belongs to.	EMEA, NorCal
city	string	The city where the device is located.	Chicago
cost_center	string	The cost center that the device belongs to.	SP01FIN
country	string	The country where the device is located.	USA
created_by	string	The name of the user who created the device in the system.	DevOps
department	string	The department that the device belongs to.	Field Reps, ITS, Products, HR
deviceType	string	The type of device.	client
dns_domain	string	The domain of the device.	www.acmetech.org
dns	string	The FQDN of the device.	server1.corp1.acmetech.org
ip	array	The IP address of the device. The field may contain multiple values. See Configure asset ingestion for multi-valued fields.	2.1.1.1
is_expected	boolean	Indicates whether or not this device is always expected. Alerts are generated if this device stops reporting events.	true
latitude	string	The latitude location of the device.	37.780080
longitude	string	The longitude location of the device.	-122.420170
mac	array	The MAC address of the device. The field may contain multiple values. See Configure asset ingestion for multi-valued fields.	00:50:ef:84:f1:21|00:50:ef:84:f1:20
managed_by	string	The manager of the device.	admin
os	string	The operating system running on the device.	macOS, WIndows
os_domain	string	The OS domain of the device.	Windows
owner	string	The owner of the device.	f.prefect@acmetech.org, DevOps, Bill
pci_domain	string	The PCI address domain of the device.	dmz, untrust
serial	string	The serial number of the device.	AB1C24D5EFGH
status	string	The hexadecimal Windows status code for the device.	0XC0000234 (user is currently locked out)
substatus	string	The hexadecimal sub-status code for the device.	0XC000006D (invalid username or authentication)
sys_created_on	timestamp	The date and time stamp of when the device was first entered into the system. The format is MM/DD/YYYY.	05/01/2019
sys_updated_on	timestamp	The data and time stamp of the last time the device was updated. For example, a laptop may be assigned to a new owner. The format is MM/DD/YYYY.	05/01/2019
Configure asset ingestion for multivalue fields
Some assets can have multiple values in a field, such as multiple IP addresses or MAC addresses. Splunk UBA creates separate devices for each IP address or MAC address if the addresses are separate by commas, as shown in the following example:

192.168.10.10,192.168.10.20,192.168.10.30
For data sources such as Splunk Enterprise Security (ES) that use a delimiter other than a comma, update the attribution.keyvalue.delimiter property in the /etc/caspida/local/conf/uba-site.properties file to specify the desired delimiter.

For example, perform the following tasks to specify that multiple IP and MAC addresses are separated using a pipe (|) character instead of a comma:

Log in to the management node of your Splunk UBA deployment as the caspida user.
Edit the /etc/caspida/local/conf/uba-site.properties file and add or edit the attribution.keyvalue.delimiter property so it looks like the following:
attribution.keyvalue.delimiter=Device.ip=\\|,Device.mac=\\|
Attribute element	Description
Device.ip	The ip attribute of Device attribution. This element is case-sensitive.
Device.mac	The Mac attribute of Device attribution. This element is case-sensitive.
\\|	The regex of the desired delimiter.
This example takes the IP addresses 192.168.10.10|192.168.10.20|192.168.10.30 and stores them as follows in Splunk UBA:

{192.168.10.10,192.168.10.20,192.168.10.30}
Remove or comment out the attribution.keyvalue.delimiter property to use a comma as the delimiter for multivalue fields.
Synchronize your Splunk UBA cluster after making any changes to your uba-site.properties file:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Perform asset identification by using the Splunk Assets data source
After you meet the requirements for performing asset identification, you can begin asset identification by using the Splunk Assets data source.

Perform the following tasks to configure a Splunk Assets data source in Splunk UBA.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Scroll down to the Device Attribution section, select Splunk Assets, and then click Next.
Enter the connection details to the Splunk platform, and then click Next. If you are connecting to Splunk ES, specify the Splunk ES search head as the URL of the data source.
A sample query sourcetype=WinEventLog:Security is populated in the Query field to get AD multiline events. If you have AD XML events in your environment, change the query accordingly, such as sourcetype=XmlWinEventLog. You must validate that this query is returning the desired AD events in your environment. This query is used by the asset proxy query to identify and exclude the proxies in your environment.
In the Domains field, specify a comma-separated list of domains in your environment. This list of domains is used by the assets domain controller query to identify and exclude the domain controllers in your environment.
In the Schedule field, specify the frequency with which asset queries are run. The frequency interval begins when the data source is configured. For example, if you finish configuring the data source at 3:30PM and you select Daily as the frequency, Splunk UBA refreshes the asset data each day at 3:30PM.
Click OK.
After the data source is configured, Splunk UBA performs asset data queries at the scheduled interval using the following queries:

Query	Description
Asset domain controller query	This query is located in /opt/caspida/conf/asset_dc_query.txt and performs an ldapsearch to identify and exclude the domain controllers in your environment. This query uses the domains specified in the Domains field when configuring a Splunk Assets data source.
Asset ES query	This query is located in /opt/caspida/conf/asset_es_pull_query.txt and uses the assets macro to obtain the assets data in Splunk ES.
Asset proxy query	There are two queries:
/opt/caspida/conf/asset_proxy_query_multiline.txt for AD multiline format.
/opt/caspida/conf/asset_proxy_query_xml.txt for AD XML format.
The proxy query performs an SPL search of your Windows Event Security logs to identify and exclude proxy servers. The search that is run depends on the setting of the assets.proxy.query.adformat property. By default, this property is set to MULTILINE in /opt/caspida/conf/uba-default.properties. Splunk UBA runs the query in asset_proxy_query_multiline.txt to find and exclude proxy servers.

If you have XML format Windows Event Security logs, perform the following tasks:

Log in to the management node in your Splunk UBA deployment as the caspida user.
Add the assets.proxy.query.adformat property to the /etc/caspida/local/conf/uba-site.properties file.
Set the property to XML:
assets.proxy.query.adformat = XML
Save and exit the file.
In distributed deployments, synchronize the cluster:
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart the job manager:
sudo service  caspida-jobmanager stop
sudo service  caspida-jobmanager start
Perform asset identification by using a CSV file
Perform asset identification by using a CSV file when you are not able to perform direct searches. Perform the LDAP query to create a lookup CSV file, then use the CSV file in a lookup query.

Use the following example as a guideline, and replace the commands and transformations as needed for your environment:

Follow the instructions in Perform asset identification by using the Splunk Assets data source to create a Splunk Assets data source.
Schedule the LDAP query as a job to run every night around 10:00 PM local time. See Scheduling searches in the Splunk Enterprise Search Manual.
Specify an LDAP query such as the one below and create the CSV file:
| ldapsearch domain=<domain-name> search="(&(objectCategory=computer)(sAMAccountName=*))"
  attrs="accountExpires,cn,countryCode,dNSHostName,department,description,distinguishedName,
  division,isCriticalSystemObject,lastLogoff,lastLogon,lastLogonTimestamp,localPolicyFlags,
  logonCount,name,objectCategory,objectGUID,objectSid,operatingSystem,operatingSystemVersion,
  primaryGroupID,pwdLastSet,sAMAccountName,sAMAccountType,userAccountControl,
  whenChanged,whenCreated" 
| outputlookup uba_ldapsearch_computers.csv
| stats count

Be sure to replace <domain-name> with an appropriate domain name for your environment.
Make local copies of the existing asset configuration files and put them in the /etc/caspida/local/conf folder:
cp -a /etc/caspida/conf/asset_* /etc/caspida/local/conf/.
Add a lookup query such as the one below to /etc/caspida/local/conf/asset_dc_query.txt:
| inputlookup uba_ldapsearch_computers.csv
| fields - _raw
| rex max_match=0 field=distinguishedName ".*?OU=(?<groups>[^,=]+),.*?"
| eval deviceType=mvjoin(groups, " - ")
| rename name as hostname,
  dNSHostName as dns,
  operatingSystem as os,
  countryCode as country,
  whenCreated as sys_created_on,
  whenChanged as sys_updated_on
| eval blackListUserIr=IF((lower(deviceType)="domain controllers" OR 
  like(lower(deviceType), "%prox%") OR 
  like(lower(deviceType), "%exch%") OR 
  like(lower(deviceType), "%dns%") OR 
  lower(deviceType)="azurecoread"),"true","false")
| table accountExpires,blackListUserIr,cn,country,department,description,deviceType,
  distinguishedName,division,dns,hostname,isCriticalSystemObject,lastLogoff,lastLogon,
  lastLogonTimestamp,localPolicyFlags,logonCount,objectCategory,objectGUID,objectSid,
  operatingSystemVersion,os,primaryGroupID,pwdLastSet,sAMAccountName,sAMAccountType,
  sys_updated_on,sys_created_on,userAccountControl

Update the other two asset configuration files /etc/caspida/local/conf/asset_es_pull_query.txt and /etc/caspida/local/conf/asset_proxy_query.txt with valid queries that return no results. For example:
| inputlookup uba_ldapsearch_computers.csv | search deviceType="abc"

View assets in your environment
Select Manage > Assets to view the assets identified in your environment.

Use Add Filter to limit the devices shown on this page.


Exclude identity resolution for devices or users
As Splunk UBA processes events, the Identity Resolution component (IDR) performs the following:

Builds associations among IP addresses, MAC addresses, and host names based on login and logout events in Active Directory, VPN, DHCP, or DNS logs. When an IP address is found in an event, the IDR system examines the time of the event and tries to associate a host name with the IP address based on various correlations.
Tracks which users have logged in to which devices. For example, if user John Smith logged in to a device at 1:00PM for 10 minutes, we can infer that events originating on that device during that time were initiated by John Smith. Splunk UBA can associate John Smith with these events in cases where the event log may not be able to identify the user, such as some firewall logs.
The IDR component in Splunk UBA maintains exclusion lists to improve the quality of these associations. For example, multi-user systems such as domain controllers, shared servers, and proxies must not be assigned to any specific user. These multi-user systems are included in an IDR exclusion list to prevent them from being associated with any specific users.

Another example is a server hosting multiple domain names that are different from the real host name of the server. The domains are assigned IP addresses using DHCP. In this case, a static IP address cannot be accurately assigned to each hosted domain name.

A scheduled job runs daily at 6:00 AM local time that analyzes the data sources listed in the following table to identify entities for IDR exclusion.

Comparing user whitelists and user IDR exclusion lists
User whitelists and user IDR exclusion lists both serve the purpose of preventing anomalies being generated against a user for legitimate purposes. Consider an Active Directory (AD) server in your environment:

A penetration tester might log in to the server regularly to perform penetration testing against the server. In this case, we include this user on a whitelist because we want to associate the user with the device, but we do not want any anomalies raised.
An IT admin might also login to the same server to perform routine admin tasks. In this case, we use a user IDR exclusion list to prevent this user from being associated with the device because the admin is not the owner of the device. However, the activity performed by the admin account (not the user) can be tracked by external alarm and endpoint models and may still raise anomalies.
IDR exclusion list usage examples
The following table provides examples of how IDR exclusion lists are used by Splunk UBA to improve the quality of device and user associations.

Entity Type	Exclusion List Type	Entity Example	Action Example
User	Device	jsmith	John Smith logs in to many devices on a regular basis and is not the owner of those devices. Do not associate John Smith with events originating from any devices.
Device	User	acme-dc-01	The device acme-dc-01 is a multi-user system. Do not assign a user to events originating from the acme-dc-01 device. The device you want to include in the exclusion list can be specified as a host name, IP address, or MAC address.
Device	DNS	10.23.150.30	The IP address 10.23.150.30 can have multiple host names associated with it. Do not assign host names to events originating from the 10.23.150.30 IP address. The device you want to include in the exclusion list can be specified as an IP address or MAC address.
The length of time that an entity remains on an exclusion list depends on whether the exclusion list was created by a user or by Splunk UBA:

Entities added to an exclusion list by a user remain on the list until removed by a user.
Entities added to an exclusion list by Splunk UBA remain on the list for 30 days from the timestamp of the entity being added or updated. For example, suppose a device was added to an exclusion list on May 1. On May 3, the same device is found again in some events and the timestamp on the exclusion list is updated to May 3. This device remains in the exclusion list for 30 days starting from May 3. After an entity has remained on an exclusion list for 30 days, it is removed from the exclusion list.
Create IDR exclusion lists in Splunk UBA
You can Create IDR exclusion lists for user and devices in Splunk UBA.

User permissions required for creating IDR exclusion lists
The following user permissions are required to Create IDR exclusion lists in Splunk UBA:

View permissions for HR Data
Edit permissions for IDR Exclusions.
See Manage user accounts and account roles in Splunk UBA for more information about roles and permissions in Splunk UBA.

PII masking and IDR exclusion lists
PII masking affects IDR exclusion lists in the following ways:

All entity names will be masked if either User Name or Device Name is selected for PII masking. For example, if User Name is selected for PII masking, then both user names and device names will be PII masked. See Mask personally-identifiable information in Splunk UBA for more information about masking PII in Splunk UBA.
When PII masking is enabled in Splunk UBA, IDR exclusion lists cannot be edited by any user even if neither User Name nor Device Name are selected as PII masking fields.
Create a user IDR exclusion list
Perform the following tasks to create a user IDR exclusion list. The specified user will not be assigned to any events originating from any devices.

In Splunk UBA, select Manage > IDR Exclusion List.
Make sure the User tab is selected on the left side of the screen.
Click New Entity.
Provide a valid login ID for the user. View your HR data for valid login IDs.
Verify that the User is the desired entity. This is automatically populated by Splunk UBA when a valid login ID is provided.
Use the Notes field to enter any specific notes about why this user is being included on this exclusion list.
Click OK.
User exclusion lists are applied per user, not per HR account (login ID). For example, user John Smith may have a normal account called user_jsmith, and an admin account called adm_jsmith. When adding John Smith to the user IDR exclusion list, if you specify adm_jsmith as the login ID then John Smith appears as the user. John Smith is not associated with any device events regardless if the events involved his normal account user_jsmith or his admin account adm_jsmith.

Create a device IDR exclusion list
Perform the following tasks to create a device IDR exclusion list.

In Splunk UBA, select Manage > IDR Exclusion List.
Select the Devices tab is selected on the left side of the screen.
Click New Entity.
Select an exclusion type. See IDR exclusion list usage examples for more information about the exclusion types.
Specify the device to be excluded.
If you selected User as the exclusion type, provide a valid IP address, MAC address, or host name. Users will not be associated with this device.
If you selected DNS as the exclusion type, provide a valid IP address or MAC address. Hostnames will not be associated with the specified IP address or MAC address.
Use the Notes field to enter any specific notes about why this device is being included on this exclusion list.
Click OK.
View IDR exclusion lists in Splunk UBA
Select Manage > IDR Exclusion List to view the exclusion lists defined on your system.

The Created By column can have the values listed below. A red star indicates that the entity was added to the exclusion list by Splunk UBA.

Value	Source	Description
User	Manually aded by a user	The entity was manually added to the exclusion list in Splunk UBA. View the audit logs to determine the specific user who added the entity. See Audit user activity in Splunk UBA.
The IDR icon	Identity resolution data	Data from the previous 7 days is analyzed to identify multi-user systems and admin users frequently logging on multiple machines. Also, data from last 24 hours is analyzed to find occurrences of more than 2 device mappings per hour for more than 6 hours. You can configure these thresholds by configuring the following properties in /etc/caspida/local/conf/uba-site.properties:
Set identity.resolution.blacklist.threshold.device.hostnamecount to the desired numbers of device mappings per hour. The default is 2.
Set identity.resolution.blacklist.threshold.device.hostnamehours to the desired number of consecutive hours. The default is 6.
After setting the properties, synchronize the cluster and restart the data sources.

The UBA Model icon	Splunk UBA model output	The device profiler model in Splunk UBA identifies domain controllers and proxy servers through machine learning. Splunk UBA uses the output of the model to create device exclusion lists.
The Assets Data icon	Splunk UBA assets data	When asset data is imported, you can configure the blackListUserIR property. Set this property to true to add the device to the user IDR exclusion list with "device" as the entity type. See Asset data fields.

Use allow and deny lists to generate or suppress anomalies
Add a domain or IP address to a deny list to generate anomalies whenever a user or device interacts with that domain or IP address.

Interaction with a denied domain generates a Blacklisted Domain anomaly.
Interaction with a denied IP generates a Blacklisted IP Address anomaly.
You can view the IP or domain, confidence rating, and source for a denied anomaly on the Anomaly details page.

Add a user, domain, or IP address to an allow list to prevent anomalies based on interactions with that user, domain, or IP address from being generated. Using an allow list prevents an anomaly from being created if the anomaly involves a single user. If an anomaly involves an allowed user and another user, it will still be generated. Add a user to an allow list if they are a penetration tester and you would expect anomalous or suspicious activity while they conduct their tests.

The allow list takes priority over the deny list. If a domain is on both lists, denied domain anomalies based on that domain are not generated.

What is denied or allowed in Splunk UBA?
This table shows what Splunk UBA denies and allows by default.

Deny list or allow list	Description
Denied domains	Splunk UBA includes a list of high-confidence entries from the Collective Intelligence Framework (CIF). New entries are periodically added to the list in new versions of Splunk UBA.
Denied IP addresses	Splunk UBA does not include a default set of denied IP addresses.
Allowed domains	Splunk UBA selects allowed domains from the top 250,000 global website domain names according to Alexa.
Allowed IP addresses	Splunk UBA selects allowed IP addresses from the top 50,000 global website IP addresses and ranges according to Alexa.
Allowed users	Splunk UBA does not include a default set of allowed users. Add HR data to Splunk UBA before uploading a list of allowed users.
How anomaly and threat models use deny lists and allow lists in Splunk UBA
This table shows the anomaly and threat models that interact with deny lists and allow lists in Splunk UBA. Any models not listed in the table do not have any interaction with deny lists or allow lists.

Click on the Model name column header to sort the table by model name, or click on the Model type column header to sort the table by model type.

Model name	Model type	How this model uses allow lists	How this model uses deny lists
Suspicious Email Detection Model	Threat model	Checks if an email sender or recipient domain is on the allow list. Senders or recipient domains on the allow list are not automatically ignored and factor in to the threat's score and whether or not a threat is generated.	Checks if an email sender or recipient domain is on the deny list. Senders or recipient domains on the deny list are used as factors in determining whether or not a threat is generated.
Device Anomaly Ranking Task
User Anomaly Ranking Task	Produces risk rankings for users and devices based on their anomalies.	Checks if the anomaly contains entities on the allow list. Entities on the allow list can decrease an anomaly's score, or prevent an anomaly from being generated.	Checks if the anomaly contains entities on the deny list. Entities on the deny list can increase an anomaly's score.
Suspicious Data Movement Threat Model	Threat model	Checks if the destination of outgoing traffic is on the allow list. Destinations on the allow list can decrease a threat's score, or prevent a threat from being generated.	Checks if the destination of outgoing traffic is on the deny list. Destinations on the deny list can increase a threat's score.
Hypergraph-based Malware Threat Detection Model	Threat model	Checks if the activity involves domains on the allow list. Domains on the allow list can decrease an anomaly's score, or prevent an anomaly from being generated.	Checks if the activity involves domains on the deny list. Domains on the deny list can increase an anomaly's score.
External Alarm Analysis Model	Anomaly model	Checks if the activity involves domains on the allow list. Domains on the allow list can decrease an anomaly's score, or prevent an anomaly from being generated.	Checks if the activity involves domains on the deny list. Domains on the deny list can increase an anomaly's score.
Suspicious Data Transfer	Anomaly model	Not used.	Compares the country in the activity to a built-in list of countries on the deny list. Blacklisted countries can increase the score of an anomaly.
IP Malware Communication Model	Anomaly Model	Destination IP addresses in the anomaly activity that are on the allow list are ignored.	Checks if the destination IP address is on the deny list. Destination IP addresses on the deny list can increase the score of an anomaly.
Malware Communication Model	Anomaly Model	Destination domains in the anomaly activity that are on the allow list are ignored.	Checks if the destination domain is on the deny list. Destination domains on the deny list can increase the score of an anomaly.
Browser Exploitation Model	Anomaly model	Destination domains in the anomaly activity that are on the allow list are ignored.	Checks if the destination domain is on the deny list. Blacklisted domains on the deny list can increase the score of an anomaly.
Web Beaconing Detection Model	Anomaly model	Destination domains in the anomaly activity that are on the allow list are ignored.

By default, this model does not generate anomalies if all involved domains are on the allow list. If at least one involved domain is not on the allow list, an anomaly is raised.

Not used.
IP Beaconing Detection Model	Anomaly model	Destination IP addresses in the anomaly activity that are on the allow list are ignored.	Not used.
Rare User Agent String Model	Anomaly model	Destination domains in the anomaly activity that are on the allow list are ignored.	Not used.
View deny lists and allow lists in Splunk UBA
View and modify existing or add new filter lists by selecting Manage > Black/White Lists. Each list displays the total number of entries. Sort the list based on the domain, IP address, or username, source, confidence percentage, or the date reported. The date reported reflects the date that the entry was added to Splunk UBA.

Use the confidence percentage and the source to determine how much you can trust a list entry.

Add new entries to a deny list or allow list
You can add new entries to a list by uploading a static file. The file must be a .txt file, with each entry on its own line.

In Splunk UBA, select Manage > Black/White Lists.
Select the list you want to add values to.
Click Upload.
Choose a List File from your computer.
(Optional) Enter a comment describing the file you are uploading.
Click OK.
Users are added to the allow list only if their user account already exists in Splunk UBA. Add HR data to Splunk UBA first before uploading a list of allowed users.

Remove entries from a deny list or allow list
You can remove individual list entries, or delete any file-based list sources.

Remove individual list entries
Remove individual list entries if you determine they are no longer needed or relevant.

In Splunk UBA, select Manage > Black/White Lists.
Select the list from which you want to remove entries.
Check the checkbox next to the entries you want to remove.
Select Action > Delete.
Click OK to confirm that you want to delete the selected entries.
Delete or disable list entries by source
Deleting or disabling a source removes all the list entries associated with a specific source.

In Splunk UBA, select Manage > Black/White Lists.
Select the list from which you want to remove a source.
Click Source to see the list entries sorted by source.
Check the checkbox next to the source you want to remove.
Select Action > Delete.
Click OK to confirm that you want to delete the selected source.
If you delete an individual entry from a list, then disable the list from source, then enable the list, previously-deleted individual entries return to the list.

Use anomaly action rules if anomalies for allowed entities are still being generated
In some cases, you may see an anomaly generated against an allowed entity such as an IP address. To suppress anomalies from being generated against this entity, perform the following tasks:

Create a new device watchlist.
Add devices to the new watchlist.
Use the watchlist in an anomaly action rule.
In the following example, we will suppress anomaly generation for the IP address 10.1.1.8.

Create a new device watchlist
First, perform the following tasks to create a new device watchlist:

In Splunk UBA, select Manage > Watchlists.
In the list of watchlist types, select Device Watchlists.
Click New Watchlist.
In the New Device Watchlist window, enter the watchlist name Whitelisted Devices.
Click OK.
See Investigate Splunk UBA entities using watchlists in Use Splunk User Behavior Analytics for more information about Splunk UBA watchlists.

Add devices to the new watchlist
Add the IP address 10.1.1.8 to the Whitelisted Devices watchlist:

In Splunk UBA, select Explore > Devices or click Devices on the home page.
Enter 10.1.1.8 in the search field to locate this device.
Click on the device to access the Device Details page.
In the Watchlist field, click the star icon and then select Whitelisted Devices to add this device to the selected watchlist.
Use the watchlist in an anomaly action rule
Use the watchlist in an anomaly action rule. In this example, any anomaly containing an IP address in the Whitelisted Devices watchlist is moved to the trash and not generated:

In Splunk UBA, select Explore > Anomalies or click Anomalies on the home page.
Click the gear icon and verify that Anomaly Action Rules is selected.
Click New Anomaly Action Rule.
Conflgure the rule action:
Click Delete Anomalies and Move to Trash as the rule action.
Click Apply to Future and Existing Anomalies as the rule scope.
Click Next.
Configure the anomaly filters:
Select Device Watchlists from the list of filters, then select the Whitelisted Devices watchlist.
Set the additional filter options to Include and Contains Any.
Click Next.
Enter Do not generate anomalies against whitelisted devices as the rule name.
(Optional) Enter a description for this rule.
Click OK.
See Take action on anomalies with anomaly action rules in Administer Splunk User Behavior Analytics for more information about anomaly action rules.





## Add data from the Splunk platform to Splunk UBA


Add CIM-compliant data from the Splunk platform to Splunk UBA
Add CIM-compliant data mapped to security-relevant data models from the Splunk platform to Splunk UBA.

Use Splunk Direct to add a CIM-compliant data source to Splunk UBA
After you determine the Splunk UBA categories that correspond to your CIM-compliant data, add the data to Splunk UBA.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source and complete the pages in the wizard to configure the data source.
Step 1 of 7: Data Source Type
Select a data source type of Splunk and click Next.
Step 2 of 7: Connection
Specify a name for the data source, such as SplunkEnterprise. The data source name must be alphanumeric with no spaces or special characters.
Type a connection URL that matches the URL for your Splunk platform or Enterprise Security search head and management port, for example, https://splunksearchhead.splunk.com:8089. If you have search head clustering configured and a load balancer is available, you can specify the load balancer host name to avoid a single point failure. Ensure that port 8089 is accessible on the load balancer.
Type the username and password for the Splunk platform account.
Leave the default Connector Type of Splunk Direct.
Click Next.
Step 3 of 7: Time Range
Select a time range.
To continuously retrieve data using time-based micro batch queries, select Live and All time. See How data gets into Splunk UBA.
To retrieve for a specific time window, select Live and Time Window and specify a time period. For example, specify 8h 30s to retrieve data for the past 8 hours and 30 seconds. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
To add historical data from the Splunk platform, select Date Range and select a calendar date range. Only events within the specified calendar window are retrieved. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
Click Next.
Step 4 of 7: Events to Process
Select the events to process. Select Splunk Query and enter a search in the field to identify the source type.
You can select Source Types to view a list of all source types from the Splunk platform. Select the check boxes for source types that all map to the same CIM data model and Splunk UBA category. For example, select three different source types of data mapped to the Malware data model that map to the Host AV category in Splunk UBA.
Selecting Source Types can have a significant impact on the performance of the Splunk indexers.

Click Next.
Step 5 of 7: Data Format
Select the category for the data source types that you selected.
Review the list of field mappings to make sure that fields are correctly mapped. The wizard automatically maps a field from the Splunk platform data to the corresponding field expected by Splunk UBA if it has the same name. If needed, map the fields required by Splunk UBA to the matching fields in the Splunk platform data.
Click Next.
Step 6 of 7: Splunk Query
Review the Splunk search created by the wizard. If you want, run the search in the Splunk platform to verify that the data output matches what you expect to see.
The source type in the Splunk platform appears on threats and anomalies in Splunk UBA. If you want to alias the source type to a more meaningful or accurate value, add an eval statement to the search to set the source type value to a custom value: |eval sourcetype="Your Custom Value"
When creating a Splunk Direct data source with multiple formats, make sure the SPL ends with | fields * so all fields are returned to Splunk UBA from the Splunk platform.

When using Splunk Direct to create a data source, Splunk UBA performs CIM validation on a sample of events returned from the query and suggests the categories you should select based on the tags observed, specifies the coverage percentage, and provides recommendations. In this example, the DHCP category was selected in the previous step but Splunk UBA's CIM validation shows that there is no coverage for this category based on sample events. In this situation, return to the previous screen and remove DHCP as a category.
This image shows Step 6 of 7 when creating a new data source. There is Tag Value Issues warning with the text: "The Splunk data for some of the categories have low coverage and some more of the categories can be added based on the tag values returned from the sample query. See Low coverage & Recommended sections below. Please make sure you selected the proper categories and field mappings." The remainder of the screen shows the DHCP category with 0.00% coverage.

If Splunk UBA detects full coverage on the sample events, no message is displayed.
Step 7 of 7: Test Mode
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK to save the data source.
You can select multiple sourcetypes and map the sourcetypes to multiple categories. It is a good idea to start by mapping one category at a time because mapping fields can be complex when attempting to map different types of data at the same time.

Use Splunk Direct to add all CIM-compliant data sources to Splunk UBA
If all of your data sources are CIM-compliant, you can add them all to Splunk UBA together. Select the CIM Compliant checkbox in the Connection page to have Splunk UBA generate a query that will pull all events from all indexes in your Splunk platform instance.

If you want to pull all events from all indexes, you must meet these conditions:

Work with a Splunk UBA customer support representative to make sure all of your data sources are verified CIM compliant. Any events that are not CIM compliant are dropped.
After the search is run, there are fewer than 10,000 events per second returned.
To add all of your CIM-compliant data sources to Splunk UBA together, perform the following tasks:

In Splunk UBA, select Manage > Data Sources.
Click New Data Source and complete the pages in the wizard to configure the data source.
Step 1 of 5: Data Source Type
Select a data source type of Splunk and click Next.
Step 2 of 5: Connection
Specify a name for the data source, such as SplunkEnterprise. The data source name must be alphanumeric, with no spaces or special characters.
Type a connection URL that matches the URL for your Splunk platform or Enterprise Security search head and management port (for example: https://splunksearchhead.splunk.com:8089). If search head clustering is configured and a load balancer is available, you can specify the load balancer host name to avoid a single point of failure. Ensure that port 8089 is accessible on the load balancer.
Type the username and password for the Splunk platform account.
Leave the default Connector Type of Splunk Direct.
Select the CIM Compliant checkbox to ingest all your CIM compliant data as a single source and automatically parse events based on the value in thetag field.
Verify the quality of the data and CIM compliance before using the CIM Compliant option; data that is not CIM compliant is dropped. You can create multiple CIM compliant data sources and edit the query so that each data source only ingests a subset of the data from all Splunk indexes.
Click Next.
Step 3 of 5: Time Range
Select a time range.
To continuously retrieve data using time-based micro batch queries, select Live and All time. See How data gets into Splunk UBA.
To retrieve for a specific time window, select Live and Time Window and specify a time period. For example, specify 8h 30s to retrieve data for the past 8 hours and 30 seconds. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
To add historical data from the Splunk platform, select Date Range and select a calendar date range. Only events within the specified calendar window are retrieved. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
Click Next.
Step 4 of 5: Splunk Query
Review the Splunk search created by the wizard. If you want, run the search in the Splunk platform to verify that the data output matches what you expect to see.
The query represents all of the data sources found in your environment, so you can edit it to include only the data sources that are verified CIM compliant.
If the query results exceed 10,000 events per second, you can create multiple, smaller data sources using any subset of queries for each data source.
Click Next.

Step 5 of 5: Test Mode
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK to save the data source.
Monitor and verify your data
After you add a CIM-compliant data source using the Splunk Direct connector, you can perform any of the following tasks for verifying your data source:

Examine some sample events to make sure that the fields are being parsed correctly. See Verify that you successfully added the data source.
If any events have field values that can't be accepted by Splunk UBA, the events may be dropped and therefore not used by any data models. See Monitor the quality of data sent from the Splunk platform.
Check the status of your data sources. See Review and edit existing data sources in Splunk UBA.
See if there are any data sources that are missing fields, thus preventing detections from being activated in Splunk UBA. See Validate data availability.

Add raw events from the Splunk platform to Splunk UBA
You can add data that is not CIM-compliant and is from a supported data source type. View supported data source types on the Data Format page in the Edit Data Source Types window. Consider mapping the data to the appropriate CIM data model and use the method described in Add CIM-compliant data to Splunk UBA from the Splunk platform to add the data.

You can add data from multiple time zones using this method. By default, the connector.splunk.use.time property is set to true to allow data from multiple time zones. For more information about time zones and events in the Splunk platform, see Specify time zones for timestamps in Splunk Enterprise Getting Data In.

To add data that is not CIM-compliant or not from a supported data source type, contact Splunk Professional Services.

Add data from one source type in the Splunk platform to Splunk UBA
In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select Splunk as the data source type and click Next.
Specify a name for the data source, such as Splunk. The data source name must be alphanumeric, with no spaces or special characters.
Type a connection URL that matches the URL for your Splunk platform or Enterprise Security search head and management port. For example, TCP port 8089.
Type the user name and password for the Splunk platform account.
Select a Connector Type of Splunk Raw Events and click Next.
Select a time range.
To continuously retrieve data using time-based micro batch queries, select Live and All time. See How data gets into Splunk UBA.
To retrieve for a specific time window, select Live and Time Window and specify a time period. For example, specify 8h 30s to retrieve data for the past 8 hours and 30 seconds. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
To add historical data from the Splunk platform, select Date Range and select a calendar date range. Only events within the specified calendar window are retrieved. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
Click Next.
Click Source Types to view the source types from your Splunk platform data.
Splunk UBA will try to form a connection with the Splunk platform and find source types across all default indexes. If no source types appear, you may have a firewall rule preventing you from being able to query the Splunk platform. You must be able to connect to the Splunk platform and see at least one data source type before you continue.
Select one data source type and click Next.
Select Single Format.
Select the format from the drop-down list of formats.
Click Next.
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK.
Add data from multiple source types in the Splunk platform to Splunk UBA
Follow this procedure to add multiple data source types from the Splunk platform to Splunk UBA:

In Splunk UBA, select Manage > Data Sources.
Click New Data Source.
Select Splunk as the data source type and click Next.
Specify a name for the data source, such as Splunk. The data source name must be alphanumeric, with no spaces or special characters.
Type a connection URL that matches the URL for your Splunk platform or Enterprise Security search head and management port. For example, TCP port 8089.
Type the user name and password for the Splunk platform account.
Select a Connector Type of Splunk Raw Events and click Next.
Select a time range.
To continuously retrieve data using time-based micro batch queries, select Live and All time. See How data gets into Splunk UBA.
To retrieve for a specific time window, select Live and Time Window and specify a time period. For example, specify 8h 30s to retrieve data for the past 8 hours and 30 seconds. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
To add historical data from the Splunk platform, select Date Range and select a calendar date range. Only events within the specified calendar window are retrieved. This is a one-time search and is performed when the data source is added to Splunk UBA. Micro-batch queries are not used for this search.
Click Next.
Click Source Types to view the source types from your Splunk platform data.
Splunk UBA will try to form a connection with the Splunk platform and find source types across all default indexes. If no source types appear, you may have a firewall rule preventing you from being able to query the Splunk platform. You must be able to connect to the Splunk platform and see at least one data source type before you continue.
Select one data source type and click Next.
Select Multiple Formats.
Click Edit Splunk Types Mapping.
Review the list of existing mappings for the data source types you want to add.
If your data source type is not listed, click Add Mapping and type the Splunk source type in the Splunk Type text box.
Do not remove any of the existing mappings, as they may be used by other data sources in your system.

Select the UBA Format that matches each data source type from the drop-down list of formats. Specify the Splunk Type in all capital letters.
Click OK to save the data source type mapping.
Click Next.
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK.


Add custom data to Splunk UBA using the generic data source
Use the generic data source type in Splunk UBA to add data that is not CIM compliant and not supported by any of Splunk UBA's native parsers. For example, you may want to add add credit card authorization and transaction data and use the custom use case framework to develop custom models to raise anomalies. See What is the custom use case framework?

Credit card data is not CIM compliant, and Splunk UBA does not have a native parser to support this data format using the Splunk Raw Events data type. Perform the following tasks to get this data into Splunk UBA as a generic data source.

In Splunk UBA, select Manage > Data Sources.
Click New Data Source and complete the pages in the wizard to configure the data source.
Step 1 of 7: Data Source Type
Select a data source type of Splunk and click Next.
Step 2 of 7: Connection
Specify a name for the data source, such as SplunkEnterprise. The data source name must be alphanumeric with no spaces or special characters.
Type a connection URL that matches the URL for your Splunk platform or Enterprise Security search head and management port, for example, https://splunksearchhead.splunk.com:8089. If you have search head clustering configured and a load balancer is available, you can specify the load balancer host name to avoid a single point failure. Ensure that port 8089 is accessible on the load balancer.
Type the username and password for the Splunk platform account.
Leave the default Connector Type of Splunk Direct.
Click Next.
Step 3 of 7: Time Range
Select a time range.
To retrieve data using time-based micro batch queries, select Live and All time. See How data gets into Splunk UBA.
To retrieve data at a regular interval defined by a time window, select Live and Time Window and specify a time period.
To add historical data from the Splunk platform, select Date Range and select a calendar date range.
Click Next.
Step 4 of 7: Events to Process
Select Splunk Query and enter a search in the field to identify the source type.
Click Next.
Step 5 of 7: Data Format
Select Single Format.
Select the GENERIC format from the drop-down list of formats.
Click Next.
Step 6 of 7: Splunk Query
Review the Splunk search created by the wizard. If you want, run the search in the Splunk platform to verify that the data output matches what you expect to see.
The source type in the Splunk platform appears on threats and anomalies in Splunk UBA. If you want to alias the source type to a more meaningful or accurate value, add an eval statement to the search to set the source type value to a custom value:
| eval sourcetype="Your Custom Value"
Step 7 of 7: Test Mode
To add the data source in test mode, leave the check box selected. See Add data sources to Splunk UBA in test mode.
Click OK to save the data source.


Send data from the Splunk platform directly to Kafka
When working with large data sets, you can send events from the Splunk platform directly to Kafka for ingestion.

See Send data from the Splunk platform directly to Kafka in the Splunk UBA Kafka Ingestion App manual.



Splunk UBA category to Splunk CIM field mapping reference
When adding CIM-compliant data to Splunk UBA, the field names from the data source must match the field names expected by Splunk UBA. Mapping the data source field names to the field names expected by Splunk UBA happens automatically when possible, but is not always possible. In those cases, you can use these tables to map the fields in Splunk UBA. See Use connectors to add data from the Splunk platform to Splunk UBA.

Do not make changes to the tags, eventtypes, or data in the Splunk platform.

Splunk UBA categories and corresponding CIM data models
Splunk UBA categories rely on the tags from CIM-compliant events to correctly parse data from the Splunk platform. Review this table to determine which category in Splunk UBA corresponds to the CIM data model that the events in the Splunk platform are mapped to. Click the name of the Splunk UBA category to review the field mappings between Splunk UBA and the CIM data models.

The tags in the table have an implied AND and are evaluated as follows:

Categories that require a single tag such as Authentication will evaluate based on that tag. For example, authentication events must have tag=authentication to be parsed by Splunk UBA. Splunk UBA generates error messages when the percentage of valid events drops below a specific threshold.
Categories with multiple tags such as DHCP have an implied AND among the tags, and are evaluated using a combination of all tags. For example, DHCP events must have all three of tag=network, tag=session, tag=dhcp to be parsed by Splunk UBA. Splunk UBA generates error messages when the combined percentage of valid events falls below a specific threshold.
Splunk UBA category	Tags required by Splunk UBA	CIM data model	Example data source types
Authentication	tag=authentication	Authentication	Source type for the Splunk Add-on for Cisco ISE
Badge	tag=badge	N/A	Brivo TA
Cloud Storage	tag=cloud	N/A	Splunk Add-on for Box
Database	tag=database	Databases	Splunk Add-on for Oracle Database
DHCP	tag=network
tag=session
tag=dhcp	The DHCP dataset of the Network Sessions data model	Source types for the Splunk Add-on for Infoblox
DNS	tag=network
tag=resolution
tag=dns	Network Resolution (DNS)	Source types for ISC BIND
DLP	tag=dlp
tag=incident
tag=email
(tag=email is needed only for email DLP events)	Data Loss Prevention	Source types for the Splunk Add-on for Symantec DLP
Email	tag=email	Email	Overview of TA-Exchange-Mailbox included with the Splunk Add-ons for Microsoft Exchange
Endpoint	See Endpoint category for specific combinations.	Endpoint	Source types for the Splunk Add-on for Bit9 Carbon Black
External Alarm	tag=attack	Intrusion Detection	Palo Alto Networks Add-on for Splunk
Firewall	tag=network
tag=communicate	Network Traffic	Palo Alto Networks Add-on for Splunk
Host AV	tag=malware
tag=attack
tag=operations	The Malware_Operations dataset and the Malware_Attacks dataset of the Malware data model	Source types for the Splunk Add-on for Symantec Endpoint Protection
IDS/IPS	tag=ids
tag=attack	Intrusion Detection	Source types for the Splunk Add-on for Bit9 Carbon Black
Source types for the Splunk Add-on for Cisco FireSIGHT
Printer	tag=printer	N/A	View printer information in Splunk Add-on for Microsoft Windows.
VPN	See VPN category for specific combinations.	The VPN dataset of the Network Sessions data model	Source types for the Splunk Add-on for Juniper
Web Proxy	tag=web
tag=proxy	The Proxy dataset of the Web data model	Source types for the Splunk Add-on for Squid Proxy
Authentication category
The Authentication category for Splunk UBA maps to the Authentication data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
action	Y	The action performed on the resource.	success, failure, unknown, added
app	N	The application involved in the event.	ssh, splunk, win:local
dest_ip	Y	The destination IP address involved in the authentication.	192.168.10.11
dest_host	N	The host name of the destination involved in the authentication.	winhost1
duration	N	The amount of time in seconds that it took to complete the authentication event.	2
src_ip	Y	The source IP address involved in the authentication.	192.168.10.12
src_host	N	The host name of the source involved in the authentication.	winhost2
src_user	N	In privilege escalation events, src_user represents the user who initiated the privilege escalation. This field is unnecessary when an escalation is not performed.	user1
user	Y	The name of the user for whom the authentication is being performed.	user2
protocol	Y	The protocol used for the authentication.	TACACS
eventtype	Y	The type of the event.	acs_authentication_success
Any custom field name, such as authType or loginType	N	The authentication login type. The default is Network. If specified, the value must be one of the categories in Filter the anomaly table.	Exfiltration
Badge category
Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
vendor	N	The vendor of the badge access solution.	brivo
category	Y	The category of the badge access event.	Failed Access
user	Y	The user involved in this badge access event.	cronaldo
site_name	Y	The location of the building.	123 Main Street
object_type	N	The type of device used in the badge access event.	ACCESS_POINT
object_name	N	The location in the building where the badge access was requested.	Mail Room
failure_reason	N	The reason for the failed operation.	Unauthorized Access Attempt
Cloud Storage category
Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
file_size	N	The size in bytes of the resource associated to this event.	10280
object	Y	The name of the file.	this_picture.png
object_type	Y	The type of the file.	File, Folder, Document, Image, etc.
file_hash	Y	The unique identifier of the resource. This should be assigned by the product, such as Box, Sharepoint, or Google Drive.	17283982137
object_path	Y	The absolute or relative location of the resource.	/bpatinho/photos
parent_category	N	The type of the parent resource.	Folder, Link, etc.
parent_hash	Y	The unique identifier of the parent resource. This should be assigned by the product, such as Box, Sharepoint, or Google Drive.	9864239674
src_user	Y	The user creating this event.	user1
change_type	Y	The type of access.	Download, Preview, Delete, Create, Edit.
app	Y	The application that is generating this event.	Box, Office365, Google Drive.
dest_user	N	The user targeted by this action. Usually this is linked to permission changes made by another user, such as when an admin change the privileges of a user in a file.	cronaldo
Data Loss Prevention category
The Data Loss Prevention (DLP) category for Splunk UBA maps to the Data Loss Prevention data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
severity	Y	The severity of the network protection event.	informational, unknown, low, medium, high, critical
action	Y	The action taken by the DLP device.	allowed, blocked
app	N	The application involved in the event.	Symantec DLP
src_ip	N	The source of the network traffic (the client requesting the connection).	10.10.10.12
src_host	N	The host name of the source.	winhost1
dest_ip	N	The IP address of the destination.	2.2.2.2
dest_host	N	The host name of the destination.	winhost2
user_department	N	The department of the user involved in the activity reported by DLP.	Finance
category	Y	The category of the DLP event.	malware, keylogger, ad-supported program
recipient	N	The individual email addresses of the message recipients.	a@b.com,c@b.com
sender	N	The email address of the message sender.	d@b.com
subject	N	The subject of the email message.	Important Message, Open Now!
policy	N	The policy that triggered the DLP alarm.	Social Security Number
signature	Y	The type of the event.	HTTP Incident
dlp_status	N	The DLP incident status.	Working
prevention_status	N	The DLP incident prevention status.	9, Blocked
event_type_id	N	The event type ID.	13
vendor	N	The USB vendor.	FUJITSU
serial_number	N	The serial number of USB device.	1234567890
device_id	N	The ID of the USB device.	987654
src_user	N	The source user involved in the activity reported by DLP.	cronaldo
dest_user	N	The destination user involved in the activity reported by DLP.	cronaldo
src_file	N	The name of the source file involved.	creditcards.xls
src_path	N	The path of the source file involved.	c:\documents
dest_file	N	The name of the destination file involved.	creditcards.xls
dest_path	N	The path of the destination file involved.	c:\documents
file_size	N	The size in bytes of the file transferred	10000
restricted	N	Is it a sensitive or restricted file?	no,yes
match_count	N	The number of unique matches of the DLP signature.	1,10,1040
Database category
The Database category for Splunk UBA maps to the Databases data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
dest_ip	N	The IP address of the destination.	2.2.2.2
dest_host	N	The host name of the destination.	winhost2
command_name	N	The SQL query command.	select, locktable, insert, delete
query	N	The full database query.	select * from my_table
action_name	N	The action performed by the user.	LOGON, LOGOFF, CREATE FUNCTION
instance_name	Y	The name of the database instance.	myinstance
object	N	The name of the database object.	view1, index1
tablespace_name	N	The name of the tablespace.	my table space
commits	N	The number of commits per second performed by the user associated with the session.	5
cpu_used	N	The number of CPU centiseconds used by the session. Divide this value by 100 to get the CPU seconds.	1
elapsed_time	N	The total amount of time in seconds that elapsed since the user started the session by logging into the database server.	10
records_affected	N	The number of records affected by the database query.	1
tables_hit	N	The names of the tables hit by the query.	table1, table2
vendor	N	The vendor and product name of the database system. This field can be automatically populated by vendor and product fields in your data.	oracle
user	Y	The name of the database process user.	cronaldo
eventtype	Y	The type of event.	oracle_auth, oracle_session
duration	N	The duration in seconds of the database connection.	241
src_ip	N	The IP address of the source server of the database event.	10.10.10.12
src_host	N	The domain name of the source server of the database event.	winhost1
DHCP category
The DHCP category for Splunk UBA maps to the DHCP dataset of the Network Sessions data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
lease_duration	Y	The duration in seconds of the Dynamic Host Configuration Protocol (DHCP) lease.	2000
dest_ip	Y	The assigned IP address.	192.168.1.12
dest_host	N	The host name of the machine to which the IP address is being assigned.	winhost1
dest_mac	Y	The MAC address of the machine to which the IP address is being assigned.	ad:7b:3d:db:49:8b
signature	Y	An indication of the type of network session event.	DHCPACK, DHCPOFFER, DHCPREQUEST, DHCPINFORM, DHCPDISCOVER , DHCPNAK, DHCPDECLINE, DHCPRELEASE
DNS category
The DNS category for Splunk UBA maps to the Network Resolution (DNS) data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
src_ip	Y	The source IP address of the network resolution event.	192.168.1.11
src_port	N	The source port of the network resolution event.	3022
dest_ip	N	The destination IP address of the network resolution event.	192.168.1.14
query	Y	The domain that needs to be resolved.	www.google.com
answer	Y	The resolved address for the query.	12.13.14.15
query_type	Y	The field may contain DNS OpCodes or Resource Record Type codes.	Query, IQuery, Status, Notify, Update, unknown, A, MX, NS, PTR
duration	N	The amount of time in seconds taken by the network resolution event.	1
ttl	N	The time-to-live of the network resolution event.	2000
record_type	N	The DNS resource record type.	A, DNAME, MX, NS, PTR
message_type	Y	The type of DNS message.	Query, Response
Email category
The Email category for Splunk UBA maps to the Email data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
direction	Y	The email direction, based on the sender.
If the sender is an internal employee, then the email is considered outbound.
If the sender is not an internal employee, then the email is considered inbound.
inbound, outbound
action	N	The action taken by the reporting device.	delivered, blocked, quarantined, deleted, unknown
file_size	N	The size of the files attached the message, if any.	10280
file_name	N	The names of the files attached to the message, if any.	example.txt
recipient	Y	A field listing individual recipient email addresses.	abc@example.com, bcd@example.com
sender	Y	The email address of the email sender.	sender@example.com
subject	Y	The subject of the email message.	Important Message, Open Now!
eventtype	Y	The type of the event.	stream_email(email)
src_ip	N	The source IP address of the system that sent the message.	11.12.13.14
src_user	N	The source user involved in the email exchange.	cronaldo
Endpoint category
The Endpoint category for Splunk UBA maps to the Endpoint data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping. Splunk UBA requires the following tag combinations to process endpoint category events:

To properly parse port data, Splunk UBA requires tag=listening, tag=port.
To properly parse process data, Splunk UBA requires tag=process, tag=report.
To properly parse service data, Splunk UBA requires tag=service, tag=report.
To properly parse filesystem data, Splunk UBA requires tag=endpoint, tag=filesystem.
To properly parse registry data, Splunk UBA requires tag=endpoint, tag=registry.
The Endpoint category contains multiple datasets. Some fields have the same names across multiple datasets.

The status field exists in the Registry and Service datasets.
The user field exists in the Ports, Processes, Services, Registry, and Filesystem datasets.
The action field exists in the Endpoint category as well as the Ports dataset of the Endpoint category.
Splunk CIM field name	Required	Field description	Example values
endpoint_ip, dest_ip	N	IP address of the endpoint where the activity happened.	1.1.1.1
endpoint_dns, dest_host	N	The host name of the endpoint.	winhost1
endpoint_nt_domain, dest_nt_domain	N	The NT domain of the endpoint, if applicable.	acme
endpoint_port	N	Network port listening on the endpoint.	53
eventtype	Y	The type of the event.	symantec_ep_risk_alert_virus, A service was installed in the system
event_id	N	The event ID or code for the activity.	7045
category	N	The event category, if applicable.	malware, watchlist.hit.ingress.process
signature	N	The sub-category or signature of the event, if applicable.	process_blocking
Any custom field name, such as alarmCategories or endpointCategory.	N	The categories that this external alarm belongs to. Multiple categories can be separated by comma. The values must be one or more of the categories in Filter the anomaly table.	Exfiltration
severity	N	The severity of the endpoint event.	informational, unknown, low, medium, high, critical
action	Y	The action taken by the endpoint.	allowed, blocked
src_ip	N	The IP address of the "remote" system connected to the listening port (if applicable).	2.2.2.2
src_port	N	The "remote" port connected to the listening port (if applicable).	53
src_host, src_dns	N	The hostname of the "remote" system connected to the listening port (if applicable)	acmehost1
Ports dataset
creation_time	N	The epoch time at which the network port started listening on the endpoint.	1547749588
dest_port	N	The network port listening on the endpoint.	53
process_id	N	The numeric identifier of the process assigned by the operating system.	12345
state	N	The status of the listening port.	established, listening
transport	N	The network transport protocol associated with the listening port.	tcp, udp
user	N	The user account that spawned the process.	cronaldo
vendor_product	N	The vendor and product name of the Endpoint solution that reported the event.	Carbon Black Cb Response
action	N	The action performed on the resource.	acl_modified, created, deleted, modified, read
cpu_load_percent	N	CPU load consumed by the process (in percent)	85
mem_used	N	Memory in bytes used by the process.	12345
os	N	The operating system of the resource.	Microsoft Windows Server 2008r2
Processes dataset
parent_process_path	N	The full command string of the parent process.	C:\\WINDOWS\\system32\\cmd.exe \/c \"\"C:\\Program Files\\SplunkUniversalForwarder\\etc\\system\\bin\\powershell.cmd\" --scheme
parent_process_exec	N	The executable name of the parent process.	notepad.exe
parent_process_guid	N	The globally unique identifier of the parent process assigned by the vendor_product.	0dd879c-ee2f-11db-8314-0800200c9a66
parent_process_id	N	The numeric identifier of the parent process assigned by the operating system.	12345
parent_process_name	N	The friendly name of the parent process.	notepad.exe
process_id	N	The numeric identifier of the process assigned by the operating system.	12345
process	N	The full command string of the spawned process.	C:\\WINDOWS\\system32\\cmd.exe \/c \"\"C:\\Program Files\\SplunkUniversalForwarder\\etc\\system\\bin\\powershell.cmd\" --scheme
process_current_directory	N	The current working directory used to spawn the process.	/usr/bin/
process_exec	N	The executable name of the process.	notepad.exe
process_guid.	N	The globally unique identifier of the process assigned by the vendor_product.	example_guid, example_id
process_hash	N	The digests of the parent process.	<md5>, <sha1>
process_integrity_level	N	The Windows integrity level of the process.	System, Medium
process_path	N	The file path of the process.	C:\Windows\System32\notepad.exe
user	N	The unique identifier of the user account which spawned the process.	example_user
Services dataset
description	N	The description of the service.	Example description
service_dll	N	The dynamic link library associated with the service.	Svc.exe
service_dll_hash	N	The digests of the dynamic link library associated with the service.	<md5>, <sha1>
service_dll_path	N	The file path to the dynamic link library associated with the service.	C:\Windows\System32\comdlg32.dll
service_dll_signature_exists	N	Whether or not the dynamic link library associated with the service has a digitally signed signature.	true
service_dll_signature_verified	N	Whether or not the dynamic link library associated with the service has had its digitally signed signature verified.	true
service_exec	N	The executable name of the service.	svchost.exe
service_hash	N	The digests of the service.	<md5>, <sha1>
service_id	N	The unique identifier of the service assigned by the operating system.	12345
service_name	N	The friendly service name.	example_name
service_path	N	The file path of the service.	C:\WINDOWS\system32\svchost.exe
start_mode	N	The start mode for the service.	example_mode
status	N	The status of the service or registry.	critical, started, stopped, warning, failure, success
user	N	The user account associated with the service or the filesystem access, or the registry access.	cronaldo
Filesystem dataset
file_access_time	N	The epoch time that the file (the object of the event) was accessed.	1547749588
file_create_time	N	The epoch time that the file (the object of the event) was created.	1547749588
file_modify_time	N	The epoch time that the file (the object of the event) was altered.	1547749588
file_acl	N	Access controls associated with the file affected by the event.	readonly
file_name	N	The name of the file.	notepad.exe
file_path	N	The path of the file.	C:\Windows\System32\notepad.exe
file_size	N	The size in kilobytes of the file that is the object of the event.	5346
user	N	The user account associated with the service or the filesystem access, or the registry access.	cronaldo
Registry dataset
registry_hive	N	The logical grouping of registry keys, subkeys, and values.	HKEY_CURRENT_CONFIG, HKEY_CURRENT_USER
registry_key_name	N	The name of the registry key.	PrinterDriverData
registry_path	N	The path to the registry value.	\win\directory\directory2\{676235CD-B656-42D5-B737-49856E97D072}\PrinterDriverData
registry_value_data	N	The unaltered registry value.	example_value
registry_value_name	N	The name of the registry value.	example_name
registry_value_text	N	The textual representation of registry_value_data (if applicable).	example_text
registry_value_type	N	The type of the registry value.	REG_BINARY, REG_DWORD, REG_DWORD_LITTLE_ENDIAN, REG_DWORD_BIG_ENDIAN, REG_EXPAND_SZ, REG_LINK, REG_MULTI_SZ, REG_NONE, REG_QWORD, REG_QWORD_LITTLE_ENDIAN, REG_SZ
status	N	The status of the service or registry.	failure, success
user	N	The user account associated with the service or the filesystem access, or the registry access.	cronaldo
External Alarm category
The External Alarm category for Splunk UBA maps to the Intrusion Detection data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
Any custom field name, such as alarmCategories or alarmType	Y	The categories that this external alarm belongs to. Multiple categories can be separated by comma. The values must be one or more of the categories in Filter the anomaly table.	Exfiltration
category	N	The category of the event, if applicable.	malware, watchlist.hit.ingress.process
severity	N	The severity of the external alarm.	informational, unknown, low, medium, high, critical
action	N	The action taken by the external device.	allowed, blocked, deferred
app	N	The application involved in the event.	ssl
src_ip	N	The source of the network traffic, such as the client requesting the connection.	10.10.10.12
src_host	N	The host name of the source.	winhost1
src_zone	N	The source zone.	contractor
dest_ip	N	The IP address of the destination.	2.2.2.2
dest_host	N	The host name of the destination.	winhost2
dest_zone	N	The destination zone.	PCI
user	N	The user involved in the activity reported.	cronaldo
url	N	The URL accessed in the request.	http://subdomain.acme.com/index.html
signature or eventtype	Y	The type of the event.	URL Filtering
Firewall category
The Firewall category for Splunk UBA maps to the Network Traffic data model and the additional firewall tag.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
action	Y	The action taken by the firewall.	allowed, blocked
app	N	The application protocol of the traffic.	SSL
bytes_in	Y	The number of inbound bytes transferred.	1028
packets_in	N	The number of inbound packets transferred.	5
bytes_out	Y	The number of outbound bytes transferred.	140
packets_out	N	The number of outbound packets transferred.	6
bytes	N	The total number of bytes transferred (bytes_in + bytes_out).	1168
protocol	Y	The OSI layer 3 (network) protocol of the traffic observed, in lowercase.	ip, appletalk, ipx
src_ip	Y	The source of the network traffic, such as the client requesting the connection.	10.10.10.12
src_host	N	The host name of the source.	winhost1
src_port	N	The port number of the source.	12345
src_zone	N	The source zone.	contractor
src_translated_ip	N	The NATed IPv4 or IPv6 address from which a packet is sent.	192.168.1.11
dest_ip	Y	The IP address of the destination.	2.2.2.2
dest_host	N	The host name of the destination.	winhost2
dest_port	N	The port number of the destination.	1234
dest_zone	N	The destination zone.	PCI
dest_translated_ip	N	The NATed IPv4 or IPv6 address to which a packet is sent.	192.168.1.12
user	N	The user who requested the traffic flow.	cronaldo
url	N	The URL accessed in the request.	http://subdomain.acme.com/index.html
duration	N	The amount of time in seconds for the completion of the network event.	241
vendor_action	Y	The type of the event.	Teardown TCP, Built inbound connection
Host Antivirus category
The Host Antivirus (AV) category for Splunk UBA maps to the Malware_Operations dataset and the Malware_Attacks dataset of the Malware data model. Host AV refers to endpoint antivirus products.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
Any custom field name, such as alarmCategories or avCategory.	N	The categories that this external alarm belongs to. Multiple categories can be separated by comma. The values must be one or more of the categories in Filter the anomaly table.	Exfiltration
category	N	The category of the event, if applicable.	malware, watchlist.hit.ingress.process
signature	Y	The subcategory or signature of the event, if applicable.	process_blocking
severity	N	The severity of the network protection event.	informational, unknown, low, medium, high, critical
action	Y	The action taken by the AV.	allowed, blocked
dest_ip	Y	The IP address of the system that was affected by the malware event.	2.2.2.2
dest_host	N	The host name of the system that was affected by the malware event.	winhost2
dest_nt_domain	N	The NT domain of the destination, if applicable.	acme
duration	N	The amount of time in seconds for the completion of the activity reported by AV.	241
user	N	The user involved in the activity reported by AV.	cronaldo
url	N	A URL containing more information about the vulnerability.	http://www.mydomain.com/a.html
file_name	N	Name of the file involved.	creditcards.xls
file_path	N	The path of the file involved.	c:\documents
eventtype	Y	The type of the event.	symantec_ep_risk_alert_virus
Intrusion Detection System and Intrusion Prevention System category
The Intrusion Detection System (IDS) and Intrusion Prevention System (IPS) category for Splunk UBA maps to the Intrusion Detection data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
Any custom field name, such as alarmCategories or idsCategory	Y	The categories that this external alarm belongs to. Multiple categories can be separated by comma. The values must be one or more of the categories in Filter the anomaly table.	Exfiltration
category	N	The category of the event, if applicable.	malware, watchlist.hit.ingress.process
signature	Y	The sub-category or signature of the event, if applicable.	process_blocking
severity	Y	The severity of the network protection event.	informational, unknown, low, medium, high, critical
action	Y	The action taken by the IDS.	allowed, blocked
bytes_in	N	The number of inbound bytes transferred.	1028
bytes_out	N	The number of outbound bytes transferred.	140
bytes	N	The total number of bytes transferred (bytes_in + bytes_out).	1168
src_ip	Y	The source of the network traffic (the client requesting the connection).	10.10.10.12
src_host	N	The host name of the source.	winhost1
src_port	N	The port number of the source.	12345
dest_ip	Y	The IP address of the destination.	2.2.2.2
dest_host	N	The host name of the destination.	winhost2
dest_port	N	The port number of the destination.	1234
duration	N	The amount of time in seconds for the completion of the activity reported by IDS.	241
user	N	The user involved in the activity reported by IDS.	cronaldo
ids_type	N	The type of IDS that generated the event.	network, host, application
eventtype	Y	The type of the event.	cisco_ips_vulnerable
Printer category
Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
file_name	Y	The name of the file that was printed.	LIN111757BPAM08-04Laboratory17-10-15-12104.pdf
user	Y	The user involved in the activity reported.	cronaldo
printer	N	The printer identifier.	acmeprinter1
driver_process	N	The name of the driver.	HP LaserJet M3035 mfp PCL6
type	N	The type or log.	PrintJob
operation	N	The printer operation.	add
file_size	N	The size of the file being printed.	10280
job_id	N	The print ID of the job.	35
data_type	N	The data type of the file that was printed.	NT EMF 1.008
print_processor	N	The print processor.	hpzppwn7
parameters	N	The print parameters.	
status	N	The status of print job.	printing
priority	N	The priority of the print job.	1
total_pages	N	The total number of pages printed.	10
page_printed	N	The page that was printed.	7
submitted_time	N	The time that the print job was submitted. The format must be either MM/dd/yyyy HH:mm:ss.SSS or MM/dd/yyyy.	05/22/2019 13:10:44:001
src_ip	N	The IP address of the device that submitted the printer job.	10.11.12.13
src_host	N	The host name of the device that submitted the printer job.	acmehost1
signature	Y	The type of the event.	Microsoft-Windows-PrintService:812
VPN category
The VPN category for Splunk UBA maps to the VPN dataset of the Network Sessions data model, and to the Network Traffic data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping. Splunk UBA requires the following tag combinations to process VPN category events:

To properly parse when VPN connections are initiated, Splunk UBA requires tag=network, tag=session, tag=vpn, tag=start.
To properly parse traffic flow in a VPN connection, Splunk UBA requires tag=network, tag=session, tag=vpn.
To properly parse when VPN connections are terminated, Splunk UBA requires tag=network, tag=session, tag=vpn, tag=end.
Splunk CIM field name	Required	Field description	Example values
bytes_in	N	The number of bytes received by the device corresponding to the src_ip (downloads).	1028
bytes_out	N	The number of bytes sent out by the device corresponding to the src_ip (uploads).	140
bytes	N	The total number of bytes transferred by the device corresponding to the src_ip (bytes_in + bytes_out).	1168
duration	N	The duration in seconds of the VPN session. This field is expected when an end tag is present.	2000
user	Y	The name of the user for whom the authentication is being performed.	user2
src_ip	Y	The IP address of the originator of the request.	11.12.13.14
dest_ip	N	The IP address of the destination device.	192.168.1.2
Web Proxy category
The Web Proxy category for Splunk UBA maps to the Proxy dataset of the Web data model.

Every category requires a tag field. All CIM-compliant data contains at least one tag field that indicates the data model mapping.

Splunk CIM field name	Required	Field description	Example values
action	Y	The action taken by the server or proxy. If this value is not present, it can be derived from the status field.	allowed, blocked
bytes_in	Y	The number of inbound bytes transferred.	1028
bytes_out	Y	The number of outbound bytes transferred.	140
bytes	N	The total number of bytes transferred (bytes_in + bytes_out).	1168
category	N	The category of traffic provided by the proxy server.	entertainment
dest_ip	N	The IP address of the remote host.	2.2.2.2
http_content_type	Y	The content-type of the requested HTTP resource.	image/gif
http_method	Y	The HTTP method used in the request.	GET
http_referrer	N	The HTTP referrer used in the request.	referrer.acme.com
http_user_agent	Y	The user agent used in the request.	Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)
response_time	N	The amount of time it took to receive a response, if applicable, in milliseconds.	200
src_ip	Y	The source of the network traffic, such as the client requesting the connection.	10.10.10.12
status	Y	The HTTP response code indicating the status of the proxy request.	200
user	N	The user that requested the HTTP resource.	cronaldo
url	Y	The URL accessed in the request.	http://subdomain.acme.com/index.html
duration	N	The time in milliseconds taken by the proxy event.	241



Send notable events from Splunk Enterprise Security to Splunk UBA
You can send notable events from Splunk Enterprise Security (ES) to Splunk UBA to be processed for anomalies. You can use Splunk UBA to generate threats from the correlation search anomalies.

See How Splunk UBA sends and receives data from the Splunk platform in the Send and Receive Data from the Splunk Platform manual for more information.




## Add other data to Splunk UBA


Configure PowerShell logging to see PowerShell anomalies in Splunk UBA
The Suspicious PowerShell Activity model produces anomalies based on suspicious activity identified in Microsoft PowerShell and Windows security event logs. For this model to work, you must log PowerShell activity at a specific level and add those logs to Splunk UBA.

By logging PowerShell activity and analyzing the commands with Splunk UBA, you can identify indicators of compromise corresponding to malicious activity by a user or malware. PowerShell provides access to Windows API calls that attackers can exploit to gain elevated access to the system, avoiding antivirus and other security controls in the process. PowerShell is also internally utilized by popular hacking tools.

PowerShell versions compatible with Splunk UBA
The PowerShell model works best with PowerShell 5.0 or the latest version of PowerShell 4.0.

PowerShell version	PowerShell requirements
4.0	Powershell version 4.0 requires the following to enable enhanced logging for Windows 7/8.1/2008/2012:
.NET 4.5
Windows Management Framework (WMF) 4.0
The appropriate WMF 4.0 update:
8.1/2012 R2  KB3000850
2012  KB3119938
7/2008 R2 SP1  KB3109118
5.0	Powershell version 5.0 requires the following to enable enhanced logging for Windows 7/8.1/2008/2012:
.NET 4.5
Windows Management Framework (WMF) 4.0 (Windows 7/2008 only)
Windows Management Framework (WMF) 5.0
You must upgrade Windows 7 and 2008 R2 to Windows Management Framework (WMF) 4.0 prior to installing WMF 5.0.

Windows 10 does not require any software updates to support enhanced PowerShell logging.

PowerShell supports the following types of logging:

module logging
script block logging
transcription
PowerShell events are written to the PowerShell operational log Microsoft-Windows-PowerShell%4Operational.evtx.

Configure module logging for PowerShell
To enable module logging:

In the Windows PowerShell GPO settings, set Turn on Module Logging to enabled.
In the Options pane, click the button to show Module Name.
In the Module Names window, enter * to record all modules.
Click OK in the Module Names window.
Click OK in the Module Logging window.
Alternately you can set the following registry values:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ModuleLogging  EnableModuleLogging = 1
HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ModuleLogging \ModuleNames  * = *
Configure script block logging for PowerShell
To enable script block logging, go to the Windows PowerShell GPO settings and set Turn on PowerShell Script Block Logging to enabled.

Alternately, you can set the following registry value:

HKLM\SOFTWARE\Wow6432Node\Policies\Microsoft\Windows\PowerShell\ScriptBlockLogging  EnableScriptBlockLogging = 1
In addition, turn on command line process auditing. You can find instructions in the Microsoft documentation:

Go to the Microsoft documentation website.
Search for command line process auditing.
Configure command line process auditing so that the process creation audit event ID 4688 includes audit information for command line processes.
Configure transcription logging
To enable automatic transcription, or deep script block logging, enable the Turn on PowerShell Transcription feature in Group Policy through Windows Components > Administrative Templates > Windows PowerShell.

The configuration settings are stored under HKLM:\Software\Policies\Microsoft\Windows\PowerShell\Transcription.

You can find more information in the Microsoft documentation:

Go to the Microsoft documentation website.
Search for script block logging.
Verifying PowerShell logging
To verify that PowerShell logging is properly configured, look for the following PowerShell activity events in Splunk UBA:

EventCode = 4103
EventCode = 4104
EventCode = 4688 and Process_Name contains PowerShell
EventCode = 7045 and Process_Name contains PowerShell

Configure the VirusTotal script to see VirusTotal anomalies in Splunk UBA
The VirusTotal script in Splunk UBA compares existing external IP addresses and domains in Splunk UBA against VirusTotal. Any matches are added to the VirusTotal watch list, which can be viewed in Splunk UBA in Anomalies Table > Add Filter > User Watchlists. The first time the script is run, it checks data from the past 180 days. You can configure the script to run regularly after that.

Prerequisites
Verify the following before running the VirusTotal script:

Ensure that Splunk UBA node 1 can connect to https://www.virustotal.com/vtapi/v2/domain/report and https://www.virustotal.com/vtapi/v2/ip-address/report.
Make sure you have an existing VirusTotal API key. If you need to obtain a key, register in the VirusTotal community. Complete the registration form and click Sign Up.
Identify the maximum number of queries you can run using your API key. If you are using a private key, exclude your regular usage (non-UBA related searches) from this limit.
Run the script
Run the VirusTotal setup:
/opt/caspida/bin/utils/virustotal_scan/virustotal_setup.sh
The script prompts you for the following:

A disclaimer for using VirusTotal. If you accept the terms of usage, press Y.
Your Virustotal API key. Enter your API key and press Enter to continue.
Find your API key under the account details, after logging in to VirusTotal.
The VirusTotal API maximum limit of queries per minute. Provide the maximum queries that Splunk UBA can run in one minute, and then press Enter to continue.
The directory where VirusTotal script writes temporary files. By default, temporary files are written in /temp. Press Enter to continue.
Prompt you for the location where VirusTotal scan logs must be stored. By default, these logs are written in /var/log/caspida. Press Enter to continue.
The VirusTotal script is executed every Saturday. If you need to manually run the VirusTotal script at another time, perform the following tasks:

Go to the /opt/caspida/bin/utils/virustotal_scan directory.
Run the following command:
/opt/caspida/bin/utils/virustotal_scan/virustotal_scan.sh &
Do not run before the weekend to avoid double execution and locking out the API key.

Additional information
You can find more information and details about the script in the README file: /opt/caspida/bin/utils/virustotal_scan/README.txt.




## Review and verify your Splunk UBA data


Verify that you successfully added the data source
Confirm that the data source you added is successfully parsing events.

In Splunk UBA, select Manage > Data Sources.
Click the name of the data source that you added.
Review the Data Source Details.
Click the parsed events icon (The parsed events icon.) and review the 10 sample events. Make sure that each event lists event views.
There are times when some data sources, such as DHCP, DNS, AD, or HTTP do not provide a destination device. If you ingest one of these data types and see validation error messages, you can ignore these messages once you examine the raw event and validate the absence of the destination device in the raw event.

Monitor the quality of data sent from the Splunk platform
When you create a data source with Splunk Direct, you can keep track of the quality of data added from the Splunk platform. One important metric is whether the value of a particular field meets Splunk UBA requirements. When an event contains a field value that is not accepted by Splunk UBA, the event may not be parsed which can cause data models to not work as expected. You can monitor these mismatches by setting up the following configuration:

Edit /etc/caspida/local/conf/uba-site.properties and add following line at the end of the file:
splunk.direct.enum.normalize=true
(Optional) Add additional customization properties.
Add properties for monitoring interval, number of mismatch values to keep, and indicator thresholds.
#the period in milliseconds of persisting the stats continuously collected per Splunk Direct datasource in the database. Also the period of time the Splunk Direct Data Source Enum Check indicator analyzes every time it checks going back from the check time. And finally the indicator's check period
splunk.direct.enum.monitor.interval = 3600000
#number of most frequent mismatch values to keep per enum field
splunk.direct.enum.monitor.top.count = 50
# ratio=mismatch_count/total_event_count
# if ratio is between 0.1 and 0.2, indicator goes to warn
ubaMonitor.etl.enumMonitor.warn.threshold = 0.1
# if ratio is above 0.2, indicator goes to bad
ubaMonitor.etl.enumMonitor.bad.threshold = 0.2
Modify the values as needed to suit the needs of your own environment.
Run the following command to have your changes take effect.
sudo service caspida-sysmon restart
Normalization rules are defined in /etc/caspida/conf/normalize.rules. Perform the following tasks to update the rule:
Make a copy of this file and place it in /etc/caspida/local/conf/normalize.rules.
Rules belonging to the same field are grouped together. In the following example, all matches with "allow" or "allowed" will be set to "allowed". If all the rules are parsed and no match is found, a mismatched value is recorded. For example, given the following event:
{"action": "foo", "severity": 6}
And the following rules:

[action]
REGEX: (?i)allow(ed)?= allowed
REGEX: (?i)denied=blocked
REGEX: (?i)drop(ped)?= dropped
REGEX: (?i)teardown = Alerted

[severity]
RANGE: [6-7] = high
The event becomes the following after it is normalized:

{"action": "foo", "severity": "high"}
Since the action "foo" is not defined in the example, it is recorded as a mismatched value.
Synchronize the changes across all Splunk UBA nodes in your cluster.
/opt/caspida/bin/Caspida sync-cluster  /etc/caspida/local/conf
Restart Splunk UBA for the changes to take effect.
/opt/caspida/bin/Caspida stop
/opt/caspida/bin/Caspida start
Health monitor status codes are available to provide visibility into the status of events containing mismatched values compared to total events. See Health Monitor status code reference.


Review and edit existing data sources in Splunk UBA
Review the data sources to make sure that data ingestion is proceeding as expected.

View job execution times in Splunk Enterprise
Splunk UBA performs micro-batched searches in one-minute intervals against Splunk Enterprise to pull in events. Review the search job execution times to make sure that they are not exceeding one minute.

In Splunk Enterprise, select Activity > Jobs to open the Jobs page.
Filter the jobs by searching for the usernames of the Splunk UBA data sources.
Examine the value in the Runtime column to make sure that the job is taking less than one minute to execute.
Use the Search job inspector to drill down and view more information if needed.
See About jobs and job management in the Splunk Enterprise Search Manual for more information about the Jobs page and using the Search job inspector to view detailed information about a job.

Review data sources in Splunk UBA
Select Manage > Data Sources to view existing data sources and the number of events added from each data source. Key indicators reveal statistics about your data. Click a key indicator to see more detail. Review the name, type, format, status, number of events, and the date added for each data source.

Data sources in Splunk UBA can have the following statuses:

Status	What the status means about the data source
Processing	Data sources begin with this status when you create them in Splunk UBA.
Complete	File-based data sources, batch jobs, and scheduled jobs have this status when data ingestion is complete.
Stopped	Data sources have this status in the following situations:
A Stop button is clicked for a data source.
JobManager is restarted. This will temporarily stop all live jobs, and can also mark unresponsive or dead jobs as Stopped.
When ingesting live data, the data source can go into Stopped mode when data ingestion in test mode is completed.
Failed	Data sources have this status when JobManager detects any errors, such as Splunk server connectivity issues, or a data source cannot be created. The job is marked as Failed with an error message displayed in Splunk UBA.
Scheduled	Scheduled jobs, such as Human Resources (HR) data or Threat Intel, have this status before they are run.
Live data sources can only be in Processing, Stopped, or Failed state.

Edit data sources in Splunk UBA
You can edit an existing data source in Splunk UBA. For example, you can change the name of a data source or update its connection information, time range, or SPL. A data source can be edited regardless of its status.

Perform the following steps to edit a data source:

Click on the data source you want to edit. You can review detailed information about the data source such as its URL, time range, and SPL. This can help you verify the information you need to update.
Click Edit.
Make the desired changes and navigate through the Edit Data Source wizard until you reach the end.
Click OK.
Changes to a data source are not picked up by Splunk UBA until the data source is restarted.

If the data source is currently running, click Stop to stop the data source, then click Start to restart it.
If the data source is currently not running, click Start to start the data source.


Validate data availability
After data is loaded into Splunk UBA, use the Data Availability page to validate or troubleshoot your data ingestion and identify missing data sources that enable Splunk UBA use cases, such as an expected anomaly not being triggered. Data availability shows the relationships and mappings among the following areas in Splunk UBA:

Anomaly types
Anomaly categories
Threat types
Models
Data Views
Data Sources
To access the Data Availability page, select System > Data Availability in Splunk UBA.

Click on a content type in the Data Available section, which is at the top of the left column. In this example, the Unusual Machine Access anomaly is selected, and the page shows the data sources and threat model used to generate this anomaly. The box containing the anomaly name has a dark blue background indicating that all expected data sources are accounted for and the use case is operational.

This screen image shows the Data Availability page. On the left side, there is a column with the Unusual Machine Access anomaly highlighted. The main portion of the screen shows four data sources with dotted lines leading to a single model named Suspicious Device Access Model, which in turn has a dotted line leading to the Unusual Machine Access anomaly.

If Splunk UBA detects that not all data sources are available, the anomaly appears in the Partial Data Available section in the left column.

In this example, the Blacklisted Entity Model takes data to generate Blacklisted Domain anomalies. Two data sources are already providing HTTP data to the model. However, the model also expects a DNS data source which is not present. The light gray DNS in the Models box indicates that the data source is missing or incomplete, and the box containing the anomaly name is light blue instead of a darker shade of blue.

This screen image shows the Data Available page for an anomaly named Blacklisted Domain. The screen shows two data sources with dotted lines leading to a model, which leads to the anomaly. The data source DNS in the model is in light gray, indicating that it is missing.

If no data is available, the anomaly appears in the No Data Available section. The box containing the anomaly name has no color, indicating that none of the expected data sources are present.

